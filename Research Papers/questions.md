https://github.com/rbackupX/QuestionBank
https://www.kaggle.com/questions-and-answers/239533

https://www.analyticsvidhya.com/blog/2018/06/comprehensive-data-science-machine-learning-interview-guide/


https://www.mlstack.cafe/blog/data-science-interview-questions
https://www.mlstack.cafe/blog/tensorflow-interview-questions
https://www.mlstack.cafe/blog/keras-interview-questions
https://www.mlstack.cafe/blog/data-scientist-interview-questions
https://www.mlstack.cafe/blog/data-analyst-interview-questions
https://www.mlstack.cafe/blog/reinforcement-learning-interview-questions
https://www.mlstack.cafe/blog/computer-vision-interview-questions
https://www.mlstack.cafe/blog/ml-design-patterns-interview-questions



20 Probability Interview Problems Asked By Top-Tech Companies & Wall Street

    [Facebook - Easy] There is a fair coin (one side heads, one side tails) and an unfair coin (both sides tails). You pick one at random, flip it 5 times, and observe that it comes up as tails all five times. What is the chance that you are flipping the unfair coin?
    [Lyft - Easy] You and your friend are playing a game. The two of you will continue to toss a coin until the sequence HH or TH shows up. If HH shows up first, you win. If TH shows up first, your friend wins. What is the probability of you winning?
    [Google - Easy] What is the probability that a seven-game series goes to 7 games?
    [Facebook - Easy] Facebook has a content team that labels pieces of content on the platform as spam or not spam. 90% of them are diligent raters and will label 20% of the content as spam and 80% as non-spam. The remaining 10% are non-diligent raters and will label 0% of the content as spam and 100% as non-spam. Assume the pieces of content are labeled independently from one another, for every rater. Given that a rater has labeled 4 pieces of content as good, what is the probability that they are a diligent rater?
    [Bloomberg - Easy] Say you draw a circle and choose two chords at random. What is the probability that those chords will intersect?
    [Amazon - Easy] 1/1000 people have a particular disease, and there is a test that is 98% correct if you have the disease. If you don’t have the disease, there is a 1% error rate. If someone tests positive, what are the odds they have the disease?
    [Facebook - Easy] There are 50 cards of 5 different colors. Each color has cards numbered between 1 to 10. You pick 2 cards at random. What is the probability that they are not of same color and also not of same number?
    [Tesla - Easy] A fair six-sided die is rolled twice. What is the probability of getting 1 on the first roll and not getting 6 on the second roll?
    [Facebook - Easy] What is the expected number of rolls needed to see all 6 sides of a fair die?
    [Microsoft - Easy] Three friends in Seattle each told you it’s rainy, and each person has a 1/3 probability of lying. What is the probability that Seattle is rainy? Assume the probability of rain on any given day in Seattle is 0.25.
    [Uber - Easy] Say you roll three dice, one by one. What is the probability that you obtain 3 numbers in a strictly increasing order?
    [Bloomberg - Medium] Three ants are sitting at the corners of an equilateral triangle. Each ant randomly picks a direction and starts moving along the edge of the triangle. What is the probability that none of the ants collide? Now, what if it is k ants on all k corners of an equilateral polygon?
    [Two Sigma - Medium] What is the expected number of coin flips needed to get two consecutive heads?
    [Amazon - Medium] How many cards would you expect to draw from a standard deck before seeing the first ace?
    [Robinhood - Medium] A and B are playing a game where A has n+1 coins, B has n coins, and they each flip all of their coins. What is the probability that A will have more heads than B?
    [Airbnb - Medium] Say you are given an unfair coin, with an unknown bias towards heads or tails. How can you generate fair odds using this coin?
    [Quora - Medium] Say you have N i.i.d. draws of a normal distribution with parameters μ and σ. What is the probability that k of those draws are larger than some value Y?
    [Spotify - Hard] A fair die is rolled n times. What is the probability that the largest number rolled is r, for each r in 1..6?
    [Snapchat - Hard] There are two groups of n users, A and B, and each user in A is friends with those in B and vice versa. Each user in A will randomly choose a user in B as their best friend and each user in B will randomly choose a user in A as their best friend. If two people have chosen each other, they are mutual best friends. What is the probability that there will be no mutual best friendships?
    [Tesla - Hard] Suppose there is a new vehicle launch upcoming. Initial data suggests that any given day there is either a malfunction with some part of the vehicle or possibility of a crash, with probability p which then requires a replacement. Additionally, each vehicle that has been around for n days must be replaced. What is the long-term frequency of vehicle replacements?

20 Statistics Problems Asked By FANG & Hedge Funds

    [Facebook - Easy] How would you explain a confidence interval to a non-technical audience?
    [Two Sigma - Easy] Say you are running a multiple linear regression and believe there are several predictors that are correlated. How will the results of the regression be affected if they are indeed correlated? How would you deal with this problem?
    [Uber - Easy] Describe p-values in layman’s terms.
    [Facebook - Easy] How would you build and test a metric to compare two user’s ranked lists of movie/tv show preferences?
    [Microsoft - Easy] Explain the statistical background behind power.
    [Twitter - Easy] Describe A/B testing. What are some common pitfalls?
    [Google - Medium] How would you derive a confidence interval from a series of coin tosses?
    [Stripe - Medium] Say you model the lifetime for a set of customers using an exponential distribution with parameter λ, and you have the lifetime history (in months) of n customers. What is your best guess for λ?
    [Lyft - Medium] Derive the mean and variance of the uniform distribution U(a, b).
    [Google - Medium] Say we have X ~ Uniform(0, 1) and Y ~ Uniform(0, 1). What is the expected value of the minimum of X and Y?
    [Spotify - Medium] You sample from a uniform distribution [0, d] n times. What is your best estimate of d?
    [Quora - Medium] You are drawing from a normally distributed random variable X ~ N(0, 1) once a day. What is the approximate expected number of days until you get a value of more than 2?
    [Facebook - Medium] Derive the expectation for a geometric distributed random variable.
    [Google - Medium] A coin was flipped 1000 times, and 550 times it showed up heads. Do you think the coin is biased? Why or why not?
    [Robinhood - Medium] Say you have n integers 1…n and take a random permutation. For any integers i, j let a swap be defined as when the integer i is in the jth position, and vice versa. What is the expected value of the total number of swaps?
    [Uber - Hard] What is the difference between MLE and MAP? Describe it mathematically.
    [Google - Hard] Say you have two subsets of a dataset for which you know their means and standard deviations. How do you calculate the blended mean and standard deviation of the total dataset? Can you extend it to K subsets?
    [Lyft - Hard] How do you randomly sample a point uniformly from a circle with radius 1?
    [Two Sigma - Hard] Say you continually sample from some i.i.d. uniformly distributed (0, 1) random variables until the sum of the variables exceeds 1. How many times do you expect to sample?
    [Uber - Hard] Given a random Bernoulli trial generator, how do you return a value sampled from a normal distribution

Solutions To Probability Interview Questions

Problem #1 Solution:

We can use Bayes Theorem here. Let U denote the case where we are flipping the unfair coin and F denote the case where we are flipping a fair coin. Since the coin is chosen randomly, we know that P(U) = P(F) = 0.5. Let 5T denote the event where we flip 5 heads in a row. Then we are interested in solving for P(U|5T), i.e., the probability that we are flipping the unfair coin, given that we saw 5 tails in a row.

We know P(5T|U) = 1 since by definition the unfair coin will always result in tails. Additionally, we know that P(5T|F) = 1/2^5 = 1/32 by definition of a fair coin. By Bayes Theorem we have:

Therefore the probability we picked the unfair coin is about 97%.

Problem #5 Solution:

By definition, a chord is a line segment whereby the two endpoints lie on the circle. Therefore, two arbitrary chords can always be represented by any four points chosen on the circle. If you choose to represent the first chord by two of the four points then you have:

choices of choosing the two points to represent chord 1 (and hence the other two will represent chord 2). However, note that in this counting, we are duplicating the count of each chord twice since a chord with endpoints p1 and p2 is the same as a chord with endpoints p2 and p1. Therefore the proper number of valid chords is:

Among these three configurations, only exactly one of the chords will intersect, hence the desired probability is:

Problem #13 Solution:

Let X be the number of coin flips needed until two heads. Then we want to solve for E[X]. Let H denote a flip that resulted in heads, and T denote a flip that resulted in tails. Note that E[X] can be written in terms of E[X|H] and E[X|T], i.e. the expected number of flips needed, conditioned on a flip being either heads or tails respectively.

Conditioning on the first flip, we have:

Note that E[X|T] = E[X] since if a tail is flipped, we need to start over in getting two heads in a row.

To solve for E[X|H], we can condition it further on the next outcome: either heads (HH) or tails (HT).

Therefore, we have:

Note that if the result is HH, then E[X|HH] = 0 since the outcome was achieved, and that E[X|HT] = E[X] since a tail was flipped, we need to start over again, so:

Plugging this into the original equation yields E[X] = 6 coin flips

Problem #15 Solution:

Consider the first n coins that A flips, versus the n coins that B flips.

There are three possible scenarios:

    A has more heads than B
    A and B have an equal amount of heads
    A has less heads than B

Notice that in scenario 1, A will always win (irrespective of coin n+1), and in scenario 3, A will always lose (irrespective of coin n+1). By symmetry, these two scenarios have an equal probability of occurring.

Denote the probability of either scenario as x, and the probability of scenario 2 as y.

We know that 2x + y = 1 since these 3 scenarios are the only possible outcomes. Now let’s consider coin n+1. If the flip results in heads, with probability 0.5, then A will have won after scenario 2 (which happens with probability y). Therefore, A’s total chances of winning the game are increased by 0.5y.

Thus, the probability that A will win the game is:

Problem #18 Solution:

Let B be the event that all n rolls have a value less than or equal to r. Then we have:

since all n rolls must have a value less than or equal to r. Let A be the event that the largest number is r. We have:

and since the two events on the right hand side are disjoint, we have:

Therefore, the probability of A is given by:

Solutions To Statistics Interview Questions

Problem #2 Solution:

There will be two main problems. The first is that the coefficient estimates and signs will vary dramatically, depending on what particular variables you include in the model. In particular, certain coefficients may even have confidence intervals that include 0 (meaning it is difficult to tell whether an increase in that X value is associated with an increase or decrease in Y). The second is that the resulting p-values will be misleading - an important variable might have a high p-value and deemed insignificant even though it is actually important.

You can deal with this problem by either removing or combining the correlated predictors. In removing the predictors, it is best to understand the causes of the correlation (i.e. did you include extraneous predictors or such as both X and 2X). For combining predictors, it is possible to include interaction terms (the product of the two). Lastly, you should also 1) center data, and 2) try to obtain a larger sample size (which will lead to narrower confidence intervals).

Problem #9 Solution:

For X ~U(a, b) we have the following:

Therefore we can calculate the mean as:

Similarly for variance we want:

And we have:

Therefore:

Problem #12 Solution:

Since X is normally distributed, we can look at the cumulative distribution function (CDF) of the normal distribution:

To check the probability X is at least 2, we can check (knowing that X is distributed as standard normal):

Therefore P(X > 2) = 1 - 0.977 = 0.023 for any given day. Since the draws are independent each day, then the expected time until drawing an X > 2 follows a geometric distribution, with p = 0.023. Let T be a random variable denoting the number of days, then we have:

Problem #14 Solution:

Because the sample size of flips is large (1000), we can apply the Central Limit Theorem. Since each individual flip is a Bernoulli random variable, we can assume it has a probability of showing up heads as p. Then we want to test whether p is 0.5 (i.e. whether it is fair). The Central Limit Theorem allows us to approximate the total number of heads seen as being normally distributed.

More specifically, the number of heads seen should follow a Binomial distribution since it a sum of Bernoulli random variables. If the coin is not biased (p = 0.5), then we have the following on the expected number of heads:

and the variance is given by:

Since this mean and standard deviation specify the normal distribution, we can calculate the corresponding z-score for 550 heads:

This means that, if the coin were fair, the event of seeing 550 heads should occur with a < 1% chance under normality assumptions. Therefore, the coin is likely biased.

Problem #20 Solution:

Assume we have n Bernoulli trials each with a success probability of p:

Assuming iid trials, we can compute the sample mean for p from a large number of trials:

We know the expectation of this sample mean is:

Additionally, we can compute the variance of this sample mean:

Assume we sample a large n. Due to the Central Limit Theorem, our sample mean will be normally distributed:

Therefore we can take a z-score of our sampled mean as:

This z-score will then be a simulated value from a standard normal distribution.



General Machine Learning Questions
Difference between convex and non-convex cost function; what does it mean when a cost function is non-convex?

Convex: local min = global min efficient solvers strong theoretical guarantees Examples of ML algorithms:

    Linear regression/ Ridge regression, with Tikhonov regularisation
    Sparse linear regression with L1 regularisation, such as Lasso
    Support vector machines
    Parameter estimation in Linear-Gaussian time series (Kalman filter and friends)

Non-convex

    Multi local min
    Many solvers come from convex world
    Weak theoretical guarantees if any Examples of ML algorithms:
    Neural networks
    Maximum likelihood mixtures of Gaussians

What is overfitting?

https://en.wikipedia.org/wiki/Overfitting
Describe Decision Tree, SVM, Random Forest and Boosting. Talk about their advantage and disadvantages.

https://www2.isye.gatech.edu/~tzhao80/Lectures/Lecture_6.pdf
Describe the criterion for a particular model selection. Why is dimension reduction important?

http://www.stat.cmu.edu/tr/tr759/tr759.pdf
What are the assumptions for logistic and linear regression?

    Linear regression: Linearity of residuals, Independence of residuals, Normal distribution of residuals, Equal variance of residuals. http://blog.uwgb.edu/bansalg/statistics-data-analytics/linear-regression/what-are-the-four-assumptions-of-linear-regression/
    Logistic regression: Dependent variable is binary, Observations are independent of each other, Little or no multicollinearity among the independent variables, Linearity of independent variables and log odds. https://www.statisticssolutions.com/assumptions-of-logistic-regression/

Compare Lasso and Ridge Regression.

https://blog.alexlenail.me/what-is-the-difference-between-ridge-regression-the-lasso-and-elasticnet-ec19c71c9028
What’s the difference between MLE and MAP inference?

https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/
How does K-means work? What kind of distance metric would you choose? What if different features have different dynamic range?

    Explain why and pseudo-code: http://stanford.edu/~cpiech/cs221/handouts/kmeans.html
    Distance metrics: Euclidean distance, Manhatan distance, https://pdfs.semanticscholar.org/a630/316f9c98839098747007753a9bb6d05f752e.pdf
    Explain normalization for K-means and different results you can have: https://www.edupristine.com/blog/k-means-algorithm

How many topic modeling techniques do you know of? Formulate LSI and LDA techniques.

https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547
What are generative and discriminative algorithms? What are their strengths and weaknesses? Which type of algorithms are usually used and why?”

https://cedar.buffalo.edu/~srihari/CSE574/Discriminative-Generative.pdf
Why scaling of the input is important? For which learning algorithms this is important? What is the problem with Min-Max scaling?

https://sebastianraschka.com/Articles/2014_about_feature_scaling.html
How can you plot ROC curves for multiple classes?

With macro-averaging of weights where PRE = (PRE1 + PRE2 + --- + PREk )/K https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html
2.neural-networks-questions.md
Neural Networks
Is random weight assignment better than assigning same weights to the units in the hidden layer?

Because of the symmetry problem, all the units will get the same values during the forward propagation. This also will bias you to a specific local minima. https://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94
Why is gradient checking important?

Gradient checking can help to find bugs in a backpropagation implementation, it is done by comparing the analytical gradient and the numerical gradient computed with calculus. https://stackoverflow.com/questions/47506521/what-exactly-is-gradient-checking http://cs231n.github.io/optimization-1/
What is the loss function in a NN?

The loss function depends on the type of problem: Regression: Mean squared error Binary classification: Binary cross entropy Multiclass: Cross entropy Ranking: Hinge loss
There is a neuron in the hidden layer that always has a large error found in backpropagation. What can be the reason?

It can be either the weight transfer from the input layer to the hidden layer for that neuron is to be blamed or the activation function for the neuron should be changed. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
3.svm-logr-em.questions.md
SVM and Log Regression (Log R)
Difference between SVM and Log R?

http://www.cs.toronto.edu/~kswersky/wp-content/uploads/svm_vs_lr.pdf
What does LogR give ?

Posterior probability (P(y|x))
Does SVM give any probabilistic output?

http://www.cs.cornell.edu/courses/cs678/2007sp/platt.pdf
What are the support vectors in SVM?

The vectors that define the hyperplane (margin) of SVM.
Evaluation of LogR?

You can use any evaluation metric such as Precision, Recall, AUC, F1.
How does a logistic regression model know what the coefficients are?

http://www-hsc.usc.edu/~eckel/biostat2/notes/notes14.pdf
Expectation-Maximization
How's EM done?

https://stackoverflow.com/questions/11808074/what-is-an-intuitive-explanation-of-the-expectation-maximization-technique
How are the params of EM updated?

https://stackoverflow.com/questions/11808074/what-is-an-intuitive-explanation-of-the-expectation-maximization-technique
When doing an EM for GMM, how do you find the mixture weights?

I replied that for 2 Gaussians, the prior or the mixture weight can be assumed to be a Bernouli distribution. http://www.aishack.in/tutorials/expectation-maximization-gaussian-mixture-model-mixtures/
If x ~ N(0,1), what does 2x follow?

N(0,2) https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables
How would you sample for a GMM?

http://www.robots.ox.ac.uk/~fwood/teaching/C19_hilary_2013_2014/gmm.pdf
How to sample from a Normal Distribution with known mean and variance?

https://stats.stackexchange.com/questions/16334/how-to-sample-from-a-normal-distribution-with-known-mean-and-variance-using-a-co
4.data-science-prob-questions.md
Data Science in Production
When you have a time series data by monthly, it has large data records, how will you find out significant difference between this month and previous months values?

Many possible answers here, mine: you sample a N large enough to reduce uncertainty over the large data, then you compare with a statistical test. https://www.sas.upenn.edu/~fdiebold/Teaching104/Ch14_slides.pdf
When users are navigating through the Amazon website, they are performing several actions. What is the best way to model if their next action would be a purchase?

A sequential machine learning algorithm where you manage to keep the state of the user and predict his/her next action. Here many options are possible HMM, RNN, Bandits.
When you recommend a set of items in a horizontal manner there is a problem we call it position bias? How do you use click data without position bias?

You sample by position making them a uniform distribution.
If you can build a perfect (100% accuracy) classification model to predict some customer behaviour, what will be the problem in application?

All the problems that can happen with overfitting.
Math and Probability
How do you weight 9 marbles three times on a balance scale to select the heaviest one?

https://mattgadient.com/2013/02/03/9-marbles-and-a-weight-balance-which-is-the-heaviest-one/
Estimate the disease probability in one city given the probability is very low nationwide. Randomly asked 1000 person in this city, with all negative response (NO disease). What is the probability of disease in this city?

https://medium.com/acing-ai/interview-guide-to-probability-distributions-a6dfb08c3766
5.programming-questions.md
Programming Questions
Given a bar plot and imagine you are pouring water from the top, how to qualify how much water can be kept in the bar chart?

https://www.geeksforgeeks.org/trapping-rain-water/
Find the cumulative sum of top 10 most profitable products of the last 6 month for customers in Seattle.

Solution: heap that keeps and updates the most profitable products.
Implement circular queue using an array.

https://www.geeksforgeeks.org/circular-queue-set-1-introduction-array-implementation/
Given a ‘csv’ file with ID and Quantity columns, 50 million records and size of data as 2 GBs, write a program in any language of your choice to aggregate the QUANTITY column.

Grep like solution, careful with overflow!
Given a function with inputs — an array with N randomly sorted numbers, and an int K, return output in an array with the K largest numbers.

https://www.geeksforgeeks.org/kth-smallestlargest-element-unsorted-array/
Given two strings, print all the inter-leavings of the Strings in which characters from two strings should be in same order as they were in original strings.

e.g. for "abc", "de", print all of these: adebc, abdec, adbce, deabc, dabce, etc, etc

https://gist.github.com/geraldyeo/6c4eaea8a1a6bcc480cac5328cbff664



ML System Design

Best way to practice is reading/watching case studies and examples on Blogs and YT.

ML Systems Design Interview Guide · Patrick Halina

Really Good Compilation

chiphuyen/machine-learning-systems-design: A booklet on machine learning systems design with exercises
Good Read

DropBox OCR Pipeline

Machine Learning System Design (YouTube Recommendation System)

Compass

Airbnb
Recommender Systems

Very high chances that a design question is related to Recommender System.

In-Depth Guide: How Recommender Systems Work | Built In

How to Design and Build a Recommendation System Pipeline in Python (Jill Cates)
Interview Questions

https://github.com/Sroy20/machine-learning-interview-questions

https://github.com/ShuaiW/data-science-question-answer

https://github.com/andrewekhalel/MLQuestions

https://github.com/iamtodor/data-science-interview-questions-and-answers

https://github.com/kojino/120-Data-Science-Interview-Questions

https://www.itshared.org/2015/10/data-science-interview-questions.html


# <a name="ml-sys"></a>  5. Machine Learning System Design

## Designing ML systems for production
This is one of my favorite interviews in which you can shine bright and up-level your career. I'd like to mention the following important notes:

- Remember, the goal of ML system design interview is NOT to measure your deep and detailed knowledge of different ML algorithms, but your ability to zoom out and design a production-level ML system that can be deployed as a service within a company's ML infrastructure.

- Deploying deep learning models in production can be challenging, and it is beyond training models with good performance. Several distinct components need to be designed and developed in order to deploy a production level deep learning system.
<p align="center">
<img src="https://github.com/alirezadir/Production-Level-Deep-Learning/blob/master/images/components.png" title="" width="70%" height="70%">
</p>

- For more insight on different components above you can check out the following resources):
  - [Full Stack Deep Learning course](https://fall2019.fullstackdeeplearning.com/)
  - [Production Level Deep Learning](https://github.com/alirezadir/Production-Level-Deep-Learning)
  - [Machine Learning Systems Design](https://github.com/chiphuyen/machine-learning-systems-design)
  - Stanford course on ML system design [TBA]

Once you learn about the basics, I highly recommend checking out different companies blogs on ML systems, which I learnt a lot from. You can refer to some of those resources in the subsection [ML at Companies](#ml-at-companies) below.

## ML System Design Flow
Approaching an ML system design problem follows a similar flow to the generic software system design.
For more insight on general system design interview you can e.g. check out:
- [Grokking the System Design Interview
](https://www.educative.io/courses/grokking-the-system-design-interview)
- [System design primer](https://github.com/donnemartin/system-design-primer)
- [Deep Learning Interviews](https://www.amazon.in/Deep-Learning-Interviews-interview-questions/dp/1916243568)

Below is a design flow that I would recommend:

1. Problem Description
    - What does it mean? 
    - Use cases 
    - Requirements
    - Assumptions 
2. Do we need ML to solve this problem? 
    - Trade off between impact and cost
      - Costs: Data collection, data annotation, compute 
    - if Yes, go to the next topic. If No, follow a general system design flow. 
3. ML Metrics 
      - Accuracy metrics: 
          - imbalanced data?
      - Latency 
      - Problem specific metric (e.g. CTR)
4. Data
    - Needs 
        - type (e.g. image, text, video, etc) and volume
    - Sources
        - availability and cost 
    - Labelling (if needed)
      - labeling cost  
5. MVP Logic 
    - Model based vs rule based logic 
        - Pros and cons, and decision 
          -  Note: Always start as simple as possible and iterate over 
    - Propose a simple model (e.g. a binary logistic regression classifier)
    - Features/ Signals (if needed)
      - what to chose as and how to chose features 
      - feature representation 
6. Training (if needed)
      - data splits (train, dev, test)
        - portions
        - how to chose a test set 
      - debugging 
    - Iterate over MVP model (if needed)
      - data augmentation  
7. Inference (online)
    - Data processing and verification 
    - Prediction module 
    - Serving infra 
    - Web app 
8. Scaling
  - Scaling for increased demand (same as in distributed systems)
      - Scaling web app and serving system 
      - Data partitioning 
  - Data parallelism 
  - Model parallelism 
9. A/B test and deployment
    - How to A/B test? 
      - what portion of users?
      - control and test groups 
10. Monitoring and Updates 
    - seasonality   


## ML System Design Topics
I observed there are certain sets of topics that are frequently brought up or can be used as part of the logic of the system. Here are some of the important ones:

### Recommendation Systems
- Collaborative Filtering (CF)
    - user based, item based
    - Cold start problem
    - Matrix factorization
- Content based filtering

### NLP

- Preprocessing
  - Normalization, tokenization, stop words
- Word Embeddings
  - Word2Vec, GloVe, Elmo, BERT
- Text classification and sentiment analysis
- NLP specialist topics:
  - Language Modeling
  - Part of speech tagging
  - POS HMM
    - Viterbi algorithm and beam search
  - Named entity recognition
  - Topic modeling
  - Speech Recognition Systems
    - Feature extraction, MFCCs
    - Acoustic modeling
      - HMMs for AM
      - CTC algorithm (advanced)
    - Language modeling
      - N-grams vs deep learning models (trade-offs)
      - Out of vocabulary problem
  - Dialog and chatbots
    - [CMU lecture on chatbots](http://tts.speech.cs.cmu.edu/courses/11492/slides/chatbots_shrimai.pdf)
    - [CMU lecture on spoken dialogue systems](http://tts.speech.cs.cmu.edu/courses/11492/slides/sds_components.pdf)
  - Machine Translation
    - Seq2seq models, NMT

Note: The reason I have more topics here is because this was my focus in my own interviews

### Ads and Ranking
- CTR prediction
- Ranking algorithms

### Information retrieval
- Search
  - Pagerank
  - Autocomplete for search

### Computer vision
- Image classification
- Object Tracking
- Popular architectures (AlexNet, VGG, ResNET)
- [TBD]

### Transfer learning
- Why and when to use transfer learning
- How to do it
  - depending on the dataset sizes and similarities


## ML Systems at Big Companies 
- AI at LinkedIn
  - [Intro to AI at Linkedin](https://engineering.linkedin.com/blog/2018/10/an-introduction-to-ai-at-linkedin)
  - [Building The LinkedIn Knowledge Graph](https://engineering.linkedin.com/blog/2016/10/building-the-linkedin-knowledge-graph)
  - [The AI Behind LinkedIn Recruiter search and recommendation systems](https://engineering.linkedin.com/blog/2019/04/ai-behind-linkedin-recruiter-search-and-recommendation-systems)
  - [A closer look at the AI behind course recommendations on LinkedIn Learning, Part 1](https://engineering.linkedin.com/blog/2020/course-recommendations-ai-part-one)
  - [A closer look at the AI behind course recommendations on LinkedIn Learning, Part 2](https://engineering.linkedin.com/blog/2020/course-recommendations-ai-part-two)
  - [Communities AI: Building communities around interests on LinkedIn](https://engineering.linkedin.com/blog/2019/06/building-communities-around-interests)
  - [Linkedin's follow feed](https://engineering.linkedin.com/blog/2016/03/followfeed--linkedin-s-feed-made-faster-and-smarter)
  - XNLT for A/B testing


- ML at Google
    - ML pipelines with TFX and KubeFlow
    - [How Google Search works](https://www.google.com/search/howsearchworks/)
      - Page Rank algorithm ([intro to page rank](https://www.youtube.com/watch?v=IKXvSKaI2Ko), [the algorithm that started google](https://www.youtube.com/watch?v=qxEkY8OScYY))
    - TFX production components
      - [TFX workshop by Robert Crowe](https://conferences.oreilly.com/artificial-intelligence/ai-ca-2019/cdn.oreillystatic.com/en/assets/1/event/298/TFX_%20Production%20ML%20pipelines%20with%20TensorFlow%20Presentation.pdf)
    - [Google Cloud Platform Big Data and Machine Learning Fundamentals](https://www.coursera.org/learn/gcp-big-data-ml-fundamentals)
- Scalable ML using AWS
  - [AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/)
  - [Deploy a machine learning model with AWS Elastic Beanstalk](https://medium.com/swlh/deploy-a-machine-learning-model-with-aws-elasticbeanstalk-dfcc47b6043e)
  - [Deploying Machine Learning Models as API using AWS](https://medium.com/towards-artificial-intelligence/deploying-machine-learning-models-as-api-using-aws-a25d05518084)
  - [Serverless Machine Learning On AWS Lambda](https://medium.com/swlh/how-to-deploy-your-scikit-learn-model-to-aws-44aabb0efcb4)
-  ML at Facebook
   -  [Machine Learning at Facebook Talk](https://www.youtube.com/watch?v=C4N1IZ1oZGw)
   -  [Scaling AI Experiences at Facebook with PyTorch](https://www.youtube.com/watch?v=O8t9xbAajbY)
   -  [Understanding text in images and videos](https://ai.facebook.com/blog/rosetta-understanding-text-in-images-and-videos-with-machine-learning/)
   -  [Protecting people](https://ai.facebook.com/blog/advances-in-content-understanding-self-supervision-to-protect-people/)
   -  Ads
      - Ad CTR prediction
      - [Practical Lessons from Predicting Clicks on Ads at Facebook](https://quinonero.net/Publications/predicting-clicks-facebook.pdf)
   - Newsfeed Ranking
     - [How Facebook News Feed Works](https://techcrunch.com/2016/09/06/ultimate-guide-to-the-news-feed/)
     - [How does Facebook’s advertising targeting algorithm work?](https://quantmar.com/99/How-does-facebooks-advertising-targeting-algorithm-work)
     - [ML and Auction Theory](https://www.youtube.com/watch?v=94s0yYECeR8)
     - [Serving Billions of Personalized News Feeds with AI - Meihong Wang](https://www.youtube.com/watch?v=wcVJZwO_py0&t=80s)
     - [Generating a Billion Personal News Feeds](https://www.youtube.com/watch?v=iXKR3HE-m8c&list=PLefpqz4O1tblTNAtKaSIOU8ecE6BATzdG&index=2)
     - [Instagram feed ranking](https://www.facebook.com/atscaleevents/videos/1856120757994353/?v=1856120757994353)
     - [How Instagram Feed Works](https://techcrunch.com/2018/06/01/how-instagram-feed-works/)
   - [Photo search](https://engineering.fb.com/ml-applications/under-the-hood-photo-search/)
   - Social graph search
   - Recommendation
     - [Recommending items to more than a billion people](https://engineering.fb.com/core-data/recommending-items-to-more-than-a-billion-people/)
     - [Social recommendations](https://engineering.fb.com/android/made-in-ny-the-engineering-behind-social-recommendations/)
   - [Live videos](https://engineering.fb.com/ios/under-the-hood-broadcasting-live-video-to-millions/)
   - [Large Scale Graph Partitioning](https://engineering.fb.com/core-data/large-scale-graph-partitioning-with-apache-giraph/)
   - [TAO: Facebook’s Distributed Data Store for the Social Graph](https://www.youtube.com/watch?time_continue=66&v=sNIvHttFjdI&feature=emb_logo) ([Paper](https://www.usenix.org/system/files/conference/atc13/atc13-bronson.pdf))
   - [NLP at Facebook](https://www.youtube.com/watch?v=ZcMvffdkSTE)
-  ML at Netflix
   -  [Recommendation at Netflix](https://www.slideshare.net/moustaki/recommending-for-the-world)
   -  [Past, Present & Future of Recommender Systems: An Industry Perspective](https://www.slideshare.net/justinbasilico/past-present-future-of-recommender-systems-an-industry-perspective)
   -  [Deep learning for recommender systems](https://www.slideshare.net/moustaki/deep-learning-for-recommender-systems-86752234)
   -  [Reliable ML at Netflix](https://www.slideshare.net/justinbasilico/making-netflix-machine-learning-algorithms-reliable)
   -  [ML at Netflix (Spark and GraphX)](https://www.slideshare.net/SessionsEvents/ehtsham-elahi-senior-research-engineer-personalization-science-and-engineering-group-at-netflix-at-mlconf-sea-50115?next_slideshow=1)
   -  [Recent Trends in Personalization](https://www.slideshare.net/justinbasilico/recent-trends-in-personalization-a-netflix-perspective)
   -  [Artwork Personalization @ Netflix](https://www.slideshare.net/justinbasilico/artwork-personalization-at-netflix)

# <a name="breadth"></a> 4. ML Breadth/Fundamentals
As the name suggests, this interview is intended to evaluate your general knowledge of ML concepts both from theoretical and practical perspectives. Unlike ML depth interviews, the breadth interviews tend to follow a pretty similar structure and coverage amongst different interviewers and interviewees.

The best way to prepare for this interview is to review your notes from ML courses as well some high quality online courses and material. In particular, I found the following resources pretty helpful.

## Courses and review material:
- [Andrew Ng's Machine Learning Course](https://www.coursera.org/learn/machine-learning) (you can also find the [lectures on Youtube](https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN) )
- [Structuring Machine Learning Projects](https://www.coursera.org/learn/machine-learning-projects)
- [Udacity's deep learning nanodegree](https://www.udacity.com/course/deep-learning-nanodegree--nd101) or  [Coursera's Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) (for deep learning)


If you already know the concepts, the following resources are pretty useful for a quick review of different concepts:
- [StatQuest Machine Learning videos](https://www.youtube.com/watch?v=Gv9_4yMHFhI&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF)
- [StatQuest Statistics](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9) (for statistics review - most useful for Data Science roles)
- [Machine Learning cheatsheets](https://ml-cheatsheet.readthedocs.io/en/latest/)
- [Chris Albon's ML falshcards](https://machinelearningflashcards.com/)

Below are the most important topics to cover:
## 1. Classic ML Concepts

### ML Algorithms' Categories
  - Supervised, unsupervised, and semi-supervised learning (with examples)
    - Classification vs regression vs clustering
  - Parametric vs non-parametric algorithms
  - Linear vs Nonlinear algorithms
### Supervised learning
  - Linear Algorithms
    - Linear regression
      - least squares, residuals,  linear vs multivariate regression
    - Logistic regression
      - cost function (equation, code),  sigmoid function, cross entropy
    - Support Vector Machines
    - Linear discriminant analysis

  - Decision Trees
    - Logits
    - Leaves
    - Training algorithm
      - stop criteria
    - Inference
    - Pruning

  - Ensemble methods
    - Bagging and boosting methods (with examples)
    - Random Forest
    - Boosting
      - Adaboost
      - GBM
      - XGBoost
  - Comparison of different algorithms
    - [TBD: LinkedIn lecture]

  - Optimization
    - Gradient descent (concept, formula, code)
    - Other variations of gradient descent
      - SGD
      - Momentum
      - RMSprop
      - ADAM
  - Loss functions
    - Logistic Loss function 
    - Cross Entropy (remember formula as well)
    - Hinge loss (SVM)

- Feature selection
  - Feature importance
- Model evaluation and selection
  - Evaluation metrics
    - TP, FP, TN, FN
    - Confusion matrix
    - Accuracy, precision, recall/sensitivity, specificity, F-score
      - how do you choose among these? (imbalanced datasets)
      - precision vs TPR (why precision)
    - ROC curve (TPR vs FPR, threshold selection)
    - AUC (model comparison)
    - Extension of the above to multi-class (n-ary) classification
    - algorithm specific metrics [TBD]
  - Model selection
    - Cross validation
      - k-fold cross validation (what's a good k value?)

### Unsupervised learning
  - Clustering
    - Centroid models: k-means clustering
    - Connectivity models: Hierarchical clustering
    - Density models: DBSCAN
  - Gaussian Mixture Models
  - Latent semantic analysis
  - Hidden Markov Models (HMMs)
    - Markov processes
    - Transition probability and emission probability
    - Viterbi algorithm [Advanced]
  - Dimension reduction techniques
    - Principal Component Analysis (PCA)
    - Independent Component Analysis (ICA)
    - T-sne


### Bias / Variance (Underfitting/Overfitting)
- Regularization techniques
  - L1/L2 (Lasso/Ridge)
### Sampling
- sampling techniques
  - Uniform sampling
  - Reservoir sampling
  - Stratified sampling
### Missing data
 - [TBD]
### Time complexity of ML algorithms
- [TBD]

## 2. Deep learning
- Feedforward NNs
  - In depth knowledge of how they work
  - [EX] activation function for classes that are not mutually exclusive
- RNN
  - backpropagation through time (BPTT)
  - vanishing/exploding gradient problem
- LSTM
  - vanishing/exploding gradient problem
  -  gradient?
- Dropout
  - how to apply dropout to LSTM?
- Seq2seq models
- Attention
  - self-attention
- Transformer and its architecture (in details, yes, no kidding! I was asked twice! In an ideal world, I wouldn't answer those detailed questions to anyone except the authors and teammates, as either you've designed it or memorized it!)
- Embeddings (word embeddings)


## 3. Statistical ML
###  Bayesian algorithms
  - Naive Bayes
  - Maximum a posteriori (MAP) estimation
  - Maximum Likelihood (ML) estimation
### Statistical significance
- R-squared
- P-values

## 4. Other topics:
  - Outliers
  - Similarity/dissimilarity metrics
    - Euclidean, Manhattan, Cosine, Mahalanobis (advanced)



# ML Q & A interview prep:

## ML Algoritms + Basic ML

Q: What are the various types of ML? 

A: Supervised (labeled data) , Unsupervised (unlabeled data) , Reinforcement (using penalty and reward)

Q: What is your favorite algorithm? Is it used for classification or regression? (explain in under a minute)

A: Open-ended

Q: What is the difference between ML, DL and AI?

A: - AI involves machines that can perform tasks that are characteristic of human intelligence
ML is a way of achieving AI by “training” an algorithm so that it can learn data.
Deep learning is one of many approaches to machine learning, which uses Neural Networks that mimic the biological structure of the brain. 
Another difference is the feature extraction and classification are separate steps for ML but are a single NN for DL.

Q: Talk about a recent ML paper that you’ve read in 2 minutes.

A: Open-ended

Be able to explain the following metrics: Accuracy, Precision, Recall, and F1, TP, TN, FP, TN

Q: Explain how a ROC (Receiver operating characteristic) curve works.

The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs specificity (false positives). You want your model to get TPs faster than FPs, if there is the same rate of gettingTP as getting FP your model is useless.

Q: Explain TP/FP/FN/TN in a simple example:

A: - Fire alarm goes off + fire = TP
Fire alarm goes off + no fire = FP
Fire alarm doesn’t go off + fire = FN
Fire alarm doesn’t go off + no fire = TN

Q: What is Bayes Theorem?

A: Essentially Bayes Theorem gives you a probability of an event given what is known as prior knowledge, Prob = TP/ All positives(TP+FP).

Q: Why is “Naive” Bayes naive? 

A: NB makes the naive assumption that the features in a dataset are independent of each other, which isn’t applicable to real-world datasets.

Q: What is a decision tree and when would you choose to use one?

A: As the name suggests decision trees are tree-like model of decisions, they make relations between features easily interpretable. They can be used for both classification (classify passenger as survived or died) and regression (continuous values like price of a house) and don’t require any assumptions of linearity in the data.

Q: How are they pruned?

A: Pruning is what happens in decision trees when branches that have weak predictive power are removed in order to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can happen bottom-up and top-down, with approaches such as reduced error pruning and cost complexity pruning.

Reduced error pruning is perhaps the simplest version: replace each node. If it doesn’t decrease predictive accuracy, keep it pruned. While simple, this heuristic actually comes pretty close to an approach that would optimize for maximum accuracy.

Q: What is the difference between Gini Impurity and Entropy in a decision tree? 

A: While both are metrics to decide how to split a tree, Gini measurement is the probability of a random sample being classified correctly by randomly picking a label from the branch. In information theory Entropy is the measured lack of information in a system and you calculate gain by making a split. This delta entropy tells you about how the uncertainty about the label was reduced. Gini is more common because it doesn’t require the log calculations that Entropy takes.

Q: When will Entropy decrease in binary tree classification?

A: It decreases the closer we get to the leaf node.

Q: Why don’t we tend to use linear regression to model binary responses?

A: Linear regression prediction output is continuous, if you want to model binary results you should use logistic regression.

Q: What is the difference between hinge loss and log loss?

A: The hinge loss is used for "maximum-margin" classification, most notably for support vector machines. Logistic loss diverges faster than hinge loss. So, in general, it will be more sensitive to outliers. Hinge loss also penalizes wrong answers, as well as correct unconfident answers.

Q: How do linear and logistic regression differ in their error minimization techniques?

A: Linear regression uses ordinary least squares method to minimize the errors and arrive at a best possible fit, while logistic regression uses maximum likelihood method to arrive at the solution.

Q: What is more important model accuracy or model performance?

A: Model accuracy is actually a subset of model performance. For example, if you wanted to detect fraud in a massive dataset with a sample of millions, a more accurate model would most likely predict no fraud at all if only a vast minority of cases were fraud. However, this would be useless for a predictive model — a model designed to find fraud that asserted there was no fraud at all!

Q: What’s the difference between a generative and discriminative model?

A: Discriminative models are great for classification (SVM, NN, NLPs, facial recognition), they map high dimensional sensory input into a class. A generative models care how the data was generated and will learn will learn categories of data (chatbot, GANs).

Q: How does SVM and logistic regression differ?

A: They only differ in the loss function — SVM minimizes hinge loss while logistic regression minimizes logistic loss.

Q: What is an SVM? What do you do if your data is not linear? (kernel trick)

A: The objective of the support vector machine algorithm is to find the hyperplane that has the maximum margin in an N-dimensional space(N — the number of features) that distinctly classifies the data points. A kernel trick allows you to map your data to a higher dimensional feature space so you can fit a hyperplane. This is done by taking the vectors in the original space and returning the dot product of the vectors in the feature space.

Q: How do you turn the regularization (C) and gamma terms in SVMs?

A: High gamma values mean only data points close to the line are considered and a high C term means a smaller-margin around line (could overfit).

Q: Explain Dijkstra's algorithm? (Know how to use it).

A: Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph.

Q: How is KNN different from K-means clustering?

A: KNN or K-Nearest Neighbors is a supervised learning method technique used from classification or regression and does not require training. K-means is an unsupervised clustering algorithm fitting to K-clusters. 

Q: What is ensemble learning?

A: Ensemble techniques use a combination of learning algorithms to optimize better predictive performance. And they typically reduce overfitting in models. Ensembling techniques are further classified into Bagging and Boosting.

Q: What is the difference between bagging and boosting?

A: Both are ensemble models that use random sampling to reduce variance. Bagging models are built independently and better solves the problem of overfitting. Boosting builds on top of old models to create models with less bias, also weights the better performing examples higher, but may overfit. 

Q: How do you go from a decision tree to a random forest? To a Gradient Boosted Tree?

A: Bagging takes many uncorrelated learners to make a final model and it reduces error by reducing variance. Example of bagging ensemble are Random Forest models.

Boosting is an ensemble technique in which the predictors are not made independently, but sequentially in order to learn from the mistakes of the previous predictors. Gradient Boosted Trees are an example of boosting algorithm.

Q: Describe a hash table.

A: A hash table is a data structure like a dictionary in python. A key is mapped to certain values through the use of a hash function. They are often used for tasks such as database indexing.

Q: How do you deal with imbalanced data?

A: Collect more data, resample the dataset to correct for imbalances, try difference models or algorithms. 

## ML Validation

Q: Name two ways to evaluate the performance of a classification algorithm.

A: 1) Confusing Matrix ([TN,FP],[FN,TP]) 
     2) Accuracy (also AUC, F1, MAE, MSE)

Q: What’s the difference between Type I and Type II error?

A: Type I error is a false positive, while Type II error is a false negative.

Q: What is the difference between MSE and MAE?

A: MAE loss is more robust to outliers, but its derivatives are not continuous, making it inefficient to find the solution. MSE loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0). Use MAE if you have a lot of anomalies in your dataset.

Q: Why do we need a cost function and which is the best cost to use in classification algorithms.

A: We need a cost function to optimize our weights for model performance and I would use the cost function Mean Squared Error and minimize the MSE to improve the accuracy of our classification model.

Q: How do you ensure you’re not overfitting with a model?

A: 1- Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data.
2- Use cross-validation techniques such as k-folds cross-validation.
3- Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting.

Q: Explain the cross-validation resampling procedure.

A: The general procedure is as follows:

1) Shuffle the dataset randomly.
2) Split the dataset into k groups
3) For each unique group:
Take the group as a hold out or test data set
Take the remaining groups as a training data set
Fit a model on the training set and evaluate it on the test set
Retain the evaluation score and discard the model
4)Combine evaluation scores into single average (CV error)
5)Repeat process for different model and choose the one w/ lowest CV error

Q: How does evaluating your model differ between using CV or bootstrapping? What is MC-CV?

A: CV tends to be less biased but K-fold CV has fairly large variance. On the other hand, bootstrapping (sampling with replacement) tends to drastically reduce the variance but gives more biased results (they tend to be pessimistic). "Monte Carlo CV" aka "leave-group-out CV" does many random splits of the data to reduce variance.

Q: What’s the trade-off between bias and variance?

A: Bias is due to overly simplistic assumptions while variance is error due to too much complexity in the learning algorithm you’re using. Bias leads to under-fitting your data and variance leads to overfitting your data. Essentially, if you make the model more complex and add more variables, you’ll lose bias but gain some variance — in order to get the optimally reduced amount of error, you’ll have to tradeoff bias and variance and try your best to minimize each.
(In machine learning/statistics as a whole, accuracy vs. precision is analogous to bias vs. variance).

Q: What cross-validation technique would you use on a time series dataset?

A: Instead of using standard k-folds cross-validation, you have to pay attention to the fact that a time series is not randomly distributed data — it is inherently ordered by chronological order. If a pattern emerges in later time periods for example, your model may still pick up on it even if that effect doesn’t hold in earlier years!

You’ll want to do something like forward chaining where you’ll be able to model on past data then look at forward-facing data.

fold 1 : training [1], test [2]
fold 2 : training [1 2], test [3]
fold 3 : training [1 2 3], test [4]
fold 4 : training [1 2 3 4], test [5]
fold 5 : training [1 2 3 4 5], test [6]

Q: What’s the difference between L1 and L2 regularization? How does it solve the problem of overfitting? Which regularizer to use and when?

A: When dealing with a large number of features we no longer want to use CV. Both L1 (Lasso Regression) and L2 (Ridge Regression) regularization techniques are used to address over-fitting and feature selection, the key difference between these two is the penalty term. Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient while Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. 
The key difference between these techniques is that Lasso is more binary/sparse and shrinks the less important feature’s coefficient to zero thus, removing some feature altogether and L2 regularization tends to spread error among all the term. L1 works well for feature selection in case we have a huge number of features.

## ML Stats

Q: What is a Fourier transform? And why do we use it.

A:  Given a smoothie, it’s how we find the recipe (in terms of superposition of symmetric functions). Fourier transforms are used to it’s a extract features from audio signals by converting a signal from time to frequency domain.

Q: What’s the difference between probability and likelihood?

A: For binomial distributions: Probability is the percentage that a success occur. Likelihood is the conditional probability, i.e. the probability that the above event will happen.

Q: What is the difference between PCA and t-SNE? What are their use cases?

A: Both methods are used for dimensionality reduction, but t-SNE tries to deconvolve relationships between neighbors in high-dimensional data to understand the underlying structure of the data. Principal component analysis first identifies the hyperplane that lies closest to the data, and then projects the data onto it. PCA preserves the maximum amount of variance and requires labels, but is much less computationally expensive than t-SNE.

Q: How do eigenvalues and eigenvectors relate to PCA?

A: Eigenvectors have corresponding eigenvalues and eigenvectors that have the largest eigenvalues will be the principal components (new dimensions of our data).

Q: What is Maximum Likelihood (MLE)?

A: Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximize the likelihood that the process described by the model produced the data that were actually observed.

Q: When are Maximum Likelihood and Least Squared Error equal?

A: For least squares parameter estimation we want to find the line that minimizes the total squared distance between the data points and the regression line. In maximum likelihood estimation we want to maximize the total probability of the data. When a Gaussian distribution is assumed, the maximum probability is found when the data points get closer to the mean value. Since the Gaussian distribution is symmetric, this is equivalent to minimizing the distance between the data points and the mean value.

### Sources: 
Astronomer Amber!
https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html
Machine Learning Interview Questions and Answers | Machine Learning Interview Preparation | Edureka
https://www.springboard.com/blog/machine-learning-interview-questions/
https://machinelearningmastery.com/k-fold-cross-validation/
https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c
https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f
https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1
https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d


# Solutions on Cracking The Machine Learning Interview
### -------> Currently under construction! <-------
I am currently writing a solution from the Medium article "Cracking the Machine Learning Interview," written by Subhrajit Roy. In the past year since the article went public, Subhrajit has only written down the questions with no update on the solutions. I plan on finishing the war. I may add more questions outside of the articles domain. Some of my solutions may contain code from the following following frameworks: Scikit-Learn, PyTorch, TensorFlow

https://medium.com/subhrajit-roy/cracking-the-machine-learning-interview-1d8c5bb752d8

<a href="https://medium.com/subhrajit-roy/cracking-the-machine-learning-interview-1d8c5bb752d8" target="_blank">
  <img src="https://github.com/rchavezj/Cracking_The_Machine_Learning_Interview/blob/master/crackingTheMachineLearningInterviewCover.png">
</a>

# Contents: 
|                        |                                          |
| ---------------------- | ---------------------------------------- |
| 1. [Linear Algebra](#Linear-Algebra)                         | 2. [Numerical Optimization](#Numerical-Optimization)                                         |
| 3. [Basics of Probability and Information Theory](#Basics-of-Probability-and-Information-Theory)                                                                                                        |  4. [Confidence Interval](#Confidence-Interval)|
| 5. [Learning Theory](#Learning-Theory)                       |  6. [Model and Feature Selection](#Model-and-Feature-Selection) |
| 7. [Curse of dimensionality](#Curse-of-Dimensionality)       |  8. [Universal approximation of neural networks](#Universal-Approximation-of-Neural-Networks) |
| 9. [Deep Learning motivation](#Deep-Learning-Motivation)     |  10. [Support Vector Machine](#Support-Vector-Machine) |
| 11. [Bayesian Machine Learning](#Bayesian-Machine-Learning)  |  12. [Regularization](#Regularization) |
| 13. [Evaluation of Machine Learning systems](#Evaluation-of-Machine-Learning-Systems) |  14. [Clustering](#Clustering)  |
| 15. [Dimensionality Reduction](#Dimensionality-Reduction)    |  16. [Basics of Natural Language Processing](#Basics-of-Natural-Language-Processing) |
| 17. [Some basic questions](#Some-basic-questions)            |  18. [Optimization Procedures](#Optimization-Procedures) |
| 19. [Sequence Modeling](#Sequence-Modeling)                  |  20. [Autoencoders](#Autoencoders)               |
| 21. [Representation Learning](#Representation-Learning)      |  22. [Monte Carlo Methods](#Monte-Carlo-Methods) |
| 23. [Generative Models](#Generative-Models)                  |  24. [Reinforcement Learning](#Reinforcement-Learning) |
| 25. [Probabilistic Graphical Models](#Probabilistic-Graphical-Models)   | 26. [Computational Logic](#Computational-Logic)     |



### [Linear Algebra](01_Linear_Algebra/#Linear-Algebra)
[(Return back to Contents)](#Contents)
<img src="01_Linear_Algebra/linear_algebra.png" width="700">

1. What is broadcasting in connection to Linear Algebra?
2. [What are scalars, vectors, matrices, and tensors?](01_Linear_Algebra/#2-What-are-scalars-vectors-matrices-and-tensors)
3. What is Hadamard product of two matrices?
4. What is an inverse matrix?
5. If inverse of a matrix exists, how to calculate it?
6. What is the determinant of a square matrix? How is it calculated? What is the connection of determinant to eigenvalues?
7. Why does the negative area of the determinent relate to orientation flipping? Check out lecture 6 from 3BLUE1BROWN? 
8. Justify in one sentence why the following equation on why it is true: "If you multiply two matrices together, the determinent of the reulting matrix is the same as the product of the determinence of the original two matrices" det(M_{1}M_{2}) = det(M_{1})det(M_{2}). If you try to justify with numbers it would take a long time. 
9.  Discuss span and linear dependence.
10. Following up on question #7, what does the following definition mean, "The basis of a vector space is a set of linearly independent vectors that span the full space."
11. What is Ax = b? When does Ax = b has a unique solution?
12. In Ax = b, what happens when A is fat or tall?
11. When does inverse of A exist?
12. [What is a norm? What is L1, L2 and L infinity norm?](#)
13. What are the conditions a norm has to satisfy?
14. Why is squared of L2 norm preferred in ML than just L2 norm?
15. When L1 norm is preferred over L2 norm?
16. Can the number of nonzero elements in a vector be defined as L0 norm? If no, why?
17. What is Frobenius norm?
18. What is a diagonal matrix?
19. Why is multiplication by diagonal matrix computationally cheap? How is the multiplication different for square vs. non-square diagonal matrix?
20. At what conditions does the inverse of a diagonal matrix exist?
21. What is a symmetrix matrix?
22. What is a unit vector?
23. When are two vectors x and y orthogonal?
24. At R^n what is the maximum possible number of orthogonal vectors with non-zero norm?
25. When are two vectors x and y orthonormal?
26. What is an orthogonal matrix? Why is computationally preferred?
27. What is eigendecomposition, eigenvectors and eigenvalues?
28. How to find eigen values of a matrix?
29. Write the eigendecomposition formula for a matrix. If the matrix is real symmetric, how will this change?
30. Is the Eigendecomposition guaranteed to be unique? If not, then how do we represent it?
31. What are positive definite, negative definite, positive semi definite and negative semi definite matrices?
32. What is Singular Value Decomposition? Why do we use it? Why not just use ED?
33. Given a matrix A, how will you calculate its Singular Value Decomposition?
34. What are singular values, left singulars and right singulars?
35. What is the connection of Singular Value Decomposition of A with functions of A?
36. Why are singular values always non-negative?
37. What is the Moore Penrose pseudo inverse and how to calculate it?
38. If we do Moore Penrose pseudo inverse on Ax = b, what solution is provided is A is fat? Moreover, what solution is provided if A is tall?
39. Which matrices can be decomposed by ED?
40. Which matrices can be decomposed by SVD?
41. What is the trace of a matrix?
42. How to write Frobenius norm of a matrix A in terms of trace?
43. Why is trace of a multiplication of matrices invariant to cyclic permutations?
44. What is the trace of a scalar?
45. Write the frobenius norm of a matrix in terms of trace?



### Numerical Optimization
[(Return back to Contents)](#Contents)

<img src="02_Numerical_Optimization/optimization_cover.png">

1. What is underflow and overflow?
2. How to tackle the problem of underflow or overflow for softmax function or log softmax function?
3. What is poor conditioning?
4. What is the condition number?
5. What are grad, div and curl?
6. What are critical or stationary points in multi-dimensions?
7. Why should you do gradient descent when you want to minimize a function?
8. What is line search?
9. What is hill climbing?
10. What is a Jacobian matrix?
11. What is curvature?
12. What is a Hessian matrix?
13. What is a gradient checking?



### Basics of Probability and Information Theory
[(Return back to Contents)](#Contents)

<img src="03_Basics_of_Probability_and_Information_Theory/Basics_of_Probability_and_Information_Theory.png">

1. Compare “Frequentist probability” vs. “Bayesian probability”?
2. What is a random variable?
3. What is a probability distribution?
4. What is a probability mass function?
5. What is a probability density function?
6. What is a joint probability distribution?
7. What are the conditions for a function to be a probability mass function?
8. What are the conditions for a function to be a probability density function?
9. What is a marginal probability? Given the joint probability function, how will you calculate it?
10. What is conditional probability? Given the joint probability function, how will you calculate it?
11. State the Chain rule of conditional probabilities.
12. What are the conditions for independence and conditional independence of two random variables?
13. What are expectation, variance and covariance?
14. Compare covariance and independence.
15. What is the covariance for a vector of random variables?
16. What is a Bernoulli distribution? Calculate the expectation and variance of a random variable that follows Bernoulli distribution?
17. What is a multinoulli distribution?
18. What is a normal distribution?
19. Why is the normal distribution a default choice for a prior over a set of real numbers?
20. What is the central limit theorem?
21. What are exponential and Laplace distribution?
22. What are Dirac distribution and Empirical distribution?
23. What is mixture of distributions?
24. Name two common examples of mixture of distributions? (Empirical and Gaussian Mixture)
25. Is Gaussian mixture model a universal approximator of densities?
26. Write the formulae for logistic and softplus function.
27. Write the formulae for Bayes rule.
28. What do you mean by measure zero and almost everywhere?
29. If two random variables are related in a deterministic way, how are the PDFs related?
30. Define self-information. What are its units?
31. What are Shannon entropy and differential entropy?
32. What is Kullback-Leibler (KL) divergence?
33. Can KL divergence be used as a distance measure?
34. Define cross-entropy.
35. What are structured probabilistic models or graphical models?
36. In the context of structured probabilistic models, what are directed and undirected models? How are they represented? What are cliques in undirected structured probabilistic models?


### Confidence interval 
[(Return back to Contents)](#Contents)

<img src="04_Confidence_Interval/04_Confidence_Interval.png">

1. What is population mean and sample mean?
2. What is population standard deviation and sample standard deviation?
3. Why population s.d. has N degrees of freedom while sample s.d. has N-1 degrees of freedom? In other words, why 1/N inside root for pop. s.d. and 1/(N-1) inside root for sample s.d.?
4. What is the formula for calculating the s.d. of the sample mean?
5. What is confidence interval?
6. What is standard error?



### Learning Theory 
[(Return back to Contents)](#Contents)

<img src="05_Learning_Theory/ml_learning_theory.png">

1. Describe bias and variance with examples.
2. What is Empirical Risk Minimization?
3. What is Union bound and Hoeffding’s inequality?
4. Write the formulae for training error and generalization error. Point out the differences.
5. State the uniform convergence theorem and derive it.
6. What is sample complexity bound of uniform convergence theorem?
7. What is error bound of uniform convergence theorem?
8. What is the bias-variance trade-off theorem?
9. From the bias-variance trade-off, can you derive the bound on training set size?
10. What is the VC dimension?
11. What does the training set size depend on for a finite and infinite hypothesis set? Compare and contrast.
12. What is the VC dimension for an n-dimensional linear classifier?
13. How is the VC dimension of a SVM bounded although it is projected to an infinite dimension?
14. Considering that Empirical Risk Minimization is a NP-hard problem, how does logistic regression and SVM loss work?



### Model and feature selection
[(Return back to Contents)](#Contents)

<img src="06_Feature_Engineering/06_Feature_Engineering.png">

1. Why are model selection methods needed?
2. How do you do a trade-off between bias and variance?
3. What are the different attributes that can be selected by model selection methods?
4. Why is cross-validation required?
5. Describe different cross-validation techniques.
6. What is hold-out cross validation? What are its advantages and disadvantages?
7. What is k-fold cross validation? What are its advantages and disadvantages?
8. What is leave-one-out cross validation? What are its advantages and disadvantages?
9. Why is feature selection required?
10. Describe some feature selection methods.
11. What is forward feature selection method? What are its advantages and disadvantages?
12. What is backward feature selection method? What are its advantages and disadvantages?
13. What is filter feature selection method and describe two of them?
14. What is mutual information and KL divergence?
15. Describe KL divergence intuitively.



### Curse of dimensionality
[(Return back to Contents)](#Contents)

<img src="07_Curse_Of_Dimensionality/Curse_Of_Dimensionality.jpeg">

1. Describe the curse of dimensionality with examples.
2. What is local constancy or smoothness prior or regularization?



### Universal approximation of neural networks
[(Return back to Contents)](#Contents)

<img src="08_Universal_Approximation_of_Neural_Networks/08_Universal_Approximation_of_Neural_Networks.png">

1. State the universal approximation theorem? What is the technique used to prove that?
2. What is a Borel measurable function?
3. Given the universal approximation theorem, why can’t a Multi Layer Perceptron (MLP) still reach an arbitrarily small positive error?



### Deep Learning motivation
[(Return back to Contents)](#Contents)

<img src="09_Deep_Learning_Motivation/09_Deep_Learning_Motivation.jpg">

1. What is the mathematical motivation of Deep Learning as opposed to standard Machine Learning techniques?
2. In standard Machine Learning vs. Deep Learning, how is the order of number of samples related to the order of regions that can be 3. recognized in the function space?
3. What are the reasons for choosing a deep model as opposed to shallow model?
4. How Deep Learning tackles the curse of dimensionality?



### Support Vector Machine
[(Return back to Contents)](#Contents)

<img src="10_Support_Vector_Machine/10_Support_Vector_Machine.png">

1. How can the SVM optimization function be derived from the logistic regression optimization function?
2. What is a large margin classifier?
3. Why SVM is an example of a large margin classifier?
4. SVM being a large margin classifier, is it influenced by outliers?
5. What is the role of C in SVM?
6. In SVM, what is the angle between the decision boundary and theta?
7. What is the mathematical intuition of a large margin classifier?
8. What is a kernel in SVM? Why do we use kernels in SVM?
9. What is a similarity function in SVM? Why it is named so?
10. How are the landmarks initially chosen in an SVM? How many and where?
11. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?
12. What is the difference between logistic regression and SVM without a kernel?
13. How does the SVM parameter C affect the bias/variance trade off?
14. How does the SVM kernel parameter sigma² affect the bias/variance trade off?
15. Can any similarity function be used for SVM?
16. Logistic regression vs. SVMs: When to use which one?



### Bayesian Machine Learning
[(Return back to Contents)](#Contents)

<img src="11_Bayesian_Machine_Learning/11_Bayesian_Machine_Learning.jpg">

1. What are the differences between “Bayesian” and “Freqentist” approach for Machine Learning?
2. Compare and contrast maximum likelihood and maximum a posteriori estimation.
3. How does Bayesian methods do automatic feature selection?
4. What do you mean by Bayesian regularization?
5. When will you use Bayesian methods instead of Frequentist methods?
6. Please explain Expectation-Maximization algorithm
7. What is Variational Inference?
8. What is Latent Dirichlet Allocation (LDA)?
9. What is Markov chain?



### Regularization
[(Return back to Contents)](#Contents)

<img src="12_Regularization/12_Regularization.png">

1. What is L1 regularization?
2. What is L2 regularization?
3. Compare L1 and L2 regularization.
4. Why does L1 regularization result in sparse models?
5. What is dropout?
6. How will you implement dropout during forward and backward pass?



### Evaluation of Machine Learning systems
[(Return back to Contents)](#Contents)

<img src="13_Evaluation_of_Machine_Learning_Systems/13_Evaluation_of_Machine_Learning_Systems.jpg">

1. What are accuracy, sensitivity, specificity, ROC?
2. What are precision and recall?
3. Describe t-test in the context of Machine Learning.



### Clustering
[(Return back to Contents)](#Contents)

<img src="14_Clustering/14_Clustering.png">

1. Describe the k-means algorithm.
2. What is distortion function? Is it convex or non-convex?
3. Tell me about the convergence of the distortion function.
4. Topic: EM algorithm
5. What is the Gaussian Mixture Model?
6. Describe the EM algorithm intuitively.
7. What are the two steps of the EM algorithm
8. Compare Gaussian Mixture Model and Gaussian Discriminant Analysis.



### Dimensionality Reduction
[(Return back to Contents)](#Contents)

<img src="15_Dimensionality_Reduction/15_Dimensionality_Reduction.png">

1. Why do we need dimensionality reduction techniques?
2. What do we need PCA and what does it do?
3. What is the difference between logistic regression and PCA?
4. What are the two pre-processing steps that should be applied before doing PCA?



### Basics of Natural Language Processing
[(Return back to Contents)](#Contents)

<img src="16_Basics_of_Natural_Language_Processing/16_Basics_of_Natural_Language_Processing.png">

1. What is WORD2VEC?
2. What is t-SNE? Why do we use PCA instead of t-SNE?
3. What is sampled softmax?
4. Why is it difficult to train a RNN with SGD?
5. How do you tackle the problem of exploding gradients?
6. What is the problem of vanishing gradients?
7. How do you tackle the problem of vanishing gradients?
8. Explain the memory cell of a LSTM.
9. What type of regularization do one use in LSTM?
10. What is Beam Search?
11. How to automatically caption an image?



### Some basic questions
[(Return back to Contents)](#Contents)

<img src="17_Some_basic_Questions/17_Some_basic_Questions.png">

1. Can you state Tom Mitchell’s definition of learning and discuss T, P and E?
2. What can be different types of tasks encountered in Machine Learning?
3. What are supervised, unsupervised, semi-supervised, self-supervised, multi-instance learning, and reinforcement learning?
4. Loosely how can supervised learning be converted into unsupervised learning and vice-versa?
5. Consider linear regression. What are T, P and E?
6. Derive the normal equation for linear regression.
7. What do you mean by affine transformation? Discuss affine vs. linear transformation.
8. Discuss training error, test error, generalization error, overfitting, and underfitting.
9. Compare representational capacity vs. effective capacity of a model.
Discuss VC dimension.
10. What are nonparametric models? What is nonparametric learning?
11. What is an ideal model? What is Bayes error? What is/are the source(s) of Bayes error occur?
12. What is the no free lunch theorem in connection to Machine Learning?
13. What is regularization? Intuitively, what does regularization do during the optimization procedure?
14. What is weight decay? What is it added?
15. What is a hyperparameter? How do you choose which settings are going to be hyperparameters and which are going to be learned?
16. Why is a validation set necessary?
17. What are the different types of cross-validation? When do you use which one?
18. What are point estimation and function estimation in the context of Machine Learning? What is the relation between them?
19. What is the maximal likelihood of a parameter vector $theta$? Where does the log come from?
20. Prove that for linear regression MSE can be derived from maximal likelihood by proper assumptions.
21. Why is maximal likelihood the preferred estimator in ML?
22. Under what conditions do the maximal likelihood estimator guarantee consistency?
23. What is cross-entropy of loss?
24. What is the difference between loss function, cost function and objective function?



### Optimization procedures
[(Return back to Contents)](#Contents)

<img src="18_Optimization_Procedures/18_Optimization_Procedures.png">

1. What is the difference between an optimization problem and a Machine Learning problem?
2. How can a learning problem be converted into an optimization problem?
3. What is empirical risk minimization? Why the term empirical? Why do we rarely use it in the context of deep learning?
4. Name some typical loss functions used for regression. Compare and contrast.
5. What is the 0–1 loss function? Why can’t the 0–1 loss function or classification error be used as a loss function for optimizing a deep neural network?



### Sequence Modeling
[(Return back to Contents)](#Contents)

<img src="19_Sequence_Modeling/19_Sequence_Modeling.jpg">

1. Write the equation describing a dynamical system. Can you unfold it? Now, can you use this to describe a RNN?
2. What determines the size of an unfolded graph?
3. What are the advantages of an unfolded graph?
4. What does the output of the hidden layer of a RNN at any arbitrary time t represent?
5. Are the output of hidden layers of RNNs lossless? If not, why?
6. RNNs are used for various tasks. From a RNNs point of view, what tasks are more demanding than others?
7. Discuss some examples of important design patterns of classical RNNs.
8. Write the equations for a classical RNN where hidden layer has recurrence. How would you define the loss in this case? What problems you might face while training it?
9. What is backpropagation through time?
10. Consider a RNN that has only output to hidden layer recurrence. What are its advantages or disadvantages compared to a RNN having only hidden to hidden recurrence?
11. What is Teacher forcing? Compare and contrast with BPTT.
12. What is the disadvantage of using a strict teacher forcing technique? How to solve this?
13. Explain the vanishing/exploding gradient phenomenon for recurrent neural networks.
14. Why don’t we see the vanishing/exploding gradient phenomenon in feedforward networks?
15. What is the key difference in architecture of LSTMs/GRUs compared to traditional RNNs?
16. What is the difference between LSTM and GRU?
17. Explain Gradient Clipping.
18. Adam and RMSProp adjust the size of gradients based on previously seen gradients. Do they inherently perform gradient clipping? If no, why?
19. Discuss RNNs in the context of Bayesian Machine Learning.
20. Can we do Batch Normalization in RNNs? If not, what is the alternative?



### Autoencoders
[(Return back to Contents)](#Contents)

<img src="20_Autoencoders/20_Autoencoders.png">

1. What is an Autoencoder? What does it “auto-encode”?
2. What were Autoencoders traditionally used for? Why there has been a resurgence of Autoencoders for generative modeling?
3. What is recirculation?
4. What loss functions are used for Autoencoders?
5. What is a linear autoencoder? Can it be optimal (lowest training reconstruction error)? If yes, under what conditions?
6. What is the difference between Autoencoders and PCA?
7. What is the impact of the size of the hidden layer in Autoencoders?
8. What is an undercomplete Autoencoder? Why is it typically used for?
9. What is a linear Autoencoder? Discuss it’s equivalence with PCA. Which one is better in reconstruction?
10. What problems might a nonlinear undercomplete Autoencoder face?
11. What are overcomplete Autoencoders? What problems might they face? Does the scenario change for linear overcomplete autoencoders?
12. Discuss the importance of regularization in the context of Autoencoders.
13. Why does generative autoencoders not require regularization?
14. What are sparse autoencoders?
15. What is a denoising autoencoder? What are its advantages? How does it solve the overcomplete problem?
16. What is score matching? Discuss it’s connections to DAEs.
17. Are there any connections between Autoencoders and RBMs?
18. What is manifold learning? How are denoising and contractive autoencoders equipped to do manifold learning?
19. What is a contractive autoencoder? Discuss its advantages. How does it solve the overcomplete problem?
20. Why is a contractive autoencoder named so?
21. What are the practical issues with CAEs? How to tackle them?
22. What is a stacked autoencoder? What is a deep autoencoder? Compare and contrast.
23. Compare the reconstruction quality of a deep autoencoder vs. PCA.
24. What is predictive sparse decomposition?
25. Discuss some applications of Autoencoders.



### Representation Learning
[(Return back to Contents)](#Contents)

<img src="21_Representation_Learning/21_Representation_Learning.png">

1. What is representation learning? Why is it useful?
2. What is the relation between Representation Learning and Deep Learning?
3. What is one-shot and zero-shot learning (Google’s NMT)? Give examples.
4. What trade offs does representation learning have to consider?
5. What is greedy layer-wise unsupervised pretraining (GLUP)? Why greedy? Why layer-wise? Why unsupervised? Why pretraining?
6. What were/are the purposes of the above technique? (deep learning problem and initialization)
7. Why does unsupervised pretraining work?
8. When does unsupervised training work? Under which circumstances?
9. Why might unsupervised pretraining act as a regularizer?
10. What is the disadvantage of unsupervised pretraining compared to other forms of unsupervised learning?
11. How do you control the regularizing effect of unsupervised pretraining?
12. How to select the hyperparameters of each stage of GLUP?



### Monte Carlo Methods
[(Return back to Contents)](#Contents)

<img src="22_Monte_Carlo_Methods/22_Monte_Carlo_Methods.png">

1. What are deterministic algorithms?
2. What are Las vegas algorithms?
3. What are deterministic approximate algorithms?
4. What are Monte Carlo algorithms?



### Generative Models
[(Return back to Contents)](#Contents)

<img src="23_Generative_Models/23_Generative_Models.png">

1. What is a Variational Autoencoder (VAE)?
2. How is VAE different from a regular Autoencoder?
3. Basics of GAN?
4. How do you train a GAN (Backpropagation)?
5. Cost function derivation?
6. What are the drawbacks for GAN?
7. Implement GAN with PyTorch
8. Implement GAN with Tensorflow



### Reinforcement Learning
[(Return back to Contents)](#Contents)

<img src="24_Reinforcement_Learning/24_Reinforcement_Learning.png">

1. What is the Reinforcement Learning?
2. Factors in Reinforcement Learning with Python
3. Types of Reinforcement Learning with Python
4. Positive Reinforcement Learning
5. Negative Reinforcement Learning
6. Reinforced Learning vs Supervised Learning
7. Decision Making
8. Dependency and Labels




Every resource needs to be broken down and put into separate sections (and cite where they came from).

    https://towardsdatascience.com/how-to-ace-data-science-interviews-statistics-f3d363ad47b

    Algorithms used on daily basis by data scientist: https://www.kdnuggets.com/2018/04/key-algorithms-statistical-models-aspiring-data-scientists.html

    http://houseofbots.com/news-detail/2851-4-this-is-what-i-really-do-as-a-data-scientist

    https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-7-unsupervised-learning-pca-and-clustering-db7879568417

    https://www.udemy.com/python-for-data-structures-algorithms-and-interviews/learn/v4/overview

    http://nbviewer.jupyter.org/github/jmportilla/Python-for-Algorithms--Data-Structures--and-Interviews/tree/master/

    https://towardsdatascience.com/data-science-and-machine-learning-interview-questions-3f6207cf040b

    useful: http://houseofbots.com/news-detail/2248-4-109-commonly-asked-data-science-interview-questions

    https://medium.com/acing-ai/google-ai-interview-questions-acing-the-ai-interview-1791ad7dc3ae

    https://medium.com/acing-ai

#### Questions on Deep Learning

* Design a network to detect two object classes if you know there is going to be only single instance of each object in the image. How the design changes if multiple, unknown number of instances are present? How the design and strategy changes if the number of object classes to be detected is huge ( > 10K)?
* Let me also share questions from published material which tests if the candidate is well prepared to understand the current literature/existing approaches. This is by no means an exhaustive list:
* Semantic segmentation, Object detection
* Explain max un-pooling operation for increasing the resolution of feature maps.
* What is a Learnable up-sampling or Transpose convolution ?
* Describe the transition between R-CNN, Fast R-CNN and Faster RCNN for object detection.
* Describe how RPNs are trained for prediction of region proposals in Faster R-CNN?
* Describe the approach in SSD and YOLO for object detection. How these approaches differ from Faster-RCNN. When will you use one over the other?
* Difference between Inception v3 and v4. How does Inception Resnet compare with V4.
* Explain main ideas behind ResNet? Why would you try ResNet over other architectures?
* Explain batch gradient descent, stochastic gradient descent and mini-match gradient descent.
* Loss functions: Cross-entropy, L2, L1
* Explain Dropout and Batch Normalization. Why Batch Normalization helps in faster convergence?
* Are neural networks and deep learning overrated?
* Can deep learning and neural networks be patented?
* What leadership questions should I expect from an Amazon on-site interview for a Software Engineering role?
* What is learning in neural network?
* What is the difference between Neural Networks and Deep Learning?
* Interestingly, this question as applied to Deep Learning does have a definitive answer for me, whereas the general form of the question may not.
* It is a relatively new topic in the general software engineering population. It has not yet been taught for years in college by professors who have extracted insightful ways to teach the fundamentals. So a lot knowledge here is gleaned from watching advanced talks and reading research papers. Unfortunately, this also means that many candidates have a strong functional knowledge of the state-of-the-art Whats and Hows, yet not fully mastering the Whys.
* So, I find that there are indeed "toughest NN and Deep Learning" questions, where many otherwise knowledgeable candidates fall down. They might give you technically correct answers, using lots of jargon, but never getting to the heart of the issue. They might give you an answer involving a lot of correct Hows, but that reveal they don't really understand the fundamental Whys. The best answers to these questions cannot (yet) be easily Googled. They are invariably of this pattern:
* Explain the following, so that a colleague new to the field/an eighth grader can understand (in no particular order, not exhaustive):
* What is an auto-encoder? Why do we "auto-encode"? Hint: it's really a misnomer.
* What is a Boltzmann Machine? Why a Boltzmann Machine?
* Why do we use sigmoid for an output function? Why tanh? Why not cosine? Why any function in particular?
* Why are CNNs used primarily in imaging and not so much other tasks?
* Explain backpropagation. Seriously. To the target audience described above.
* Is it OK to connect from a Layer 4 output back to a Layer 2 input?
* A data-scientist person recently put up a YouTube video explaining that the essential difference between a Neural Network and a Deep Learning network is that the former is trained from output back to input, while the latter is trained from input toward output. Do you agree? Explain.

* Try these yourself and see if you do indeed have mastery of the fundamentals. If you do, an eighth grader ought to be able to understand and repeat your explanation.
*
* I had some “deep learning interviews” recently, and I thought I could share some questions. First of all, be aware that most of the time, questions don’t have a single answer, and the interviewer just wants to talk with you to see if you are confident about the notions.
* Usually the first questions are : what do you know about some “pre-deep learning epoch” algorithms, like SVM, KNN, Kmeans, Random Forest…?
* Talking about deep learning, here are the questions I was asked to answer:
* Implement dropout during forward and backward pass?
* Was not very hard, you just have to consider what’s happening during testing vs training phase. In this question, the interviewer can test your knowledge on dropout, and backprop
* Neural network training loss/testing loss stays constant, what do you do?
* Open question (ask if there could be an error in your code, going deeper, going simpler…)
* Why do RNNs have a tendency to suffer from exploding/vanishing gradient?
* And probably you know the next question… How to prevent this? You can talk about LSTM cell which helps the gradient from vanishing, but make sure you know why it does so. I also remember having a nice conversation about gradient clipping, where we wonder whether we should clip the gradient element wise, or clip the norm of the gradient.
* Then I had a lot of question about some modern architecture, such as Do you know GAN, VAE, and memory augmented neural network? Can you talk about it?
* Of course, let me talk about the beauty of variational auto encoder.
* Some maths questions such as: Does using full batch means that the convergence is always better given unlimited power?
* What is the problem with sigmoid during backpropagation?
* Very small, between 0.25 and zero.[2]
* Given a black box machine learning algorithm that you can’t modify, how could you improve its error?
* Open question, you can transform the input for example.
* How to find the best hyper parameters?
* Random search, grid search, Bayesian search (and what it is?)
* What is transfer learning?
* I was also asked to implement some papers idea, but it was more as an assignment, than during an interview. Finally I also get non ML questions, more like algorithmic questions


* Good luck for your interview. If you are enough curious, and have a correct knowledge of the field, they will notice it, and you will pass a good moment with the interviewer.
*
* However, I do have some questions to test whether candidates really understand deep learning.
* Can they derive the back-propagation and weights update?
* Extend the above question to non-trivial layers such as convolutional layers, pooling layers, etc.
* How to implement dropout
* Their intuition when and why some tricks such as max pooling, ReLU, maxout, etc. work. There are no right answers but it helps to understand their thoughts and research experience.
* Can they abstract the forward, backward, update operations as matrix operations, to leverage BLAS and GPU?
* If a candidate shows early signs that he/she is an expert in DL, it's not necessary to ask all those questions in details. We can discuss one of their papers or a recent hot paper or something that is not necessarily DL.
* 86.3k Views · View 220 Upvoters
* Related Questions
* What are the learning algorithm in deep neural network?
* How can a neural network learn itself?
* Do you have to show your face during a Google Hangouts interview?
* What is neural networking?
* How can deep learning networks generate images?
* How do neural networks of neural networks behave?
* What is the best book or resource to learn about Neural Networks and Deep Neural Networks?
* What is the best YouTube channel to learn deep learning and neural networks?
* What topics come under deep learning other than neural networks?
* How do I implement deep neural network?
* What is the difference between neural networks and deep neural networks?
* Why are deep networks characterized by neural networks?
* ELI5: What are neural networks?
* What are neural networks in machine learning?
* What are the deep learning algorithms other than neural networks?
* Are neural networks and deep learning overrated?
* Can deep learning and neural networks be patented?
* What leadership questions should I expect from an Amazon on-site interview for a Software Engineering role?
* What is learning in neural network?
* What is the difference between Neural Networks and Deep Learning?
* What are the learning algorithm in deep neural network?
* How can a neural network learn itself?
* Do you have to show your face during a Google Hangouts interview?
* What is neural networking?
* How can deep learning networks generate images?
'
source    
[1]  https://www.quora.com/What-are-the-toughest-neural-networks-and-deep-learning-interview-questions'
[2] Is full-batch gradient descent, with unlimited computer power, always better than mini-batch gradient descent?

# ML-Interview
This is a list of resources I found useful during my preparation for interviews. Broadly speaking, I interviewd for three different profiles: Machine Learning Engineer, Applied Scientist and Quantitative Researcher. 

NOTE: All these profiles usually include multiple "traditional" programming/algorithm rounds, and for that, I relied upon mild leetcoding spree, spread over a period of 3 months. 

## Classical Machine Learning

+ A very good (slighly advanced) course on Machine Learning by Alex Smola. [Link](http://alex.smola.org/teaching/cmu2013-10-701/stats.html) 
+ Perhaps everything that you'll ever need to know for the interview sake. [Link](http://alumni.media.mit.edu/~tpminka/statlearn/glossary/) 
+ Generative vs Discriminative Classifiers (you should know the difference, and tradeoffs when choosing one over the other) [Link](http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf)
+ Gradient Boosted Trees [Link](https://web.njit.edu/~usman/courses/cs675_spring20/BoostedTree.pdf)
+ Gentle Introduction to Gradient Boosting [Link](https://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)
+ ROC and AUC (I like this video) [Link](https://www.youtube.com/watch?v=OAl6eAyP-yo&t=729s)
+ Clustering (from Ryan Tibshirani's Data Mining course, other slides are really good as well) [Link 1](https://www.stat.cmu.edu/~ryantibs/datamining/lectures/04-clus1.pdf) [Link 2](https://www.stat.cmu.edu/~ryantibs/datamining/lectures/05-clus2.pdf) [Link 3](https://www.stat.cmu.edu/~ryantibs/datamining/lectures/06-clus3.pdf)
+ Good old Linear Regression. [Link](https://www.cs.cmu.edu/~epxing/Class/10715/lectures/lecture2-LR.pdf)
+ L0, L1 and L2 regularization (Subset Selection, Lasso and Ridge Regression), a comparison. The Elements of
Statistical Learning, Trevor Hastie, Robert Tibshirani, Jerome Friedman, 2nd Edition, Section 3.4.3 [Link](https://web.stanford.edu/~hastie/ElemStatLearn/)
+ 
## Deep Learning
+ Why tanh for Recurrent Networks [Link](https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2015/slides/lec10.recurrent.pdf)
+ Receptive Fields in CNNs [Link](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)
+ For everything Convolution [Link](https://arxiv.org/pdf/1603.07285.pdf) 
+ For eveything Gradient Descent [Link](https://ruder.io/optimizing-gradient-descent/)
+ Adaptive Learning rates in SGD [Link](https://www.cs.cornell.edu/courses/cs6787/2019fa/lectures/Lecture8.pdf)
+ Backpropagation in Python, Andrej Karpathy [Link](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)

## Probability and Statistics
+ As the title would say, "Generalized Linear Models, abridged".[Link](https://bwlewis.github.io/GLM/)
+ A good course to cover Statistics [Link](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/) 
+ Basic Statistics: Introduction to Mathematical Statistics, Hogg, McKean and Craig, Chapters 1-4. [Link](https://www.amazon.com/Introduction-Mathematical-Statistics-Robert-Hogg/dp/0321795431)
+ Introduction to Hypothesis Testing: Introduction to Mathematical Statistics, Hogg, McKean and Craig, Section 4.5-4.6 [Link](https://www.amazon.com/Introduction-Mathematical-Statistics-Robert-Hogg/dp/0321795431)
+ Examples of Uncorrelated vs Independent Random Variable [Link](https://www.stat.cmu.edu/~cshalizi/uADA/13/reminders/uncorrelated-vs-independent.pdf)
+ Discrete time Markov Chains,Poisson Processes, Renewal Theory Adventures in Stochastic Processes, 2nd Edition, Sidney Resnick [Link](http://do.unicyb.kiev.ua/iksan/lectures/Adventures.pdf) TODO: Add a link to more succint notes.
+ Q-Q Plots [Link](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot)

## Large Scale Machine Learning
+ Distributed version of several algorithms. https://10605.github.io/spring2020/

## Assorted Mathematics
+ Some facts about Symmetric Matrices. [Link](http://www.doc.ic.ac.uk/~ae/papers/lecture05.pdf)
+ Bare minimum SVD by Gilbert Strang. [Link](https://mitocw.ups.edu.ec/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/singular-value-decomposition/MIT18_06SCF11_Ses3.5sum.pdf)
+ SVD and PCA in real-life. [Link](https://jeremykun.com/2011/07/27/eigenfaces/)
+ If you are not sure how SVD and PCA are related. [Link](https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca)
+ If you want to brush up on Chain Rule (or if you are like me and get confused between gradient and derivative notation) [Link](http://www.met.reading.ac.uk/~ross/Documents/Chain.pdf).
 [Wikipedia](https://en.wikipedia.org/wiki/Gradient#Derivative) has some useful information as well.
+ Collection of Quantitative Interview problems by Pete Benson, University of Michigan. [Link](https://pbenson.github.io/docs/quantTechnicalQuestions/quantTechnicalQuestions.pdf)
+ Cholesky Factorization [Link](http://www.math.sjsu.edu/~foster/m143m/cholesky.pdf)
+ QR Factorization [Link](https://www.math.usm.edu/lambers/mat610/sum10/lecture9.pdf)

## System Design (for ML) 
+ Structure for Google Index [Link](http://infolab.stanford.edu/~backrub/google.html)
+ Recommender Systems, Xavier Amatriain [Link 1](https://www.youtube.com/watch?v=mRToFXlNBpQ)
[Link 2](https://www.youtube.com/watch?v=bLhq63ygoU8)
+ News Feed Ranking @ Facebook (Lars Backstrom) [Link](https://www.youtube.com/watch?v=Xpx5RYNTQvg)

## Uncategorized
+ Sobel Operator [Link](https://en.wikipedia.org/wiki/Sobel_operator)
+ You have a fair Die, and you can choose to roll it up to 3 times. Whenever you decide to stop, the number that’s facing up is your score. What strategy would you choose to maximize your score? What is the expected score with your strategy? If you are given more than 3 chances, can you improve your score?


# Cracking the Data Science Interview

![Cover Page](coverpage.jpg)

Welcome to the *Cracking the Data Science Interview* Github page. Here you will find data science related links, tutorials, blog posts, code snippets, interview prep material, case studies, and more! Have fun!


# Data Science Interview Preparation Materials

# General
- [How Do I Prepare For a Data Science Interview (Quora)](https://www.quora.com/How-do-I-prepare-for-a-data-scientist-interview)

- [How to Ace a Data Science Interview (Blog)](https://alyaabbott.wordpress.com/2014/10/01/how-to-ace-a-data-science-interview/)

- [How To Learn Data Science If You’re Broke (Towards Data Science)](https://towardsdatascience.com/how-to-learn-data-science-if-youre-broke-7ecc408b53c7)

- [Data Science Interview Questions (PDF)](https://rstudio-pubs-static.s3.amazonaws.com/172473_91262a8a4188445a8b5e81d5d31c7731.html)

- [120 Data Science Interview Questions](https://github.com/kojino/120-Data-Science-Interview-Questions)

- [Kaggle Kernels](https://www.kaggle.com/kernels)

- [Data Science Cheatsheets (Github Repo)](https://github.com/abhat222/Data-Science--Cheat-Sheet)



# Online Resources for Practice

- [Leetcode (Over 1350 Qustions To Practice Coding)](https://leetcode.com)

- [HackerRank (Coding)](https://www.hackerrank.com/home?utm_expid=.2u09ecQTSny1HV02SEVoCg.1&utm_referrer=https%3A%2F%2Fwww.google.com%2F)

- [SQLZoo (Place to Practice SQL)](https://sqlzoo.net)

- [SQLCourse- Interactive Online SQL Training](http://www.sqlcourse.com)


# Data Science Interview Prep Material

## Mathematical Prequisites
### Statistics
- [Statistics for Data Science (Blog)](https://blog.floydhub.com/statistics-for-data-science/)

- [The Math Behind A/B Testing with Example Python Code (Towards Data Science)](https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f)


### Probability
- [Probability Cheatsheet (PDF)](http://www.wzchen.com/s/probability_cheatsheet.pdf), [(Github Repo)](https://github.com/wzchen/probability_cheatsheet)

- [Basics of Probability for Data Science explained with examples in R (Analytics Vidhya)](https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/?source=post_page-----2db4f651bd63----------------------)

- [What is an intuitive explanation of Bayes' Rule? (Quora)](https://www.quora.com/What-is-an-intuitive-explanation-of-Bayes-Rule)


### Linear Algebra
- [Linear Algebra Cheat Sheet for Deep Learning (Towards Data Science)](https://towardsdatascience.com/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)

- [No Bullshit Guide to Linear ALgebra- Linear algebra explained in four pages (PDF)](https://www.souravsengupta.com/cds2016/lectures/Savov_Notes.pdf)

## Computer Science

### Data Structures
- [A Data Scientist’s Guide to Data Structures & Algorithms, Part 1 (Towards Data Science)](https://towardsdatascience.com/a-data-scientists-guide-to-data-structures-algorithms-1176395015a0)

- [A Data Scientist’s Guide to Data Structures & Algorithms, Part 2 (Towards Data Science)](https://towardsdatascience.com/a-data-scientists-guide-to-data-structures-algorithms-part-2-6bc27066f3fe)

- [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook)

### Algorithms
- [Top Algorithms/Data Structures/Concepts every computer science student should know](https://medium.com/@codingfreak/top-algorithms-data-structures-concepts-every-computer-science-student-should-know-e0549c67b4ac)

### Databases
- [CAP Theorem (Wikipedia)](https://en.wikipedia.org/wiki/CAP_theorem)
- [Choosing The Right Database (Towards Data Science)](https://towardsdatascience.com/choosing-the-right-database-c45cd3a28f77)
### SQL
- [How To Ace Data Science Interviews: SQL (Towards Data Science)](https://towardsdatascience.com/how-to-ace-data-science-interviews-sql-b71de212e433)

### Python Packages/Libraries

- [Pandas](https://pandas.pydata.org/)

- [NumPy](http://www.numpy.org/)

- [SciPy](https://www.scipy.org/index.html)

- [Scikit-learn](https://scikit-learn.org/stable/)

- [Statsmodels](http://www.statsmodels.org/stable/index.html#)

- [PySpark](https://spark.apache.org/docs/latest/api/python/index.html)

- [Matplotlib](http://www.statsmodels.org/stable/index.html#)

- [IPython](http://ipython.org)

- [SymPy](https://www.sympy.org/en/index.html)

## Data Wrangling
- [Fundamental Techniques of Feature Engineering for Machine Learning](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)

- [The Ultimate Guide to Data Cleaning](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)







## Machine Learning

- [A Tour of Machine Learning Algorithms (Blog)](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)

- [41 Essential Machine Learning Interview Questions (Blog)](https://www.springboard.com/blog/machine-learning-interview-questions/?source=post_page-----2db4f651bd63----------------------)
### Supervised Learning Algorithms

- [Linear Regression — Detailed View](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)

- [7 Regression Techniques you should know!](https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/)

- [Naive Bayes Classification — Theory](https://medium.com/machine-learning-101/chapter-1-supervised-learning-and-naive-bayes-classification-part-1-theory-8b9e361897d5)

- [SVM (Support Vector Machine) — Theory](https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72)

- [Decision Trees - Explained, Demystified and Simplified](https://adityashrm21.github.io/Decision-Trees/)

- [An Implementation and Explanation of the Random Forest in Python](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)

- [A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)

- [A Gentle Introduction to XGBoost for Applied Machine Learning](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)

### Unsupervised Learning Algorithms

- [Unsupervised Learning and Data Clustering](https://towardsdatascience.com/unsupervised-learning-and-data-clustering-eeecb78b422a)

- [Understanding K-means Clustering in Machine Learning](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)

- [Hierarchical Clustering](https://www.saedsayad.com/clustering_hierarchical.htm)

- [Introduction to Autoencoders](https://www.jeremyjordan.me/autoencoders/)

- [An Intuitive Introduction to Generative Adversarial Networks](http://blog.kaggle.com/2018/01/18/an-intuitive-introduction-to-generative-adversarial-networks/)


### Reinforcement Learning Algorithms

- [Applications of Reinforcement Learning in Real World (Towards Data Science)](https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12)

- [Open AI Gym](https://gym.openai.com)

- [Simple Reinforcement Learning Methods to Learn CartPole (Blog)](http://kvfrans.com/simple-algoritms-for-solving-cartpole/)

- [An introduction to Policy Gradients with Cartpole and Doom (freeCodeCamp)](https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/)

- [Implementation of Reinforcement Learning Algorithms (Github Repo)](https://github.com/dennybritz/reinforcement-learning)

# Miscellaneous

- [A Gentle Introduction To Graph Theory](https://medium.com/basecs/a-gentle-introduction-to-graph-theory-77969829ead8)

- [How to get started with machine learning on graphs](https://medium.com/octavian-ai/how-to-get-started-with-machine-learning-on-graphs-7f0795c83763)

- [Solving the Knapsack Problem with Dynamic Programming](https://dev.to/downey/solving-the-knapsack-problem-with-dynamic-programming-4hce)

- [An Overview of Monte Carlo Methods](https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694)

- [Introductory Guide on Linear Programming](https://www.analyticsvidhya.com/blog/2017/02/lintroductory-guide-on-linear-programming-explained-in-simple-english/)

- [ARIMA Models](http://www.forecastingsolutions.com/arima.html)

# Product 
- [Key Performance Indicators](https://www.shopify.com/blog/7365564-32-key-performance-indicators-kpis-for-ecommerce)


# Data Science Case Studies
- [Uber Data Science](https://www.uber.com/us/en/careers/teams/data-science/)

- [BCG Gamma](https://www.bcg.com/beyond-consulting/bcg-gamma/default.aspx)

- [McKinsey Solutions](https://www.mckinsey.com/solutions)

- [Bain Advanced Analytics](https://www.bain.com/vector-digital/advanced-analytics/)

- [Facebook Data Science](https://research.fb.com/teams/core-data-science/)

- [Kaggle](https://www.kaggle.com)

# Books
- [Cracking the Coding Interview](https://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/0984782850/ref=sr_1_1?keywords=cracking+the+co&qid=1575944295&sr=8-1)

- [The Hundred-Page Machine Learning Book ](https://www.amazon.com/Hundred-Page-Machine-Learning-Book/dp/199957950X/ref=pd_lutyp_crtyp_simh_1_7?_encoding=UTF8&pd_rd_i=199957950X&pd_rd_r=fbee59df-96ce-45b6-a41d-dcb9a769ad94&pd_rd_w=Dpj0r&pd_rd_wg=EokMX&pf_rd_p=11bf186d-590b-449a-8161-5414a5d28305&pf_rd_r=X99FPRSYC7DW3CXYJHCK&psc=1&refRID=X99FPRSYC7DW3CXYJHCK)

- [Deep Learning](http://www.deeplearningbook.org)

- [Practical Statistics for Data Scientists: 50 Essential Concepts](https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential-ebook/dp/B071NVDFD6/ref=pd_sim_351_1/147-3762810-2360818?_encoding=UTF8&pd_rd_i=B071NVDFD6&pd_rd_r=52496a3c-32d4-4d5c-a873-0b832ff9b0a4&pd_rd_w=FohrU&pd_rd_wg=lIxdZ&pf_rd_p=04d27813-a1f2-4e7b-a32b-b5ab374ce3f9&pf_rd_r=NWTQ82150B8GMD6P43DD&psc=1&refRID=NWTQ82150B8GMD6P43DD)

- [Introduction to Algorithms](https://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844)

# Blogs

- [Machine Learning Mastery](https://machinelearningmastery.com)

- [GeeksforGeeks](https://www.geeksforgeeks.org)

- [No Free Hunch- The Official Blog of Kaggle.com](http://blog.kaggle.com)

- [Towards Data Science](https://towardsdatascience.com)

- [The Art of Data Science](https://www.quora.com/q/art-of-data-science)


<a href="https://www.buymeacoffee.com/khalel" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" ></a>

# Machine Learning Interview Questions
A collection of technical interview questions for machine learning and computer vision engineering positions.

#### 1) What's the trade-off between bias and variance? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]

If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data. [[src]](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)

#### 2) What is gradient descent? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
[[Answer]](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)

Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).

Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.

#### 3) Explain over- and under-fitting and how to combat them? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
[[Answer]](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765)

#### 4) How do you combat the curse of dimensionality? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]

 - Manual Feature Selection
 - Principal Component Analysis (PCA)
 - Multidimensional Scaling
 - Locally linear embedding  
[[src]](https://towardsdatascience.com/why-and-how-to-get-rid-of-the-curse-of-dimensionality-right-with-breast-cancer-dataset-7d528fb5f6c0)

#### 5) What is regularization, why do we use it, and give some examples of common methods? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
A technique that discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. 
Examples
 - Ridge (L2 norm)
 - Lasso (L1 norm)  
The obvious *disadvantage* of **ridge** regression, is model interpretability. It will shrink the coefficients for least important predictors, very close to zero. But it will never make them exactly zero. In other words, the *final model will include all predictors*. However, in the case of the **lasso**, the L1 penalty has the effect of forcing some of the coefficient estimates to be *exactly equal* to zero when the tuning parameter λ is sufficiently large. Therefore, the lasso method also performs variable selection and is said to yield sparse models.
[[src]](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)

#### 6) Explain Principal Component Analysis (PCA)? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
[[Answer]](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)

#### 7) Why is ReLU better and more often used than Sigmoid in Neural Networks? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
Imagine a network with random initialized weights ( or normalised ) and almost 50% of the network yields 0 activation because of the characteristic of ReLu ( output 0 for negative values of x ). This means a fewer neurons are firing ( sparse activation ) and the network is lighter. [[src]](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)


#### 8) Given stride S and kernel sizes  for each layer of a (1-dimensional) CNN, create a function to compute the [receptive field](https://www.quora.com/What-is-a-receptive-field-in-a-convolutional-neural-network) of a particular node in the network. This is just finding how many input nodes actually connect through to a neuron in a CNN. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

The receptive field are defined portion of space within an inputs that will be used during an operation to generate an output.

Considering a CNN filter of size k, the receptive field of a peculiar layer is only the number of input used by the filter, in this case k, multiplied by the dimension of the input that is not being reduced by the convolutionnal filter a. This results in a receptive field of k*a.

More visually, in the case of an image of size 32x32x3, with a CNN with a filter size of 5x5, the corresponding recpetive field will be the the filter size, 5 multiplied by the depth of the input volume (the RGB colors) which is the color dimensio. This thus gives us a recpetive field of dimension 5x5x3.

#### 9) Implement [connected components](http://aishack.in/tutorials/labelling-connected-components-example/) on an image/matrix. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]


#### 10) Implement a sparse matrix class in C++. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

[[Answer]](https://www.geeksforgeeks.org/sparse-matrix-representation/)

#### 11) Create a function to compute an [integral image](https://en.wikipedia.org/wiki/Summed-area_table), and create another function to get area sums from the integral image.[[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

[[Answer]](https://www.geeksforgeeks.org/submatrix-sum-queries/)

#### 12) How would you remove outliers when trying to estimate a flat plane from noisy samples? [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates.
[[src]](https://en.wikipedia.org/wiki/Random_sample_consensus)



#### 13) How does [CBIR](https://www.robots.ox.ac.uk/~vgg/publications/2013/arandjelovic13/arandjelovic13.pdf) work? [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

[[Answer]](https://en.wikipedia.org/wiki/Content-based_image_retrieval)
Content-based image retrieval is the concept of using images to gather metadata on their content. Compared to the current image retrieval approach based on the keywords associated to the images, this technique generates its metadata from computer vision techniques to extract the relevant informations that will be used during the querying step. Many approach are possible from feature detection to retrieve keywords to the usage of CNN to extract dense features that will be associated to a known distribution of keywords. 

With this last approach, we care less about what is shown on the image but more about the similarity between the metadata generated by a known image and a list of known label and or tags projected into this metadata space.

#### 14) How does image registration work? Sparse vs. dense [optical flow](http://www.ncorr.com/download/publications/bakerunify.pdf) and so on. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

#### 15) Describe how convolution works. What about if your inputs are grayscale vs RGB imagery? What determines the shape of the next layer? [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

[[Answer]](https://dev.to/sandeepbalachandran/machine-learning-convolution-with-color-images-2p41)

#### 16) Talk me through how you would create a 3D model of an object from imagery and depth sensor measurements taken at all angles around the object. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

#### 17) Implement SQRT(const double & x) without using any special functions, just fundamental arithmetic. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

The taylor series can be used for this step by providing an approximation of sqrt(x):

[[Answer]](https://math.stackexchange.com/questions/732540/taylor-series-of-sqrt1x-using-sigma-notation)

#### 18) Reverse a bitstring. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

If you are using python3 :

```
data = b'\xAD\xDE\xDE\xC0'
my_data = bytearray(data)
my_data.reverse()
```
#### 19) Implement non maximal suppression as efficiently as you can. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]



#### 20) Reverse a linked list in place. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]

[[Answer]](https://www.geeksforgeeks.org/reverse-a-linked-list/)

#### 21) What is data normalization and why do we need it? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
Data normalization is very important preprocessing step, used to rescale values to fit in a specific range to assure better convergence during backpropagation. In general, it boils down to subtracting the mean of each data point and dividing by its standard deviation. If we don't do this then some of the features (those with high magnitude) will be weighted more in the cost function (if a higher-magnitude feature changes by 1%, then that change is pretty big, but for smaller features it's quite insignificant). The data normalization makes all features weighted equally.

#### 22) Why do we use convolutions for images rather than just FC layers? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
Firstly, convolutions preserve, encode, and actually use the spatial information from the image. If we used only FC layers we would have no relative spatial information. Secondly, Convolutional Neural Networks (CNNs) have a partially built-in translation in-variance, since each convolution kernel acts as it's own filter/feature detector.

#### 23) What makes CNNs translation invariant? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
As explained above, each convolution kernel acts as it's own filter/feature detector. So let's say you're doing object detection, it doesn't matter where in the image the object is since we're going to apply the convolution in a sliding window fashion across the entire image anyways.

#### 24) Why do we have max-pooling in classification CNNs? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
for a role in Computer Vision. Max-pooling in a CNN allows you to reduce computation since your feature maps are smaller after the pooling. You don't lose too much semantic information since you're taking the maximum activation. There's also a theory that max-pooling contributes a bit to giving CNNs more translation in-variance. Check out this great video from Andrew Ng on the [benefits of max-pooling](https://www.coursera.org/learn/convolutional-neural-networks/lecture/hELHk/pooling-layers).

#### 25) Why do segmentation CNNs typically have an encoder-decoder style / structure? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
The encoder CNN can basically be thought of as a feature extraction network, while the decoder uses that information to predict the image segments by "decoding" the features and upscaling to the original image size.

#### 26) What is the significance of Residual Networks? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
The main thing that residual connections did was allow for direct feature access from previous layers. This makes information propagation throughout the network much easier. One very interesting paper about this shows how using local skip connections gives the network a type of ensemble multi-path structure, giving features multiple paths to propagate throughout the network.

#### 27) What is batch normalization and why does it work? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. The idea is then to normalize the inputs of each layer in such a way that they have a mean output activation of zero and standard deviation of one. This is done for each individual mini-batch at each layer i.e compute the mean and variance of that mini-batch alone, then normalize. This is analogous to how the inputs to networks are standardized. How does this help? We know that normalizing the inputs to a network helps it learn. But a network is just a series of layers, where the output of one layer becomes the input to the next. That means we can think of any layer in a neural network as the first layer of a smaller subsequent network. Thought of as a series of neural networks feeding into each other, we normalize the output of one layer before applying the activation function, and then feed it into the following layer (sub-network).

#### 28) Why would you use many small convolutional kernels such as 3x3 rather than a few large ones? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]
This is very well explained in the [VGGNet paper](https://arxiv.org/pdf/1409.1556.pdf). There are 2 reasons: First, you can use several smaller kernels rather than few large ones to get the same receptive field and capture more spatial context, but with the smaller kernels you are using less parameters and computations. Secondly, because with smaller kernels you will be using more filters, you'll be able to use more activation functions and thus have a more discriminative mapping function being learned by your CNN.

#### 29) Why do we need a validation set and test set? What is the difference between them? [[src](https://www.toptal.com/machine-learning/interview-questions)]
When training a model, we divide the available data into three separate sets:

 - The training dataset is used for fitting the model’s parameters. However, the accuracy that we achieve on the training set is not reliable for predicting if the model will be accurate on new samples.
 - The validation dataset is used to measure how well the model does on examples that weren’t part of the training dataset. The metrics computed on the validation data can be used to tune the hyperparameters of the model. However, every time we evaluate the validation data and we make decisions based on those scores, we are leaking information from the validation data into our model. The more evaluations, the more information is leaked. So we can end up overfitting to the validation data, and once again the validation score won’t be reliable for predicting the behaviour of the model in the real world.
 - The test dataset is used to measure how well the model does on previously unseen examples. It should only be used once we have tuned the parameters using the validation set.

So if we omit the test set and only use a validation set, the validation score won’t be a good estimate of the generalization of the model.

#### 30) What is stratified cross-validation and when should we use it? [[src](https://www.toptal.com/machine-learning/interview-questions)]
Cross-validation is a technique for dividing data between training and validation sets. On typical cross-validation this split is done randomly. But in stratified cross-validation, the split preserves the ratio of the categories on both the training and validation datasets.

For example, if we have a dataset with 10% of category A and 90% of category B, and we use stratified cross-validation, we will have the same proportions in training and validation. In contrast, if we use simple cross-validation, in the worst case we may find that there are no samples of category A in the validation set.

Stratified cross-validation may be applied in the following scenarios:

 - On a dataset with multiple categories. The smaller the dataset and the more imbalanced the categories, the more important it will be to use stratified cross-validation.
 - On a dataset with data of different distributions. For example, in a dataset for autonomous driving, we may have images taken during the day and at night. If we do not ensure that both types are present in training and validation, we will have generalization problems.

#### 31) Why do ensembles typically have higher scores than individual models? [[src](https://www.toptal.com/machine-learning/interview-questions)]
An ensemble is the combination of multiple models to create a single prediction. The key idea for making better predictions is that the models should make different errors. That way the errors of one model will be compensated by the right guesses of the other models and thus the score of the ensemble will be higher.

We need diverse models for creating an ensemble. Diversity can be achieved by:
 - Using different ML algorithms. For example, you can combine logistic regression, k-nearest neighbors, and decision trees.
 - Using different subsets of the data for training. This is called bagging.
 - Giving a different weight to each of the samples of the training set. If this is done iteratively, weighting the samples according to the errors of the ensemble, it’s called boosting.
Many winning solutions to data science competitions are ensembles. However, in real-life machine learning projects, engineers need to find a balance between execution time and accuracy.

#### 32) What is an imbalanced dataset? Can you list some ways to deal with it? [[src](https://www.toptal.com/machine-learning/interview-questions)]
An imbalanced dataset is one that has different proportions of target categories. For example, a dataset with medical images where we have to detect some illness will typically have many more negative samples than positive samples—say, 98% of images are without the illness and 2% of images are with the illness.

There are different options to deal with imbalanced datasets:
 - Oversampling or undersampling. Instead of sampling with a uniform distribution from the training dataset, we can use other distributions so the model sees a more balanced dataset.
 - Data augmentation. We can add data in the less frequent categories by modifying existing data in a controlled way. In the example dataset, we could flip the images with illnesses, or add noise to copies of the images in such a way that the illness remains visible.
 - Using appropriate metrics. In the example dataset, if we had a model that always made negative predictions, it would achieve a precision of 98%. There are other metrics such as precision, recall, and F-score that describe the accuracy of the model better when using an imbalanced dataset.

#### 33) Can you explain the differences between supervised, unsupervised, and reinforcement learning? [[src](https://www.toptal.com/machine-learning/interview-questions)]
In supervised learning, we train a model to learn the relationship between input data and output data. We need to have labeled data to be able to do supervised learning.

With unsupervised learning, we only have unlabeled data. The model learns a representation of the data. Unsupervised learning is frequently used to initialize the parameters of the model when we have a lot of unlabeled data and a small fraction of labeled data. We first train an unsupervised model and, after that, we use the weights of the model to train a supervised model.

In reinforcement learning, the model has some input data and a reward depending on the output of the model. The model learns a policy that maximizes the reward. Reinforcement learning has been applied successfully to strategic games such as Go and even classic Atari video games.

#### 34) What is data augmentation? Can you give some examples? [[src](https://www.toptal.com/machine-learning/interview-questions)]
Data augmentation is a technique for synthesizing new data by modifying existing data in such a way that the target is not changed, or it is changed in a known way.

Computer vision is one of fields where data augmentation is very useful. There are many modifications that we can do to images:
 - Resize
 - Horizontal or vertical flip
 - Rotate
 - Add noise
 - Deform
 - Modify colors
Each problem needs a customized data augmentation pipeline. For example, on OCR, doing flips will change the text and won’t be beneficial; however, resizes and small rotations may help.

#### 35) What is Turing test? [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
The Turing test is a method to test the machine’s ability to match the human level intelligence. A machine is used to challenge the human intelligence that when it passes the test, it is considered as intelligent. Yet a machine could be viewed as intelligent without sufficiently knowing about people to mimic a human.

#### 36) What is Precision?  
Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances  
Precision = true positive / (true positive + false positive)  
[[src]](https://en.wikipedia.org/wiki/Precision_and_recall)

#### 37) What is Recall?  
Recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.
Recall = true positive / (true positive + false negative)  
[[src]](https://en.wikipedia.org/wiki/Precision_and_recall)

#### 38) Define F1-score. [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
It is the weighted average of precision and recall. It considers both false positive and false negative into account. It is used to measure the model’s performance.  
F1-Score = 2 * (precision * recall) / (precision + recall)

#### 39) What is cost function? [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
Cost function is a scalar functions which Quantifies the error factor of the Neural Network. Lower the cost function better the Neural network. Eg: MNIST Data set to classify the image, input image is digit 2 and the Neural network wrongly predicts it to be 3

#### 40) List different activation neurons or functions. [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
 - Linear Neuron
 - Binary Threshold Neuron
 - Stochastic Binary Neuron
 - Sigmoid Neuron
 - Tanh function
 - Rectified Linear Unit (ReLU)

#### 41) Define Learning Rate.
Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. [[src](https://en.wikipedia.org/wiki/Learning_rate)]

#### 42) What is Momentum (w.r.t NN optimization)?
Momentum lets the optimization algorithm remembers its last step, and adds some proportion of it to the current step. This way, even if the algorithm is stuck in a flat region, or a small local minimum, it can get out and continue towards the true minimum. [[src]](https://www.quora.com/What-is-the-difference-between-momentum-and-learning-rate)

#### 43) What is the difference between Batch Gradient Descent and Stochastic Gradient Descent?
Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.

Stochastic gradient descent (SGD) computes the gradient using a single sample. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. [[src]](https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent)

#### 44) Epoch vs. Batch vs. Iteration.
 - **Epoch**: one forward pass and one backward pass of **all** the training examples  
 - **Batch**: examples processed together in one pass (forward and backward)  
 - **Iteration**: number of training examples / Batch size  

#### 45) What is vanishing gradient? [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
As we add more and more hidden layers, back propagation becomes less and less useful in passing information to the lower layers. In effect, as information is passed back, the gradients begin to vanish and become small relative to the weights of the networks.

#### 46) What are dropouts? [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
Dropout is a simple way to prevent a neural network from overfitting. It is the dropping out of some of the units in a neural network. It is similar to the natural reproduction process, where the nature produces offsprings by combining distinct genes (dropping out others) rather than strengthening the co-adapting of them.

#### 47) Define LSTM. [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
Long Short Term Memory – are explicitly designed to address the long term dependency problem, by maintaining a state what to remember and what to forget.

#### 48) List the key components of LSTM. [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
 - Gates (forget, Memory, update & Read)
 - tanh(x) (values between -1 to 1)
 - Sigmoid(x) (values between 0 to 1)

#### 49) List the variants of RNN. [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
 - LSTM: Long Short Term Memory
 - GRU: Gated Recurrent Unit
 - End to End Network
 - Memory Network

#### 50) What is Autoencoder, name few applications. [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
Auto encoder is basically used to learn a compressed form of given data. Few applications include
 - Data denoising
 - Dimensionality reduction
 - Image reconstruction
 - Image colorization

#### 51) What are the components of GAN? [[src](https://intellipaat.com/interview-question/artificial-intelligence-interview-questions/)]
 - Generator
 - Discriminator

#### 52) What's the difference between boosting and bagging?
Boosting and bagging are similar, in that they are both ensembling techniques, where a number of weak learners (classifiers/regressors that are barely better than guessing) combine (through averaging or max vote) to create a strong learner that can make accurate predictions. Bagging means that you take bootstrap samples (with replacement) of your data set and each sample trains a (potentially) weak learner. Boosting, on the other hand, uses all data to train each learner, but instances that were misclassified by the previous learners are given more weight so that subsequent learners give more focus to them during training. [[src]](https://www.quora.com/Whats-the-difference-between-boosting-and-bagging)

#### 53) Explain how a ROC curve works. [[src]](https://www.springboard.com/blog/machine-learning-interview-questions/)
The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).

#### 54) What’s the difference between Type I and Type II error? [[src]](https://www.springboard.com/blog/machine-learning-interview-questions/)
Type I error is a false positive, while Type II error is a false negative. Briefly stated, Type I error means claiming something has happened when it hasn’t, while Type II error means that you claim nothing is happening when in fact something is.
A clever way to think about this is to think of Type I error as telling a man he is pregnant, while Type II error means you tell a pregnant woman she isn’t carrying a baby.

#### 55) What’s the difference between a generative and discriminative model? [[src]](https://www.springboard.com/blog/machine-learning-interview-questions/)
A generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks.

#### 56) Instance-Based Versus Model-Based Learning.

 - **Instance-based Learning**: The system learns the examples by heart, then generalizes to new cases using a similarity measure.

 - **Model-based Learning**: Another way to generalize from a set of examples is to build a model of these examples, then use that model to make predictions. This is called model-based learning.
[[src]](https://medium.com/@sanidhyaagrawal08/what-is-instance-based-and-model-based-learning-s1e10-8e68364ae084)


#### 57) When to use a Label Encoding vs. One Hot Encoding?

This question generally depends on your dataset and the model which you wish to apply. But still, a few points to note before choosing the right encoding technique for your model:

We apply One-Hot Encoding when:

- The categorical feature is not ordinal (like the countries above)
- The number of categorical features is less so one-hot encoding can be effectively applied
We apply Label Encoding when:

- The categorical feature is ordinal (like Jr. kg, Sr. kg, Primary school, high school)
- The number of categories is quite large as one-hot encoding can lead to high memory consumption

[[src]](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/)

#### 58) What is the difference between LDA and PCA for dimensionality reduction?

Both LDA and PCA are linear transformation techniques: LDA is a supervised whereas PCA is unsupervised – PCA ignores class labels.

We can picture PCA as a technique that finds the directions of maximal variance. In contrast to PCA, LDA attempts to find a feature subspace that maximizes class separability.

[[src]](https://sebastianraschka.com/faq/docs/lda-vs-pca.html)

#### 59) What is t-SNE?

t-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data. In simpler terms, t-SNE gives you a feel or intuition of how the data is arranged in a high-dimensional space. 

[[src]](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1)

#### 60) What is the difference between t-SNE and PCA for dimensionality reduction?

The first thing to note is that PCA was developed in 1933 while t-SNE was developed in 2008. A lot has changed in the world of data science since 1933 mainly in the realm of compute and size of data. Second, PCA is a linear dimension reduction technique that seeks to maximize variance and preserves large pairwise distances. In other words, things that are different end up far apart. This can lead to poor visualization especially when dealing with non-linear manifold structures. Think of a manifold structure as any geometric shape like: cylinder, ball, curve, etc.

t-SNE differs from PCA by preserving only small pairwise distances or local similarities whereas PCA is concerned with preserving large pairwise distances to maximize variance.

[[src]](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1)

#### 61) What is UMAP?

UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data.

[[src]](https://arxiv.org/abs/1802.03426#:~:text=UMAP%20)

#### 62) What is the difference between t-SNE and UMAP for dimensionality reduction?

The biggest difference between the the output of UMAP when compared with t-SNE is this balance between local and global structure - UMAP is often better at preserving global structure in the final projection. This means that the inter-cluster relations are potentially more meaningful than in t-SNE. However, it's important to note that, because UMAP and t-SNE both necessarily warp the high-dimensional shape of the data when projecting to lower dimensions, any given axis or distance in lower dimensions still isn’t directly interpretable in the way of techniques such as PCA.

[[src]](https://pair-code.github.io/understanding-umap/)

#### 63) How Random Number Generator Works, e.g. rand() function in python works?
It generates a pseudo random number based on the seed and there are some famous algorithm, please see below link for further information on this.
[[src]](https://en.wikipedia.org/wiki/Linear_congruential_generator)


## Contributions
Contributions are most welcomed.
 1. Fork the repository.
 2. Commit your *questions* or *answers*.
 3. Open **pull request**.


[Source](https://docs.google.com/document/d/1ajyJhXyt4q9ZsufXV1kZxDH_3Isg3MYAKsFqNytXrCw/)

- [1. Why do you use feature selection?](#1-why-do-you-use-feature-selection)
    - [Filter Methods](#filter-methods)
    - [Embedded Methods](#embedded-methods)
    - [Misleading](#misleading)
    - [Overfitting](#overfitting)
- [2. Explain what regularization is and why it is useful.](#2-explain-what-regularization-is-and-why-it-is-useful)
- [3. What’s the difference between L1 and L2 regularization?](#3-whats-the-difference-between-l1-and-l2-regularization)
- [4. How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression?](#4-how-would-you-validate-a-model-you-created-to-generate-a-predictive-model-of-a-quantitative-outcome-variable-using-multiple-regression)
- [5. Explain what precision and recall are. How do they relate to the ROC curve?](#5-explain-what-precision-and-recall-are-how-do-they-relate-to-the-roc-curve)
- [6. Is it better to have too many false positives, or too many false negatives?](#6-is-it-better-to-have-too-many-false-positives--or-too-many-false-negatives)
- [7. How do you deal with unbalanced binary classification?](#7-how-do-you-deal-with-unbalanced-binary-classification)
- [8. What is statistical power?](#8-what-is-statistical-power)
- [9. What are bias and variance, and what are their relation to modeling data?](#9-what-are-bias-and-variance--and-what-are-their-relation-to-modeling-data)
    - [Approaches](#approaches)
- [10. What if the classes are imbalanced? What if there are more than 2 groups?](#10-what-if-the-classes-are-imbalanced-what-if-there-are-more-than-2-groups)
- [11. What are some ways I can make my model more robust to outliers?](#11-what-are-some-ways-i-can-make-my-model-more-robust-to-outliers)
- [12. In unsupervised learning, if a ground truth about a dataset is unknown, how can we determine the most useful number of clusters to be?](#12-in-unsupervised-learning--if-a-ground-truth-about-a-dataset-is-unknown--how-can-we-determine-the-most-useful-number-of-clusters-to-be)
- [13. Define variance](#13-define-variance)
- [14. Expected value](#14-expected-value)
- [15. Describe the differences between and use cases for box plots and histograms](#15-describe-the-differences-between-and-use-cases-for-box-plots-and-histograms)
- [16. How would you find an anomaly in a distribution?](#16-how-would-you-find-an-anomaly-in-a-distribution)
    - [Statistical methods](#statistical-methods)
    - [Metric methods](#metric-methods)
- [17. How do you deal with outliers in your data?](#17-how-do-you-deal-with-outliers-in-your-data)
- [18. How do you deal with sparse data?](#18-how-do-you-deal-with-sparse-data)
- [19. Big Data Engineer Can you explain what REST is?](#19-big-data-engineer-can-you-explain-what-rest-is)
- [20. Logistic regression](#20-logistic-regression)
- [21. What is the effect on the coefficients of logistic regression if two predictors are highly correlated? What are the confidence intervals of the coefficients?](#21-what-is-the-effect-on-the-coefficients-of-logistic-regression-if-two-predictors-are-highly-correlated-what-are-the-confidence-intervals-of-the-coefficients)
- [22. What’s the difference between Gaussian Mixture Model and K-Means?](#22-whats-the-difference-between-gaussian-mixture-model-and-k-means)
- [23. Describe how Gradient Boosting works.](#23-describe-how-gradient-boosting-works)
  - [AdaBoost the First Boosting Algorithm](#adaboost-the-first-boosting-algorithm)
    - [Loss Function](#loss-function)
    - [Weak Learner](#weak-learner)
    - [Additive Model](#additive-model)
  - [Improvements to Basic Gradient Boosting](#improvements-to-basic-gradient-boosting)
    - [Tree Constraints](#tree-constraints)
    - [Weighted Updates](#weighted-updates)
    - [Stochastic Gradient Boosting](#stochastic-gradient-boosting)
    - [Penalized Gradient Boosting](#penalized-gradient-boosting)
- [24. Difference between AdaBoost and XGBoost](#24-difference-between-AdaBoost-and-XGBoost)
- [25. Data Mining Describe the decision tree model.](#25-data-mining-describe-the-decision-tree-model)
- [26. Notes from Coursera Deep Learning courses by Andrew Ng](#26-notes-from-coursera-deep-learning-courses-by-andrew-ng)
- [27. What is a neural network?](#27-what-is-a-neural-network)
- [28. How do you deal with sparse data?](#28-how-do-you-deal-with-sparse-data)
- [29. RNN and LSTM](#29-rnn-and-lstm)
- [30. Pseudo Labeling](#30-pseudo-labeling)
- [31. Knowledge Distillation](#31-knowledge-distillation)
- [32. What is an inductive bias?](#32-what-is-an-inductive-bias)
- [33. What is a confidence interval in layman's terms?](#33-confidence-interval-in-layman's-terms)


## 1. Why do you use feature selection?
Feature selection is the process of selecting a subset of relevant features for use in model construction. Feature selection is itself useful, but it mostly acts as a filter, muting out features that aren’t useful in addition to your existing features.
Feature selection methods aid you in your mission to create an accurate predictive model. They help you by choosing features that will give you as good or better accuracy whilst requiring less data.
Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model.
Fewer attributes is desirable because it reduces the complexity of the model, and a simpler model is simpler to understand and explain.
#### Filter Methods
Filter feature selection methods apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset. The methods are often univariate and consider the feature independently, or with regard to the dependent variable.
Some examples of some filter methods include the Chi squared test, information gain and correlation coefficient scores.
#### Embedded Methods
Embedded methods learn which features best contribute to the accuracy of the model while the model is being created. The most common type of embedded feature selection methods are regularization methods.
Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).
Examples of regularization algorithms are the LASSO, Elastic Net and Ridge Regression.
#### Misleading
Including redundant attributes can be misleading to modeling algorithms. Instance-based methods such as k-nearest neighbor use small neighborhoods in the attribute space to determine classification and regression predictions. These predictions can be greatly skewed by redundant attributes.
#### Overfitting
Keeping irrelevant attributes in your dataset can result in overfitting. Decision tree algorithms like C4.5 seek to make optimal spits in attribute values. Those attributes that are more correlated with the prediction are split on first. Deeper in the tree less relevant and irrelevant attributes are used to make prediction decisions that may only be beneficial by chance in the training dataset. This overfitting of the training data can negatively affect the modeling power of the method and cripple the predictive accuracy.

## 2. Explain what regularization is and why it is useful.
Regularization is the process of adding a tuning parameter to a model to induce smoothness in order to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting).

This is most often done by adding a constant multiple to an existing weight vector. This constant is often either the [L1 (Lasso)](https://en.wikipedia.org/wiki/Lasso_(statistics)) or [L2 (ridge)](https://en.wikipedia.org/wiki/Tikhonov_regularization), but can in actuality can be any norm. The model predictions should then minimize the mean of the loss function calculated on the regularized training set.

It is well known, as explained by others, that L1 regularization helps perform feature selection in sparse feature spaces, and that is a good practical reason to use L1 in some situations. However, beyond that particular reason I have never seen L1 to perform better than L2 in practice. If you take a look at [LIBLINEAR FAQ](https://www.csie.ntu.edu.tw/~cjlin/liblinear/FAQ.html#l1_regularized_classification) on this issue you will see how they have not seen a practical example where L1 beats L2 and encourage users of the library to contact them if they find one. Even in a situation where you might benefit from L1's sparsity in order to do feature selection, using L2 on the remaining variables is likely to give better results than L1 by itself.

## 3. What’s the difference between L1 and L2 regularization?
Regularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term in order to prevent the coefficients to fit so perfectly to overfit. The difference between the L1(Lasso) and L2(Ridge) is just that L2(Ridge) is the sum of the square of the weights, while L1(Lasso) is just the sum of the absolute weights in MSE or another loss function. As follows:
![alt text](images/regularization1.png)
The difference between their properties can be promptly summarized as follows:
![alt text](images/regularization2.png)

**Solution uniqueness** is a simpler case but requires a bit of imagination. First, this picture below:
![alt text](images/regularization3.png)

## 4. How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression?
[Proposed methods](http://support.sas.com/resources/papers/proceedings12/333-2012.pdf) for model validation:
* If the values predicted by the model are far outside of the response variable range, this would immediately indicate poor estimation or model inaccuracy.
* If the values seem to be reasonable, examine the parameters; any of the following would indicate poor estimation or multi-collinearity: opposite signs of expectations, unusually large or small values, or observed inconsistency when the model is fed new data.
* Use the model for prediction by feeding it new data, and use the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) (R squared) as a model validity measure.
* Use data splitting to form a separate dataset for estimating model parameters, and another for validating predictions.
* Use [jackknife resampling](https://en.wikipedia.org/wiki/Jackknife_resampling) if the dataset contains a small number of instances, and measure validity with R squared and [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE).

## 5. Explain what precision and recall are. How do they relate to the ROC curve?
Calculating precision and recall is actually quite easy. Imagine there are 100 positive cases among 10,000 cases. You want to predict which ones are positive, and you pick 200 to have a better chance of catching many of the 100 positive cases. You record the IDs of your predictions, and when you get the actual results you sum up how many times you were right or wrong. There are four ways of being right or wrong:
1. TN / True Negative: case was negative and predicted negative
2. TP / True Positive: case was positive and predicted positive
3. FN / False Negative: case was positive but predicted negative
4. FP / False Positive: case was negative but predicted positive

![alt text](images/confusion-matrix.png)

Now, your boss asks you three questions:
* What percent of your predictions were correct?
You answer: the "accuracy" was (9,760+60) out of 10,000 = 98.2%
* What percent of the positive cases did you catch?
You answer: the "recall" was 60 out of 100 = 60%
* What percent of positive predictions were correct?
You answer: the "precision" was 60 out of 200 = 30%
See also a very good explanation of [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) in Wikipedia.

![alt text](images/precision-recall.jpg)

ROC curve represents a relation between sensitivity (RECALL) and specificity(NOT PRECISION) and is commonly used to measure the performance of binary classifiers. However, when dealing with highly skewed datasets, [Precision-Recall (PR)](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf) curves give a more representative picture of performance. Remember, a ROC curve represents a relation between sensitivity (RECALL) and specificity(NOT PRECISION). Sensitivity is the other name for recall but specificity is not PRECISION.

Recall/Sensitivity is the measure of the probability that your estimate is 1 given all the samples whose true class label is 1. It is a measure of how many of the positive samples have been identified as being positive. Specificity is the measure of the probability that your estimate is 0 given all the samples whose true class label is 0. It is a measure of how many of the negative samples have been identified as being negative.

PRECISION on the other hand is different. It is a measure of the probability that a sample is a true positive class given that your classifier said it is positive. It is a measure of how many of the samples predicted by the classifier as positive is indeed positive. Note here that this changes when the base probability or prior probability of the positive class changes. Which means PRECISION depends on how rare is the positive class. In other words, it is used when positive class is more interesting than the negative class.

* Sensitivity also known as the True Positive rate or Recall is calculated as,
`Sensitivity = TP / (TP + FN)`. Since the formula doesn’t contain FP and TN, Sensitivity may give you a biased result, especially for imbalanced classes.
In the example of Fraud detection, it gives you the percentage of Correctly Predicted Frauds from the pool of Actual Frauds pool of Actual Non-Frauds.
* Specificity, also known as True Negative Rate is calculated as, `Specificity = TN / (TN + FP)`. Since the formula does not contain FN and TP, Specificity may give you a biased result, especially for imbalanced classes.
In the example of Fraud detection, it gives you the percentage of Correctly Predicted Non-Frauds from the pool of Actual Frauds pool of Actual Non-Frauds

[Assessing and Comparing Classifier Performance with ROC Curves](https://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)

## 6. Is it better to have too many false positives, or too many false negatives?
It depends on the question as well as on the domain for which we are trying to solve the question.

In medical testing, false negatives may provide a falsely reassuring message to patients and physicians that disease is absent, when it is actually present. This sometimes leads to inappropriate or inadequate treatment of both the patient and their disease. So, it is desired to have too many false positive.

For spam filtering, a false positive occurs when spam filtering or spam blocking techniques wrongly classify a legitimate email message as spam and, as a result, interferes with its delivery. While most anti-spam tactics can block or filter a high percentage of unwanted emails, doing so without creating significant false-positive results is a much more demanding task. So, we prefer too many false negatives over many false positives.

## 7. How do you deal with unbalanced binary classification?
Imbalanced data typically refers to a problem with classification problems where the classes are not represented equally.
For example, you may have a 2-class (binary) classification problem with 100 instances (rows). A total of 80 instances are labeled with Class-1 and the remaining 20 instances are labeled with Class-2.

This is an imbalanced dataset and the ratio of Class-1 to Class-2 instances is 80:20 or more concisely 4:1.
You can have a class imbalance problem on two-class classification problems as well as multi-class classification problems. Most techniques can be used on either.
The remaining discussions will assume a two-class classification problem because it is easier to think about and describe.
1. Can You Collect More Data?</br>
A larger dataset might expose a different and perhaps more balanced perspective on the classes.
More examples of minor classes may be useful later when we look at resampling your dataset.
2. Try Changing Your Performance Metric</br>
Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.
From that post, I recommend looking at the following performance measures that can give more insight into the accuracy of the model than traditional classification accuracy:
  - [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix): A breakdown of predictions into a table showing correct predictions (the diagonal) and the types of incorrect predictions made (what classes incorrect predictions were assigned).
  - [Precision](https://en.wikipedia.org/wiki/Information_retrieval#Precision): A measure of a classifiers exactness. Precision is the number of True Positives divided by the number of True Positives and False Positives. Put another way, it is the number of positive predictions divided by the total number of positive class values predicted. It is also called the [Positive Predictive Value (PPV)](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values). Precision can be thought of as a measure of a classifiers exactness. A low precision can also indicate a large number of False Positives.
  - [Recall](https://en.wikipedia.org/wiki/Information_retrieval#Recall): A measure of a classifiers completeness. Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate. Recall can be thought of as a measure of a classifiers completeness. A low recall indicates many False Negatives.
  - [F1 Score (or F-score)](https://en.wikipedia.org/wiki/F1_score): A weighted average of precision and recall.
I would also advise you to take a look at the following:
  - Kappa (or [Cohen’s kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa)): Classification accuracy normalized by the imbalance of the classes in the data.
ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.
3. Try Resampling Your Dataset
  * You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement)
  * You can delete instances from the over-represented class, called under-sampling.
5. Try Different Algorithms
6. Try Penalized Models</br>
You can use the same algorithms but give them a different perspective on the problem.
Penalized classification imposes an additional cost on the model for making classification mistakes on the minority class during training. These penalties can bias the model to pay more attention to the minority class.
Often the handling of class penalties or weights are specialized to the learning algorithm. There are penalized versions of algorithms such as penalized-SVM and penalized-LDA.
Using penalization is desirable if you are locked into a specific algorithm and are unable to resample or you’re getting poor results. It provides yet another way to “balance” the classes. Setting up the penalty matrix can be complex. You will very likely have to try a variety of penalty schemes and see what works best for your problem.
7. Try a Different Perspective</br>
Taking a look and thinking about your problem from these perspectives can sometimes shame loose some ideas.
Two you might like to consider are anomaly detection and change detection.

## 8. What is statistical power?
[Statistical power or sensitivity](https://en.wikipedia.org/wiki/Statistical_power) of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is true.

It can be equivalently thought of as the probability of accepting the alternative hypothesis (H1) when it is true—that is, the ability of a test to detect an effect, if the effect actually exists.

To put in another way, [Statistical power](https://effectsizefaq.com/2010/05/31/what-is-statistical-power/) is the likelihood that a study will detect an effect when the effect is present. The higher the statistical power, the less likely you are to make a Type II error (concluding there is no effect when, in fact, there is).

A type I error (or error of the first kind) is the incorrect rejection of a true null hypothesis. Usually a type I error leads one to conclude that a supposed effect or relationship exists when in fact it doesn't. Examples of type I errors include a test that shows a patient to have a disease when in fact the patient does not have the disease, a fire alarm going on indicating a fire when in fact there is no fire, or an experiment indicating that a medical treatment should cure a disease when in fact it does not.

A type II error (or error of the second kind) is the failure to reject a false null hypothesis. Examples of type II errors would be a blood test failing to detect the disease it was designed to detect, in a patient who really has the disease; a fire breaking out and the fire alarm does not ring; or a clinical trial of a medical treatment failing to show that the treatment works when really it does.
![alt text](images/statistical-power.png)

## 9. What are bias and variance, and what are their relation to modeling data?
**Bias** is how far removed a model's predictions are from correctness, while variance is the degree to which these predictions vary between model iterations.

Bias is generally the distance between the model that you build on the training data (the best model that your model space can provide) and the “real model” (which generates data).

**Error due to Bias**: Due to randomness in the underlying data sets, the resulting models will have a range of predictions. [Bias](https://en.wikipedia.org/wiki/Bias_of_an_estimator) measures how far off in general these models' predictions are from the correct value. The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).

**Error due to Variance**: The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model. The variance is error from sensitivity to small fluctuations in the training set.

High variance can cause an algorithm to model the random [noise](https://en.wikipedia.org/wiki/Noise_(signal_processing)) in the training data, rather than the intended outputs (overfitting).

Big dataset -> low variance <br/>
Low dataset -> high variance <br/>
Few features -> high bias, low variance <br/>
Many features -> low bias, high variance <br/>
Complicated model -> low bias <br/>
Simplified model -> high bias <br/>
Decreasing λ -> low bias <br/>
Increasing λ -> low variance <br/>

We can create a graphical visualization of bias and variance using a bulls-eye diagram. Imagine that the center of the target is a model that perfectly predicts the correct values. As we move away from the bulls-eye, our predictions get worse and worse. Imagine we can repeat our entire model building process to get a number of separate hits on the target. Each hit represents an individual realization of our model, given the chance variability in the training data we gather. Sometimes we will get a good distribution of training data so we predict very well and we are close to the bulls-eye, while sometimes our training data might be full of outliers or non-standard values resulting in poorer predictions. These different realizations result in a scatter of hits on the target.
![alt text](images/bulls-eye-diagram.jpg)

[As an example](https://www.kdnuggets.com/2016/08/bias-variance-tradeoff-overview.html), using a simple flawed Presidential election survey as an example, errors in the survey are then explained through the twin lenses of bias and variance: selecting survey participants from a phonebook is a source of bias; a small sample size is a source of variance.

Minimizing total model error relies on the balancing of bias and variance errors. Ideally, models are the result of a collection of unbiased data of low variance. Unfortunately, however, the more complex a model becomes, its tendency is toward less bias but greater variance; therefore an optimal model would need to consider a balance between these 2 properties.

The statistical evaluation method of cross-validation is useful in both demonstrating the importance of this balance, as well as actually searching it out. The number of data folds to use -- the value of k in k-fold cross-validation -- is an important decision; the lower the value, the higher the bias in the error estimates and the less variance.
![alt text](images/model-complexity.jpg)

The most important takeaways are that bias and variance are two sides of an important trade-off when building models, and that even the most routine of statistical evaluation methods are directly reliant upon such a trade-off.

We may estimate a model f̂ (X) of f(X) using linear regressions or another modeling technique. In this case, the expected squared prediction error at a point x is:
`Err(x)=E[(Y−f̂ (x))^2]`

This error may then be decomposed into bias and variance components:
`Err(x)=(E[f̂ (x)]−f(x))^2+E[(f̂ (x)−E[f̂ (x)])^2]+σ^2e`
`Err(x)=Bias^2+Variance+Irreducible`

That third term, irreducible error, is the noise term in the true relationship that cannot fundamentally be reduced by any model. Given the true model and infinite data to calibrate it, we should be able to reduce both the bias and variance terms to 0. However, in a world with imperfect models and finite data, there is a tradeoff between minimizing the bias and minimizing the variance.

That third term, irreducible error, is the noise term in the true relationship that cannot fundamentally be reduced by any model. Given the true model and infinite data to calibrate it, we should be able to reduce both the bias and variance terms to 0. However, in a world with imperfect models and finite data, there is a tradeoff between minimizing the bias and minimizing the variance.

If a model is suffering from high bias, it means that model is less complex, to make the model more robust, we can add more features in feature space. Adding data points will reduce the variance.

The bias–variance tradeoff is a central problem in supervised learning. Ideally, one wants to [choose a model](https://en.wikipedia.org/wiki/Model_selection) that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well, but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that don't tend to overfit, but may underfit their training data, failing to capture important regularities.

Models with low bias are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large noise component in the training set, making their predictions less accurate - despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials), but may produce lower variance predictions when applied beyond the training set.

#### Approaches

[Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) and [feature selection](https://en.wikipedia.org/wiki/Feature_selection) can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance, e.g.:
* (Generalized) linear models can be [regularized](#2-explain-what-regularization-is-and-why-it-is-useful) to decrease their variance at the cost of increasing their bias.
* In artificial neural networks, the variance increases and the bias decreases with the number of hidden units. Like in GLMs, regularization is typically applied.
* In k-nearest neighbor models, a high value of k leads to high bias and low variance (see below).
* In Instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars.[
* In decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.

One way of resolving the trade-off is to use [mixture models](https://en.wikipedia.org/wiki/Mixture_model) and [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning). For example, [boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) combines many "weak" (high bias) models in an ensemble that has lower bias than the individual models, while [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating) combines "strong" learners in a way that reduces their variance.

[Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)

## 10. What if the classes are imbalanced? What if there are more than 2 groups?
Binary classification involves classifying the data into two groups, e.g. whether or not a customer buys a particular product or not (Yes/No), based on independent variables such as gender, age, location etc.

As the target variable is not continuous, binary classification model predicts the probability of a target variable to be Yes/No. To evaluate such a model, a metric called the confusion matrix is used, also called the classification or co-incidence matrix. With the help of a confusion matrix, we can calculate important performance measures:
* True Positive Rate (TPR) or Recall or Sensitivity = TP / (TP + FN)
* [Precision](https://github.com/iamtodor/data-science-interview-questions-and-answers#5-explain-what-precision-and-recall-are-how-do-they-relate-to-the-roc-curve) = TP / (TP + FP)
* False Positive Rate(FPR) or False Alarm Rate = 1 - Specificity = 1 - (TN / (TN + FP))
* Accuracy = (TP + TN) / (TP + TN + FP + FN)
* Error Rate = 1 – Accuracy
* F-measure = 2 / ((1 / Precision) + (1 / Recall)) = 2 * (precision * recall) / (precision + recall)
* ROC (Receiver Operating Characteristics) = plot of FPR vs TPR
* AUC (Area Under the [ROC] Curve)  
Performance measure across all classification thresholds. Treated as the probability that a model ranks a randomly chosen positive sample higher than negative



## 11. What are some ways I can make my model more robust to outliers?
There are several ways to make a model more robust to outliers, from different points of view (data preparation or model building). An outlier in the question and answer is assumed being unwanted, unexpected, or a must-be-wrong value to the human’s knowledge so far (e.g. no one is 200 years old) rather than a rare event which is possible but rare.

Outliers are usually defined in relation to the distribution. Thus outliers could be removed in the pre-processing step (before any learning step), by using standard deviations `(Mean +/- 2*SD)`, it can be used for normality. Or interquartile ranges `Q1 - Q3`, `Q1` -  is the "middle" value in the first half of the rank-ordered data set, `Q3` - is the "middle" value in the second half of the rank-ordered data set. It can be used for not normal/unknown as threshold levels.

Moreover, data transformation (e.g. log transformation) may help if data have a noticeable tail. When outliers related to the sensitivity of the collecting instrument which may not precisely record small values, Winsorization may be useful. This type of transformation (named after Charles P. Winsor (1895–1951)) has the same effect as clipping signals (i.e. replaces extreme data values with less extreme values).  Another option to reduce the influence of outliers is using mean absolute difference rather mean squared error.

For model building, some models are resistant to outliers (e.g. tree-based approaches) or non-parametric tests. Similar to the median effect, tree models divide each node into two in each split. Thus, at each split, all data points in a bucket could be equally treated regardless of extreme values they may have.

## 12. In unsupervised learning, if a ground truth about a dataset is unknown, how can we determine the most useful number of clusters to be?
The elbow method is often the best place to state, and is especially useful due to its ease of explanation and verification via visualization. The elbow method is interested in explaining variance as a function of cluster numbers (the k in k-means). By plotting the percentage of variance explained against k, the first N clusters should add significant information, explaining variance; yet, some eventual value of k will result in a much less significant gain in information, and it is at this point that the graph will provide a noticeable angle. This angle will be the optimal number of clusters, from the perspective of the elbow method,
It should be self-evident that, in order to plot this variance against varying numbers of clusters, varying numbers of clusters must be tested. Successive complete iterations of the clustering method must be undertaken, after which the results can be plotted and compared.
DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.

## 13. Define variance
Variance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of (random) numbers are spread out from their average value. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself.

Var(X) = E[(X - m)^2], m=E[X]

Variance is, thus, a measure of the scatter of the values of a random variable relative to its mathematical expectation.

## 14. Expected value
Expected value — [Expected Value](https://en.wikipedia.org/wiki/Expected_value) ([Probability Distribution](https://en.wikipedia.org/wiki/Probability_distribution) In a probability distribution, expected value is the value that a random variable takes with greatest likelihood. 

Based on the law of distribution of a random variable x, we know that a random variable x can take values x1, x2, ..., xk with probabilities p1, p2, ..., pk.
The mathematical expectation M(x) of a random variable x is equal.
The mathematical expectation of a random variable X (denoted by M (X) or less often E (X)) characterizes the average value of a random variable (discrete or continuous). Mathematical expectation is the first initial moment of a given CB.

Mathematical expectation is attributed to the so-called characteristics of the distribution position (to which the mode and median also belong). This characteristic describes a certain average position of a random variable on the numerical axis. Say, if the expectation of a random variable - the lamp life is 100 hours, then it is considered that the values of the service life are concentrated (on both sides) from this value (with dispersion on each side, indicated by the variance).

The mathematical expectation of a discrete random variable X is calculated as the sum of the products of the values xi that the CB takes X by the corresponding probabilities pi:
```python
import numpy as np
X = [3,4,5,6,7]
P = [0.1,0.2,0.3,0.4,0.5]
np.sum(np.dot(X, P))
```

## 15. Describe the differences between and use cases for box plots and histograms
A [histogram](http://www.brighthubpm.com/six-sigma/13307-what-is-a-histogram/) is a type of bar chart that graphically displays the frequencies of a data set. Similar to a bar chart, a histogram plots the frequency, or raw count, on the Y-axis (vertical) and the variable being measured on the X-axis (horizontal).

The only difference between a histogram and a bar chart is that a histogram displays frequencies for a group of data, rather than an individual data point; therefore, no spaces are present between the bars. Typically, a histogram groups data into small chunks (four to eight values per bar on the horizontal axis), unless the range of data is so great that it easier to identify general distribution trends with larger groupings.

A box plot, also called a [box-and-whisker](http://www.brighthubpm.com/six-sigma/43824-using-box-and-whiskers-plots/) plot, is a chart that graphically represents the five most important descriptive values for a data set. These values include the minimum value, the first quartile, the median, the third quartile, and the maximum value. When graphing this five-number summary, only the horizontal axis displays values. Within the quadrant, a vertical line is placed above each of the summary numbers. A box is drawn around the middle three lines (first quartile, median, and third quartile) and two lines are drawn from the box’s edges to the two endpoints (minimum and maximum).
Boxplots are better for comparing distributions than histograms!
![alt text](images/histogram-vs-boxplot.png)

## 16. How would you find an anomaly in a distribution?
Before getting started, it is important to establish some boundaries on the definition of an anomaly. Anomalies can be broadly categorized as:
1. Point anomalies: A single instance of data is anomalous if it's too far off from the rest. Business use case: Detecting credit card fraud based on "amount spent."
2. Contextual anomalies: The abnormality is context specific. This type of anomaly is common in time-series data. Business use case: Spending $100 on food every day during the holiday season is normal, but may be odd otherwise.
3. Collective anomalies: A set of data instances collectively helps in detecting anomalies. Business use case: Someone is trying to copy data form a remote machine to a local host unexpectedly, an anomaly that would be flagged as a potential cyber attack.

Best steps to prevent anomalies is to implement policies or checks that can catch them during the data collection stage. Unfortunately, you do not often get to collect your own data, and often the data you're mining was collected for another purpose. About 68% of all the data points are within one standard deviation from the mean. About 95% of the data points are within two standard deviations from the mean. Finally, over 99% of the data is within three standard deviations from the mean. When the value deviate too much from the mean, let’s say by ± 4σ, then we can considerate this almost impossible value as anomaly. (This limit can also be calculated using the percentile).

#### Statistical methods
Statistically based anomaly detection uses this knowledge to discover outliers. A dataset can be standardized by taking the z-score of each point. A z-score is a measure of how many standard deviations a data point is away from the mean of the data. Any data-point that has a z-score higher than 3 is an outlier, and likely to be an anomaly. As the z-score increases above 3, points become more obviously anomalous. A z-score is calculated using the following equation. A box-plot is perfect for this application.

#### Metric method
Judging by the number of publications, metric methods are the most popular methods among researchers. They postulate the existence of a certain metric in the space of objects, which helps to find anomalies. Intuitively, the anomaly has few neighbors in the instannce space, and a typical point has many. Therefore, a good measure of anomalies can be, for example, the «distance to the k-th neighbor». (See method: [Local Outlier Factor](https://en.wikipedia.org/wiki/Local_outlier_factor)). Specific metrics are used here, for example [Mahalonobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). Mahalonobis distance is a measure of distance between vectors of random variables, generalizing the concept of Euclidean distance. Using Mahalonobis distance, it is possible to determine the similarity of unknown and known samples. It differs from Euclidean distance in that it takes into account correlations between variables and is scale invariant.
![alt text](images/metrical-methods.png)

The most common form of clustering-based anomaly detection is done with prototype-based clustering.

Using this approach to anomaly detection, a point is classified as an anomaly if its omission from the group significantly improves the prototype, then the point is classified as an anomaly. This logically makes sense. K-means is a clustering algorithm that clusters similar points. The points in any cluster are similar to the centroid of that cluster, hence why they are members of that cluster. If one point in the cluster is so far from the centroid that it pulls the centroid away from it's natural center, than that point is literally an outlier, since it lies outside the natural bounds for the cluster. Hence, its omission is a logical step to improve the accuracy of the rest of the cluster. Using this approach, the outlier score is defined as the degree to which a point doesn't belong to any cluster, or the distance it is from the centroid of the cluster. In K-means, the degree to which the removal of a point would increase the accuracy of the centroid is the difference in the SSE, or standard squared error, or the cluster with and without the point. If there is a substantial improvement in SSE after the removal of the point, that correlates to a high outlier score for that point.
More specifically, when using a k-means clustering approach towards anomaly detection, the outlier score is calculated in one of two ways. The simplest is the point's distance from its closest centroid. However, this approach is not as useful when there are clusters of differing densities. To tackle that problem, the point's relative distance to it's closest centroid is used, where relative distance is defined as the ratio of the point's distance from the centroid to the median distance of all points in the cluster from the centroid. This approach to anomaly detection is sensitive to the value of k. Also, if the data is highly noisy, then that will throw off the accuracy of the initial clusters, which will decrease the accuracy of this type of anomaly detection. The time complexity of this approach is obviously dependent on the choice of clustering algorithm, but since most clustering algorithms have linear or close to linear time and space complexity, this type of anomaly detection can be highly efficient.

## 17. How do you deal with outliers in your data?

For the most part, if your data is affected by these extreme cases, you can bound the input to a historical representative of your data that excludes outliers. So 
that could be a number of items (>3) or a lower or upper bounds on your order value.

If the outliers are from a data set that is relatively unique then analyze them for your specific situation. Analyze both with and without them, and perhaps with a replacement alternative, if you have a reason for one, and report your results of this assessment. 
One option is to try a transformation. Square root and log transformations both pull in high numbers.  This can make assumptions work better if the outlier is a dependent.

## 18. How do you deal with sparse data?

We could take a look at L1 regularization since it best fits to the sparse data and do feature selection. If linear relationship - linear regression either - svm. 

Also it would be nice to use one-hot-encoding or bag-of-words. A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.

## 19. Big Data Engineer Can you explain what REST is?

REST stands for Representational State Transfer. (It is sometimes spelled "ReST".) It relies on a stateless, client-server, cacheable communications protocol -- and in virtually all cases, the HTTP protocol is used.
REST is an architecture style for designing networked applications. The idea is simple HTTP is used to make calls between machines.
* In many ways, the World Wide Web itself, based on HTTP, can be viewed as a REST-based architecture.
RESTful applications use HTTP requests to post data (create and/or update), read data (e.g., make queries), and delete data. Thus, REST uses HTTP for all four CRUD (Create/Read/Update/Delete) operations.
REST is a lightweight alternative to mechanisms like RPC (Remote Procedure Calls) and Web Services (SOAP, WSDL, et al.). Later, we will see how much more simple REST is.
* Despite being simple, REST is fully-featured; there's basically nothing you can do in Web Services that can't be done with a RESTful architecture.
REST is not a "standard". There will never be a W3C recommendation for REST, for example. And while there are REST programming frameworks, working with REST is so simple that you can often "roll your own" with standard library features in languages like Perl, Java, or C#.

## 20. Logistic regression

Log odds - raw output from the model; odds - exponent from the output of the model. Probability of the output - odds / (1+odds).

## 21. What is the effect on the coefficients of logistic regression if two predictors are highly correlated? What are the confidence intervals of the coefficients?
When predictor variables are correlated, the estimated regression coefficient of any one variable depends on which other predictor variables are included in the model. When predictor variables are correlated, the precision of the estimated regression coefficients decreases as more predictor variables are added to the model.

In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multiple regression model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.

The consequences of multicollinearity:
* Ratings estimates remain unbiased.
* Standard coefficient errors increase.
* The calculated t-statistics are underestimated.
* Estimates become very sensitive to changes in specifications and changes in individual observations.
* The overall quality of the equation, as well as estimates of variables not related to multicollinearity, remain unaffected.
* The closer multicollinearity to perfect (strict), the more serious its consequences.

Indicators of multicollinearity: 
1. High R2 and negligible odds.
2. Strong pair correlation of predictors.
3. Strong partial correlations of predictors.
4. High VIF - variance inflation factor.

Confidence interval (CI) is a type of interval estimate (of a population parameter) that is computed from the observed data. The confidence level is the frequency (i.e., the proportion) of possible confidence intervals that contain the true value of their corresponding parameter. In other words, if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level.

Confidence intervals consist of a range of values (interval) that act as good estimates of the unknown population parameter. However, the interval computed from a particular sample does not necessarily include the true value of the parameter. Since the observed data are random samples from the true population, the confidence interval obtained from the data is also random. If a corresponding hypothesis test is performed, the confidence level is the complement of the level of significance, i.e. a 95% confidence interval reflects a significance level of 0.05. If it is hypothesized that a true parameter value is 0 but the 95% confidence interval does not contain 0, then the estimate is significantly different from zero at the 5% significance level.

The desired level of confidence is set by the researcher (not determined by data). Most commonly, the 95% confidence level is used. However, other confidence levels can be used, for example, 90% and 99%.

Factors affecting the width of the confidence interval include the size of the sample, the confidence level, and the variability in the sample. A larger sample size normally will lead to a better estimate of the population parameter.
A Confidence Interval is a range of values we are fairly sure our true value lies in.

`X  ±  Z*s/√(n)`, X is the mean, Z is the chosen Z-value from the table, s is the standard deviation, n is the number of samples. The value after the ± is called the margin of error.

## 22. What’s the difference between Gaussian Mixture Model and K-Means?
Let's says we are aiming to break them into three clusters. K-means will start with the assumption that a given data point belongs to one cluster.

Choose a data point. At a given point in the algorithm, we are certain that a point belongs to a red cluster. In the next iteration, we might revise that belief, and be certain that it belongs to the green cluster. However, remember, in each iteration, we are absolutely certain as to which cluster the point belongs to. This is the "hard assignment".

What if we are uncertain? What if we think, well, I can't be sure, but there is 70% chance it belongs to the red cluster, but also 10% chance its in green, 20% chance it might be blue. That's a soft assignment. The Mixture of Gaussian model helps us to express this uncertainty. It starts with some prior belief about how certain we are about each point's cluster assignments. As it goes on, it revises those beliefs. But it incorporates the degree of uncertainty we have about our assignment.

Kmeans: find kk to minimize `(x−μk)^2`

Gaussian Mixture (EM clustering) : find kk to minimize `(x−μk)^2/σ^2`

The difference (mathematically) is the denominator “σ^2”, which means GM takes variance into consideration when it calculates the measurement.
Kmeans only calculates conventional Euclidean distance.
In other words, Kmeans calculate distance, while GM calculates “weighted” distance.

**K means**:
* Hard assign a data point to one particular cluster on convergence.
* It makes use of the L2 norm when optimizing (Min {Theta} L2 norm point and its centroid coordinates).

**EM**:
* Soft assigns a point to clusters (so it give a probability of any point belonging to any centroid).
* It doesn't depend on the L2 norm, but is based on the Expectation, i.e., the probability of the point belonging to a particular cluster. This makes K-means biased towards spherical clusters.

## 23. Describe how Gradient Boosting works.
The idea of boosting came out of the idea of whether a weak learner can be modified to become better.

Gradient boosting relies on regression trees (even when solving a classification problem) which minimize **MSE**. Selecting a prediction for a leaf region is simple: to minimize MSE we should select an average target value over samples in the leaf. The tree is built greedily starting from the root: for each leaf a split is selected to minimize MSE for this step.

To begin with, gradient boosting is an ensembling technique, which means that prediction is done by an ensemble of simpler estimators. While this theoretical framework makes it possible to create an ensemble of various estimators, in practice we almost always use GBDT — gradient boosting over decision trees. 

The aim of gradient boosting is to create (or "train") an ensemble of trees, given that we know how to train a single decision tree. This technique is called **boosting** because we expect an ensemble to work much better than a single estimator.

Here comes the most interesting part. Gradient boosting builds an ensemble of trees **one-by-one**, then the predictions of the individual trees **are summed**: D(x)=d​tree 1​​(x)+d​tree 2​​(x)+...

The next decision tree tries to cover the discrepancy between the target function f(x) and the current ensemble prediction **by reconstructing the residual**.

For example, if an ensemble has 3 trees the prediction of that ensemble is:
D(x)=d​tree 1​​(x)+d​tree 2​​(x)+d​tree 3​​(x). The next tree (tree 4) in the ensemble should complement well the existing trees and minimize the training error of the ensemble.

In the ideal case we'd be happy to have: D(x)+d​tree 4​​(x)=f(x).

To get a bit closer to the destination, we train a tree to reconstruct the difference between the target function and the current predictions of an ensemble, which is called the **residual**: R(x)=f(x)−D(x). Did you notice? If decision tree completely reconstructs R(x), the whole ensemble gives predictions without errors (after adding the newly-trained tree to the ensemble)! That said, in practice this never happens, so we instead continue the iterative process of ensemble building.

### AdaBoost the First Boosting Algorithm
The weak learners in AdaBoost are decision trees with a single split, called decision stumps for their shortness.

AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns.
**Gradient boosting involves three elements:**
1. A loss function to be optimized.
2. A weak learner to make predictions.
3. An additive model to add weak learners to minimize the loss function.

#### Loss Function
The loss function used depends on the type of problem being solved.
It must be differentiable, but many standard loss functions are supported and you can define your own.
For example, regression may use a squared error and classification may use logarithmic loss.
A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used.

#### Weak Learner
Decision trees are used as the weak learner in gradient boosting.

Specifically regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions.

Trees are constructed in a greedy manner, choosing the best split points based on purity scores like Gini or to minimize the loss.
Initially, such as in the case of AdaBoost, very short decision trees were used that only had a single split, called a decision stump. Larger trees can be used generally with 4-to-8 levels.

It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes.
This is to ensure that the learners remain weak, but can still be constructed in a greedy manner.

#### Additive Model
Trees are added one at a time, and existing trees in the model are not changed.

A gradient descent procedure is used to minimize the loss when adding trees.
Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error.

Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by reducing the residual loss.

Generally this approach is called functional gradient descent or gradient descent with functions.
The output for the new tree is then added to the output of the existing sequence of trees in an effort to correct or improve the final output of the model.

A fixed number of trees are added or training stops once loss reaches an acceptable level or no longer improves on an external validation dataset.

### Improvements to Basic Gradient Boosting
Gradient boosting is a greedy algorithm and can overfit a training dataset quickly.
It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting.
In this section we will look at 4 enhancements to basic gradient boosting:
* Tree Constraints
* Shrinkage
* Random sampling
* Penalized Learning

#### Tree Constraints
It is important that the weak learners have skill but remain weak.
There are a number of ways that the trees can be constrained.

A good general heuristic is that the more constrained tree creation is, the more trees you will need in the model, and the reverse, where less constrained individual trees, the fewer trees that will be required.

Below are some constraints that can be imposed on the construction of decision trees:
* Number of trees, generally adding more trees to the model can be very slow to overfit. The advice is to keep adding trees until no further improvement is observed.
* Tree depth, deeper trees are more complex trees and shorter trees are preferred. Generally, better results are seen with 4-8 levels.
* Number of nodes or number of leaves, like depth, this can constrain the size of the tree, but is not constrained to a symmetrical structure if other constraints are used.
* Number of observations per split imposes a minimum constraint on the amount of training data at a training node before a split can be considered
* Minimum improvement to loss is a constraint on the improvement of any split added to a tree.

#### Weighted Updates
The predictions of each tree are added together sequentially.
The contribution of each tree to this sum can be weighted to slow down the learning by the algorithm. This weighting is called a shrinkage or a learning rate.

Each update is simply scaled by the value of the “learning rate parameter” *v*

The effect is that learning is slowed down, in turn require more trees to be added to the model, in turn taking longer to train, providing a configuration trade-off between the number of trees and learning rate.

Decreasing the value of v [the learning rate] increases the best value for M [the number of trees].

It is common to have small values in the range of 0.1 to 0.3, as well as values less than 0.1.

Similar to a learning rate in stochastic optimization, shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model.
#### Stochastic Gradient Boosting
A big insight into bagging ensembles and random forest was allowing trees to be greedily created from subsamples of the training dataset.

This same benefit can be used to reduce the correlation between the trees in the sequence in gradient boosting models.

This variation of boosting is called stochastic gradient boosting.

At each iteration a subsample of the training data is drawn at random (without replacement) from the full training dataset. The randomly selected subsample is then used, instead of the full sample, to fit the base learner.

A few variants of stochastic boosting that can be used:
* Subsample rows before creating each tree.
* Subsample columns before creating each tree
* Subsample columns before considering each split.
Generally, aggressive sub-sampling such as selecting only 50% of the data has shown to be beneficial. According to user feedback, using column sub-sampling prevents over-fitting even more so than the traditional row sub-sampling.
#### Penalized Gradient Boosting
Additional constraints can be imposed on the parameterized trees in addition to their structure.
Classical decision trees like CART are not used as weak learners, instead a modified form called a regression tree is used that has numeric values in the leaf nodes (also called terminal nodes). The values in the leaves of the trees can be called weights in some literature.

As such, the leaf weight values of the trees can be regularized using popular regularization functions, such as:
* L1 regularization of weights.
* L2 regularization of weights.

The additional regularization term helps to smooth the final learnt weights to avoid over-fitting. Intuitively, the regularized objective will tend to select a model employing simple and predictive functions.

More details in 2 posts (russian):
* https://habr.com/company/ods/blog/327250/
* https://alexanderdyakonov.files.wordpress.com/2017/06/book_boosting_pdf.pdf

## 24. Difference between AdaBoost and XGBoost.
Both methods combine weak learners into one strong learner. For example, one decision tree is a weak learner, and an emsemble of them would be a random forest model, which is a strong learner. 

Both methods in the learning process will increase the ensemble of weak-trainers, adding new weak learners to the ensemble at each training iteration, i.e. in the case of the forest, the forest will grow with new trees. The only difference between AdaBoost and XGBoost is how the ensemble is replenished.

AdaBoost works by weighting the observations, putting more weight on difficult to classify instances and less on those already handled well. New weak learners are added sequentially that focus their training on the more difficult patterns. AdaBoost at each iteration changes the sample weights in the sample. It raises the weight of the samples in which more mistakes were made. The sample weights vary in proportion to the ensemble error. We thereby change the probabilistic distribution of samples - those that have more weight will be selected more often in the future. It is as if we had accumulated samples on which more mistakes were made and would use them instead of the original sample. In addition, in AdaBoost, each weak learner has its own weight in the ensemble (alpha weight) - this weight is higher, the “smarter” this weak learner is, i.e. than the learner least likely to make mistakes.

XGBoost does not change the selection or the distribution of observations at all. XGBoost builds the first tree (weak learner), which will fit the observations with some prediction error. A second tree (weak learner) is then added to correct the errors made by the existing model. Errors are minimized using a gradient descent algorithm. Regularization can also be used to penalize more complex models through both Lasso and Ridge regularization. 

In short, AdaBoost- reweighting examples. Gradient boosting - predicting the loss function of trees. Xgboost - the regularization term was added to the loss function (depth + values ​​in leaves).

## 25. Data Mining Describe the decision tree model
A decision tree is a structure that includes a root node, branches, and leaf nodes. Each internal node denotes a test on an attribute, each branch denotes the outcome of a test, and each leaf node holds a class label. The topmost node in the tree is the root node.

Each internal node represents a test on an attribute. Each leaf node represents a class.
The benefits of having a decision tree are as follows:
* It does not require any domain knowledge.
* It is easy to comprehend.
* The learning and classification steps of a decision tree are simple and fast.

**Tree Pruning**

Tree pruning is performed in order to remove anomalies in the training data due to noise or outliers. The pruned trees are smaller and less complex.

**Tree Pruning Approaches**

Here is the Tree Pruning Approaches listed below:
* Pre-pruning − The tree is pruned by halting its construction early.
* Post-pruning - This approach removes a sub-tree from a fully grown tree.

**Cost Complexity**

The cost complexity is measured by the following two parameters − Number of leaves in the tree, and Error rate of the tree.

## 26. Notes from Coursera Deep Learning courses by Andrew Ng
[Notes from Coursera Deep Learning courses by Andrew Ng](https://pt.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng/)

## 27. What is a neural network?
Neural networks are typically organized in layers. Layers are made up of a number of interconnected 'nodes' which contain an 'activation function'. Patterns are presented to the network via the 'input layer', which communicates to one or more 'hidden layers' where the actual processing is done via a system of weighted 'connections'. The hidden layers then link to an 'output layer' where the answer is output as shown in the graphic below.

Although there are many different kinds of learning rules used by neural networks, this demonstration is concerned only with one: the delta rule. The delta rule is often utilized by the most common class of ANNs called 'backpropagation neural networks' (BPNNs). Backpropagation is an abbreviation for the backwards propagation of error. With the delta rule, as with other types of back propagation, 'learning' is a supervised process that occurs with each cycle or 'epoch' (i.e. each time the network is presented with a new input pattern) through a forward activation flow of outputs, and the backwards error propagation of weight adjustments. More simply, when a neural network is initially presented with a pattern it makes a random 'guess' as to what it might be. It then sees how far its answer was from the actual one and makes an appropriate adjustment to its connection weights. More graphically, the process looks something like this: 
![alt text](images/neural_network.png)

Backpropagation performs a gradient descent within the solution's vector space towards a 'global minimum' along the steepest vector of the error surface. The global minimum is that theoretical solution with the lowest possible error. The error surface itself is a hyperparaboloid but is seldom 'smooth'. Indeed, in most problems, the solution space is quite irregular with numerous 'pits' and 'hills' which may cause the network to settle down in a 'local minimum' which is not the best overall solution.

Since the nature of the error space can not be known a priori, neural network analysis often requires a large number of individual runs to determine the best solution. Most learning rules have built-in mathematical terms to assist in this process which control the 'speed' (Beta-coefficient) and the 'momentum' of the learning. The speed of learning is actually the rate of convergence between the current solution and the global minimum. Momentum helps the network to overcome obstacles (local minima) in the error surface and settle down at or near the global minimum.

Once a neural network is 'trained' to a satisfactory level it may be used as an analytical tool on other data. To do this, the user no longer specifies any training runs and instead allows the network to work in forward propagation mode only. New inputs are presented to the input pattern where they filter into and are processed by the middle layers as though training were taking place, however, at this point the output is retained and no backpropagation occurs. The output of a forward propagation run is the predicted model for the data which can then be used for further analysis and interpretation.

## 28. How do you deal with sparse data?
We could take a look at L1 regularization since it best fits the sparse data and does feature selection. If linear relationship - linear regression either - svm. Also it would be nice to use one-hot-encoding or bag-of-words. 
A one hot encoding is a representation of categorical variables as binary vectors.
This first requires that the categorical values be mapped to integer values.
Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.

## 29. RNN and LSTM
Here are a few of my favorites:
* [Understanding LSTM Networks, Chris Olah's LSTM post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [Exploring LSTMs, Edwin Chen's LSTM post](http://blog.echen.me/2017/05/30/exploring-lstms/)
* [The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy's blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [CS231n Lecture 10 - Recurrent Neural Networks, Image Captioning, LSTM, Andrej Karpathy's lecture](https://www.youtube.com/watch?v=iX5V1WpxxkY)
* [Jay Alammar's The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) the guy generally focuses on visualizing different ML concepts

## 30. Pseudo Labeling
Pseudo-labeling is a technique that allows you to use predicted with **confidence** test data in your training process. This effectivey works by allowing your model to look at more samples, possibly varying in distributions. I have found [this](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969) Kaggle kernel to be useful in understanding how one can use pseudo-labeling in light of having too few train data points.

## 31. Knowledge Distillation
It is the process by which a considerably larger model is able to transfer its knowledge to a smaller one. Applications include NLP and object detection allowing for less powerful hardware to make good inferences without significant loss of accuracy.

Example: model compression which is used to compress the knowledge of multiple models into a single neural network.

[Explanation](https://nervanasystems.github.io/distiller/knowledge_distillation.html)

## 32. What is an inductive bias?
A model's inductive bias is referred to as assumptions made within that model to learn your target function from independent variables, your features. Without these assumptions, there is a whole space of solutions to our problem and finding the one that works best becomes a problem. Found [this](https://stackoverflow.com/questions/35655267/what-is-inductive-bias-in-machine-learning) StackOverflow question useful to look at and explore.

Consider an example of an inducion bias when choosing a learning algorithm with the minimum cross-validation (CV) error. Here, we **rely** on the hypothesis of the minimum CV error and **hope** it is able to generalize well on the data yet to be seen. Effectively, this choice is what helps us (in this case) make a choice in favor of the learning algorithm (or model) being tried.

## 33. What is a confidence interval in layman's terms?
Confidence interval as the name suggests is the amount of confidence associated with an interval of values to get the desired outcome. For example : if 100 - 200 range is a 95% confidence interval , it implies that someone can have 95% assurance that the data point or any desired value is present in that range.



Data-Science-Interview-Questions-and-Answers-General (Updating)
====================================================

I hope this article could help beginners to better understanding of Data Science, and have a better performance in your first interviews.  

I will do long update and please feel free to contact me if you have any questions.  

I'm just a porter, most of them are borrowing from others

## Data Science Questions and Answers (General) for beginner
### Editor : Zhiqiang ZHONG 

# Content
#### Q1 How would you create a taxonomy to identify key customer trends in unstructured data?

    The best way to approach this question is to mention that it is good to check with the business owner 
    and understand their objectives before categorizing the data. Having done this, it is always good to 
    follow an iterative approach by pulling new data samples and improving   the model accordingly by validating 
    it for accuracy by soliciting feedback from the stakeholders of the business. This helps ensure that your 
    model is producing actionable results and improving over the time.
    
#### Q2 Python or R – Which one would you prefer for text analytics?

    The best possible answer for this would be Python because it has Pandas library that provides easy to use 
    data structures and high performance data analysis tools.
    
#### Q3 Which technique is used to predict categorical responses?

    Classification technique is used widely in mining for classifying data sets.
    
#### Q4 What is logistic regression? Or State an example when you have used logistic regression recently.

    Logistic Regression often referred as logit model is a technique to predict the binary outcome from a linear 
    combination of predictor variables. For example, if you want to predict whether a particular political leader 
    will win the election or not. In this case, the outcome of prediction is binary i.e. 0 or 1 (Win/Lose). The 
    predictor variables here would be the amount of money spent for election campaigning of a particular candidate, 
    the amount of time spent in campaigning, etc.
    
#### Q5 What are Recommender Systems?

    A subclass of information filtering systems that are meant to predict the preferences or ratings that a user 
    would give to a product. Recommender systems are widely used in movies, news, research articles, products, 
    social tags, music, etc.
    
#### Q6 Why data cleaning plays a vital role in analysis?

    Cleaning data from multiple sources to transform it into a format that data analysts or data scientists can work 
    with is a cumbersome process because - as the number of data sources increases, the time take to clean the data 
    increases exponentially due to the number of sources and the volume of data generated in these sources. It might 
    take up to 80% of the time for just cleaning data making it a critical part of analysis task.
    
#### Q7 Differentiate between univariate, bivariate and multivariate analysis.

    These are descriptive statistical analysis techniques which can be differentiated based on the number of 
    variables involved at a given point of time. For example, the pie charts of sales based on territory involve 
    only one variable and can be referred to as univariate analysis.

    If the analysis attempts to understand the difference between 2 variables at time as in a scatterplot, then it 
    is referred to as bivariate analysis. For example, analysing the volume of sale and a spending can be considered 
    as an example of bivariate analysis.

    Analysis that deals with the study of more than two variables to understand the effect of variables on the 
    responses is referred to as multivariate analysis.

#### Q8 What do you understand by the term Normal Distribution?

    Data is usually distributed in different ways with a bias to the left or to the right or it can all be jumbled
    up. However, there are chances that data is distributed around a central value without any bias to the left or
    right and reaches normal distribution in the form of a bell shaped curve. The random variables are distributed
    in the form of an symmetrical bell shaped curve.
    
![](https://s3.amazonaws.com/files.dezyre.com/images/blog/100+Data+Science+Interview+Questions+and+Answers+(General)/Bell+Shaped+Curve+for+Normal+Distribution.jpg)

#### Q9 What is Linear Regression?

    Linear regression is a statistical technique where the score of a variable Y is predicted from the score of a 
    second variable X. X is referred to as the predictor variable and Y as the criterion variable.
    
#### Q10 What is Interpolation and Extrapolation?

    Estimating a value from 2 known values from a list of values is Interpolation. Extrapolation is approximating 
    a value by extending a known set of values or facts.
    
#### Q11 What is power analysis?

    An experimental design technique for determining the effect of a given sample size.
    
#### Q12 What is K-means? How can you select K for K-means?

    K-means is a clestering algorithm, handle with un-supervised problem. k-means clustering aims to partition
    n observations into k clusters in which each observation belongs to the cluster with the nearest mean, 
    serving as a prototype of the cluster.
    
    You can choose the number of cluster by visually but there is lots of ambiguity, or computethe sum of SSE(the
    sum of squared error) for some values of K. To find one good K.
    
![](https://qph.ec.quoracdn.net/main-qimg-678795190794dd4c071366c06bf32115.webp)

    In this case, k=6 is the value.
    
[More reading](https://www.quora.com/How-can-we-choose-a-good-K-for-K-means-clustering)
    
#### Q13 What is Collaborative filtering?

    The process of filtering used by most of the recommender systems to find patterns or information by collaborating 
    viewpoints, various data sources and multiple agents.
    
#### Q14 What is the difference between Cluster and Systematic Sampling?

    Cluster sampling is a technique used when it becomes difficult to study the target population spread across
    a wide area and simple random sampling cannot be applied. Cluster Sample is a probability sample where each 
    sampling unit is a collection, or cluster of elements. Systematic sampling is a statistical technique where 
    elements are selected from an ordered sampling frame. In systematic sampling, the list is progressed in a 
    circular manner so once you reach the end of the list,it is progressed from the top again. The best example
    for systematic sampling is equal probability method.
    
#### Q15 Are expected value and mean value different?

    They are not different but the terms are used in different contexts. Mean is generally referred when talking 
    about a probability distribution or sample population whereas expected value is generally referred in a 
    random variable context.

    ***For Sampling Data***
    Mean value is the only value that comes from the sampling data.
    Expected Value is the mean of all the means i.e. the value that is built from multiple samples. Expected 
    value is the population mean.

    ***For Distributions***
    Mean value and Expected value are same irrespective of the distribution, under the condition that the 
    distribution is in the same population.
    
#### Q16 What does P-value signify about the statistical data?

    P-value is used to determine the significance of results after a hypothesis test in statistics. P-value 
    helps the readers to draw conclusions and is always between 0 and 1.
- P- Value > 0.05 denotes weak evidence against the null hypothesis which means the null hypothesis cannot be rejected.
- P-value <= 0.05 denotes strong evidence against the null hypothesis which means the null hypothesis can be rejected.
- P-value=0.05is the marginal value indicating it is possible to go either way.

#### Q17 Do gradient descent methods always converge to same point?

    No, they do not because in some cases it reaches a local minima or a local optima point. You don’t reach 
    the global optima point. It depends on the data and starting conditions
    
~~#### Q18 What are categorical variables?~~

#### Q19 A test has a true positive rate of 100% and false positive rate of 5%. There is a population with a 1/1000 rate of having the condition the test identifies. Considering a positive test, what is the probability of having that condition?

    Let’s suppose you are being tested for a disease, if you have the illness the test will end up saying you 
    have the illness. However, if you don’t have the illness- 5% of the times the test will end up saying you
    have the illness and 95% of the times the test will give accurate result that you don’t have the illness. 
    Thus there is a 5% error in case you do not have the illness.

    Out of 1000 people, 1 person who has the disease will get true positive result.

    Out of the remaining 999 people, 5% will also get true positive result.

    Close to 50 people will get a true positive result for the disease.

    This means that out of 1000 people, 51 people will be tested positive for the disease even though only one 
    person has the illness. There is only a 2% probability of you having the disease even if your reports say 
    that you have the disease.

#### Q20 How you can make data normal using Box-Cox transformation?

    The calculation fomula of Box-Cox: 
![](http://images.cnblogs.com/cnblogs_com/zgw21cn/WindowsLiveWriter/BoxCox_119E9/clip_image002_thumb.gif)

    It change the calculation between log, sqrt and reciprocal operation by changing lambda. Find a suitable 
    lambda based on specific data set.
    
#### Q21 What is the difference between Supervised Learning an Unsupervised Learning?

    If an algorithm learns something from the training data so that the knowledge can be applied to the test data,
    then it is referred to as Supervised Learning. Classification is an example for Supervised Learning. If the
    algorithm does not learn anything beforehand because there is no response variable or any training data, 
    then it is referred to as unsupervised learning. Clustering is an example for unsupervised learning.
    
#### Q22 Explain the use of Combinatorics in data science.

    Combinatorics used a lot in data science, from feature engineer to algorithms(ensemble algorithms).Creat new features
    by merge original feature and merge several networks in one to creat news, like bagging, boosting and stacking. 

#### Q23 Why is vectorization considered a powerful method for optimizing numerical code?

    Vectorization can change original data to be structed.

#### Q24 What is the goal of A/B Testing?

    It is a statistical hypothesis testing for randomized experiment with two variables A and B. The goal of A/B 
    Testing is to identify any changes to the web page to maximize or increase the outcome of an interest. An
    example for this could be identifying the click through rate for a banner ad.
    
#### Q25 What is an Eigenvalue and Eigenvector?

    Eigenvectors are used for understanding linear transformations. In data analysis, we usually calculate the
    eigenvectors for a correlation or covariance matrix. Eigenvectors are the directions along which a particular
    linear transformation acts by flipping, compressing or stretching. Eigenvalue can be referred to as the strength
    of the transformation in the direction of eigenvector or the factor by which the compression occurs.
#### Q26 What is Gradient Descent?

    A method to find the local minimum of a function. From a point along the direction of gradient to iterational 
    search by a certain step length, until gradient equals zero. 

#### Q27 How can outlier values be treated?

    Outlier values can be identified by using univariate or any other graphical analysis method. If the number of
    outlier values is few then they can be assessed individually but for large number of outliers the values can
    be substituted with either the 99th or the 1st percentile values. All extreme values are not outlier values.
    The most common ways to treat outlier values –
    
1. To change the value and bring in within a range

2. To just remove the value.

#### Q28 How can you assess a good logistic model?

    There are various methods to assess the results of a logistic regression analysis-
    
- Using Classification Matrix to look at the true negatives and false positives.
- Concordance that helps identify the ability of the logistic model to differentiate between the event happening and not happening.
- Lift helps assess the logistic model by comparing it with random selection.

#### Q29 What are various steps involved in an analytics project?

- Understand the business problem
- Explore the data and become familiar with it.
- Prepare the data for modelling by detecting outliers, treating missing values, transforming variables, etc.
- After data preparation, start running the model, analyse the result and tweak the approach. This is an iterative step till the best possible outcome is achieved.
- Validate the model using a new data set.
- Start implementing the model and track the result to analyse the performance of the model over the period of time.

#### Q30 How can you iterate over a list and also retrieve element indices at the same time?

    This can be done using the enumerate function which takes every element in a sequence just like in a list
    and adds its location just before it.
    
#### Q31 During analysis, how do you treat missing values?

Minsing values has many reasons, like:
- Information not advisable for this time
- Information was missed by collect
- Some attributes of some items are not avaliable
- Some information was thinked not important
- It's too expensive to collect all these data
    
Types of Missing values:
- Missing completely at Random (MCAR): no relationship with missing values and other variables, like 
    family adress
- Missing at random (MAR): not completely random, missing denpends on other variables, like finance situation
    data missing has relationship with the company size
- Missing not at random (MNAR): there is relationship with the value of variable self, like high income families 
    don't will to open its income situation
      
Methods treatment (you need to know clearly about your missing values firstly)
- Delect tuple
    Delect tuples have any missing values
    - List wise delection
    - Pair wise delection
![](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Data_Exploration_2_2.png)

- Imputation
    - Filling manually
    - Treating Missing Attribute values as Special values (mean, mode, median imputation)
    - Hot deck imputation
    - KNN 
    - Assigning All Possible values of the Attribute
    - Combinational Completer
    - Regression
    - Expectation maximization, EM
    - Multiple Imputation

[More Reading (In Chinese)](http://blog.csdn.net/lujiandong1/article/details/52654703)

[Python package](https://pypi.python.org/pypi/fancyimpute)

~~#### Q32 Explain about the box cox transformation in regression models.~~

#### Q33 Can you use machine learning for time series analysis?

    Yes, it can be used but it depends on the applications.
    
#### Q34 Write a function that takes in two sorted lists and outputs a sorted list that is their union. 

    First solution which will come to your mind is to merge two lists and short them afterwards
    **Python code-**
    def return_union(list_a, list_b):
        return sorted(list_a + list_b)
    
    **R code-**
    return_union <- function(list_a, list_b)
    {
    list_c<-list(c(unlist(list_a),unlist(list_b)))
    return(list(list_c[[1]][order(list_c[[1]])]))
    }

    Generally, the tricky part of the question is not to use any sorting or ordering function. In that 
    case you will have to write your own logic to answer the question and impress your interviewer.
    
    ***Python code-***
    def return_union(list_a, list_b):
        len1 = len(list_a)
        len2 = len(list_b)
        final_sorted_list = []
        j = 0
        k = 0
    
        for i in range(len1+len2):
            if k == len1:
                final_sorted_list.extend(list_b[j:])
                break
            elif j == len2:
                final_sorted_list.extend(list_a[k:])
                break
            elif list_a[k] < list_b[j]:
                final_sorted_list.append(list_a[k])
                k += 1
            else:
                final_sorted_list.append(list_b[j])
                j += 1
        return final_sorted_list

    Similar function can be returned in R as well by following the similar steps.

    return_union <- function(list_a,list_b)
    {
    #Initializing length variables
    len_a <- length(list_a)
    len_b <- length(list_b)
    len <- len_a + len_b
    
    #initializing counter variables
    
    j=1
    k=1
    
    #Creating an empty list which has length equal to sum of both the lists
    
    list_c <- list(rep(NA,len))
    
    #Here goes our for loop 
    
    for(i in 1:len)
    {
        if(j>len_a)
        {
            list_c[i:len] <- list_b[k:len_b]
            break
        }
        else if(k>len_b)
        {
            list_c[i:len] <- list_a[j:len_a]
            break
        }
        else if(list_a[[j]] <= list_b[[k]])
        {
            list_c[[i]] <- list_a[[j]]
            j <- j+1
        }
        else if(list_a[[j]] > list_b[[k]])
        {
        list_c[[i]] <- list_b[[k]]
        k <- k+1
        }
    }
    return(list(unlist(list_c)))

    }
#### Q35 What is the difference between Bayesian Inference and Maximum Likelihood Estimation (MLE)?

#### Q36 What is Regularization and what kind of problems does regularization solve?
    A central problem in machine learning is how to make an algorithm that will perform weel not just on
    the training data, but also on new inputs. Many strategies used in machine learning are explicitly 
    designed to reduce the test error, possibly at the expense of increased training error. These 
    strategies are known collectively as regularization.
    Briefly, regularization is any modification we make to a learning algorithm that is intended to 
    reduce its generalization error but not its training error.

#### Q37 What is multicollinearity and how you can overcome it?
    In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor
    variables in a multiple regression model are highly correlated, meaning that one can be linearly 
    predicted from the others with a substantial degree of accuracy. 
    Solutions:
        Remove variables that lead to multicollinearity.
        Obtain more data.
        Ridge regression or PCA (principal component regression) or partial least squares regression
[More reading in WIKI](https://en.wikipedia.org/wiki/Multicollinearity)

#### Q38 What is the curse of dimensionality?
    It refers to various phenomena that arise when analyzing and organizing data in high-dimensional 
    spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional
    settings.

#### Q39 How do you decide whether your linear regression model fits the data?
    Many solutions, such as use a loss function and check it situation, or use test data to verify 
    our model

~~#### Q40 What is the difference between squared error and absolute error?~~

#### Q41 What is Machine Learning?

    The simplest way to answer this question is – we give the data and equation to the machine. Ask the
    machine to look at the data and identify the coefficient values in an equation.

    For example for the linear regression y=mx+c, we give the data for the variable x, y and the machine
    learns about the values of m and c from the data.
    
#### Q42 How are confidence intervals constructed and how will you interpret them?
    Confidence interval is: under a certain confidence, the length of the area where the overall parameter
    is located. 

#### Q43 How will you explain logistic regression to an economist, physican scientist and biologist?

#### Q44 How can you overcome Overfitting?
    Regularization: add a regularizer or a penalty term.
    Cross Validation: Simple cross validation; S-folder cross validation; Leave-one-out cross validation.  

#### Q45 Differentiate between wide and tall data formats?
    Wide: data formats have lots of columns.
    Tall: data formats have lots of examples.

#### Q46 Is Naïve Bayes bad? If yes, under what aspects.

#### Q47 How would you develop a model to identify plagiarism?

#### Q48 How will you define the number of clusters in a clustering algorithm?

    Though the Clustering Algorithm is not specified, this question will mostly be asked in reference to
    K-Means clustering where “K” defines the number of clusters. The objective of clustering is to group 
    similar entities in a way that the entities within a group are similar to each other but the groups 
    are different from each other.

    For example, the following image shows three different groups.
    
![](https://s3.amazonaws.com/files.dezyre.com/images/blog/100+Data+Science+Interview+Questions+and+Answers+(General)/Data+Science+Interview+Questions+K-Means+Clustering.jpg)

    K-Mean Clustering Machine Learning Algorithm

    Within Sum of squares is generally used to explain the homogeneity within a cluster. If you plot WSS 
    for a range of number of clusters, you will get the plot shown below. The Graph is generally known as 
    Elbow Curve.
    
![](https://s3.amazonaws.com/files.dezyre.com/images/blog/100+Data+Science+Interview+Questions+and+Answers+(General)/Data+Science+Interview+Questions+K-Means.png)

    Red circled point in above graph i.e. Number of Cluster =6 is the point after which you don’t see any 
    decrement in WSS. This point is known as bending point and taken as K in K – Means.

    This is the widely used approach but few data scientists also use Hierarchical clustering first to 
    create dendograms and identify the distinct groups from there.
#### Q49 Is it better to have too many false negatives or too many false positives?
    It depends on the situation, for example, if we use the model for cancer detection, FN(False Negative)
    is more serious than FP(False Positive) because a FN could be verified in futher check, but
    FP maybe will let a patient be missed and delay the best treatment period.

#### Q50 Is it possible to perform logistic regression with Microsoft Excel?
    Yep, i must say Microsoft Excel is more and more powerful, and many data science could be 
    realized in simple way.

#### Q51 What do you understand by Fuzzy merging ? Which language will you use to handle it?

#### Q51 What is the difference between skewed and uniform distribution?

#### G52 You created a predictive model of a quantitative outcome variable using multiple regressions. What are the steps you would follow to validate the model?

    Since the question asked, is about post model building exercise, we will assume that you have 
    already tested for null hypothesis, multi collinearity and Standard error of coefficients.
    
    Once you have built the model, you should check for following –
- Global F-test to see the significance of group of independent variables on dependent variable
- R^2
- Adjusted R^2
- RMSE, MAPE

In addition to above mentioned quantitative metrics you should also check for-
- Residual plot
- Assumptions of linear regression 

#### Q54 What do you understand by Hypothesis in the content of Machine Learning?

#### Q55 What do you understand by Recall and Precision?

#### Q56 How will you find the right K for K-means?
    No any other way just do experiment on instance dataset, see the result of different K, find
    the better one. 

#### Q57 Why L1 regularizations causes parameter sparsity whereas L2 regularization does not?

    Regularizations in statistics or in the field of machine learning is used to include some extra 
    information in order to solve a problem in a better way. L1 & L2 regularizations are generally used 
    to add constraints to optimization problems.

![](https://s3.amazonaws.com/files.dezyre.com/images/blog/100+Data+Science+Interview+Questions+and+Answers+(General)/L1+L2+Regularizations.png)

    In the example shown above H0 is a hypothesis. If you observe, in L1 there is a high likelihood to 
    hit the corners as solutions while in L2, it doesn’t. So in L1 variables are penalized more as compared
    to L2 which results into sparsity.
    In other words, errors are squared in L2, so model sees higher error and tries to minimize that squared 
    error.
    
#### Q58 How can you deal with different types of seasonality in time series modelling?

#### Q59 In experimental design, is it necessary to do randomization? If yes, why?
    Normally yes, but never do it for time series dataset.

#### Q60 What do you understand by conjugate-prior with respect to Naïve Bayes?

#### Q61 Can you cite some examples where a false positive is important than a false negative?

    Before we start, let us understand what are false positives and what are false negatives.
    False Positives are the cases where you wrongly classified a non-event as an event a.k.a Type I error.
    And, False Negatives are the cases where you wrongly classify events as non-events, a.k.a Type II error.
    
![](https://s3.amazonaws.com/files.dezyre.com/images/blog/100+Data+Science+Interview+Questions+and+Answers+(General)/False+Positive+False+Negative.png)
    
    In medical field, assume you have to give chemo therapy to patients. Your lab tests patients for certain 
    vital information and based on those results they decide to give radiation therapy to a patient.
    Assume a patient comes to that hospital and he is tested positive for cancer (But he doesn’t have cancer) 
    based on lab prediction. What will happen to him? (Assuming Sensitivity is 1)

    One more example might come from marketing. Let’s say an ecommerce company decided to give $1000 Gift 
    voucher to the customers whom they assume to purchase at least $5000 worth of items. They send free voucher 
    mail directly to 100 customers without any minimum purchase condition because they assume to make at 
    least 20% profit on sold items above 5K.

    Now what if they have sent it to false positive cases? 
    
#### Q62 Can you cite some examples where a false negative important than a false positive?

    Assume there is an airport ‘A’ which has received high security threats and based on certain 
    characteristics they identify whether a particular passenger can be a threat or not. Due to shortage 
    of staff they decided to scan passenger being predicted as risk positives by their predictive model.
    What will happen if a true threat customer is being flagged as non-threat by airport model?
    
    Another example can be judicial system. What if Jury or judge decide to make a criminal go free?
    
    What if you rejected to marry a very good person based on your predictive model and you happen to
    meet him/her after few years and realize that you had a false negative?
    
#### Q63 Can you cite some examples where both false positive and false negatives are equally important?

    In the banking industry giving loans is the primary source of making money but at the same time if 
    your repayment rate is not good you will not make any profit, rather you will risk huge losses.
    
    Banks don’t want to lose good customers and at the same point of time they don’t want to acquire 
    bad customers. In this scenario both the false positives and false negatives become very important 
    to measure.

#### Q64 Can you explain the difference between a Test Set and a Validation Set?

    Validation set can be considered as a part of the training set as it is used for parameter selection
    and to avoid Overfitting of the model being built. On the other hand, test set is used for testing 
    or evaluating the performance of a trained machine leaning model.

    In simple terms ,the differences can be summarized as-
    
-   Training Set is to fit the parameters i.e. weights.
-   Test Set is to assess the performance of the model i.e. evaluating the predictive power and generalization.
-   Validation set is to tune the parameters.

#### Q65 What makes a dataset gold standard?
    

#### Q66 What do you understand by statistical power of sensitivity and how do you calculate it?

    Sensitivity is commonly used to validate the accuracy of a classifier (Logistic, SVM, RF etc.). 
    Sensitivity is nothing but “Predicted TRUE events/ Total events”. True events here are the events
    which were true and model also predicted them as true.
    
    Calculation of seasonality is pretty straight forward-
    
    ***Seasonality = True Positives /Positives in Actual Dependent Variable***
    
    Where, True positives are Positive events which are correctly classified as Positives.
    
#### Q67 What is the importance of having a selection bias?

#### Q68 Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa.

    SVM and Random Forest are both used in classification problems.
    
    a)      If you are sure that your data is outlier free and clean then go for SVM. It is the 
    opposite - if your data might contain outliers then Random forest would be the best choice
    b)      Generally, SVM consumes more computational power than Random Forest, so if you are constrained 
    with memory go for Random Forest machine learning algorithm.
    c)  Random Forest gives you a very good idea of variable importance in your data, so if you want to 
    have variable importance then choose Random Forest machine learning algorithm.
    d)      Random Forest machine learning algorithms are preferred for multiclass problems.
    e)     SVM is preferred in multi-dimensional problem set - like text classification
    but as a good data scientist, you should experiment with both of them and test for accuracy or rather 
    you can use ensemble of many Machine Learning techniques.

#### Q69 What do you understand by feature vectors?

~~#### Q70 How do data management procedures like missing data handling make selection bias worse?~~

#### Q71 What are the advantages and disadvantages of using regularization methods like Ridge Regression?

~~#### Q72 What do you understand by long and wide data formats?~~

#### Q73 What do you understand by outliers and inliers? What would you do if you find them in your dataset?

~~#### Q74 Write a program in Python which takes input as the diameter of a coin and weight of the coin and produces output as the money value of the coin.

#### Q75 What are the basic assumptions to be made for linear regression?

    Normality of error distribution, statistical independence of errors, linearity and additivity.

#### Q76 Can you write the formula to calculat R-square?

    R-Square can be calculated using the below formular -
    1 - (Residual Sum of Squares/ Total Sum of Squares)

#### Q77 What is the advantage of performing dimensionality reduction before fitting an SVM?

    Support Vector Machine Learning Algorithm performs better in the reduced space. It is beneficial to 
    perform dimensionality reduction before fitting an SVM if the number of features is large when 
    compared to the number of observations.

#### Q78 How will you assess the statistical significance of an insight whether it is a real insight or just by chance?

    Statistical importance of an insight can be accessed using Hypothesis Testing.

## Machine Learning Interview Questions: Algorithms/Theory

#### Q79 What’s the trade-off between bias and variance?
    
    Bias is error due to erroneous or overly simplistic assumptions in the learning algorithm 
    you’re using. This can lead to the model underfitting your data, making it hard for it to have 
    high predictive accuracy and for you to generalize your knowledge from the training set to the 
    test set.

    Variance is error due to too much complexity in the learning algorithm you’re using. This leads 
    to the algorithm being highly sensitive to high degrees of variation in your training data, which 
    can lead your model to overfit the data. You’ll be carrying too much noise from your training data 
    for your model to be very useful for your test data.

    The bias-variance decomposition essentially decomposes the learning error from any algorithm by 
    adding the bias, the variance and a bit of irreducible error due to noise in the underlying dataset. 
    Essentially, if you make the model more complex and add more variables, you’ll lose bias but gain 
    some variance — in order to get the optimally reduced amount of error, you’ll have to tradeoff bias 
    and variance. You don’t want either high bias or high variance in your model.

#### Q80 What is the difference between supervised and unsupervised machine learning?
    
    Supervised learning requires training labeled data. For example, in order to do classification 
    (a supervised learning task), you’ll need to first label the data you’ll use to train the model
    to classify data into your labeled groups. Unsupervised learning, in contrast, does not require
    labeling data explicitly.

#### Q81 How is KNN different from k-means clustering?

    K-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an
    unsupervised clustering algorithm. While the mechanisms may seem similar at first, what this
    really means is that in order for K-Nearest Neighbors to work, you need labeled data you want to 
    classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires only
    a set of unlabeled points and a threshold: the algorithm will take unlabeled points and gradually
    learn how to cluster them into groups by computing the mean of the distance between different points.
    
    The critical difference here is that KNN needs labeled points and is thus supervised learning, while
    k-means doesn’t — and is thus unsupervised learning.

#### Q82 Explain how a ROC curve works.
    
    The ROC curve is a graphical representation of the contrast between true positive rates and the 
    false positive rate at various thresholds. It’s often used as a proxy for the trade-off between
    the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger 
    a false alarm (false positives).
    
![](https://lh3.googleusercontent.com/zUWYO4VwGpoyu9oygT12F3hgZ30GxVY7sg_ZF46INrNbDutd9mVz9GnYIYGw2r1ZcbPLQXF4HV-uNXvQcVrP7Sg2BDDqRkaY3RAApumdXgH2mQZ8OCSgqqsVl7UDVjqwVFq224Z_)
    
#### Q83 Define precision and recall.
    
    Recall is also known as the true positive rate: the amount of positives your model claims 
    compared to the actual number of positives there are throughout the data. Precision is also 
    known as the positive predictive value, and it is a measure of the amount of accurate positives
    your model claims compared to the number of positives it actually claims. It can be easier to think
    of recall and precision in the context of a case where you’ve predicted that there were 10 apples
    and 5 oranges in a case of 10 apples. You’d have perfect recall (there are actually 10 apples, and
    you predicted there would be 10) but 66.7% precision because out of the 15 events you predicted,
    only 10 (the apples) are correct.

#### Q84 What is Bayes’ Theorem? How is it useful in a machine learning context?
    
    Bayes’ Theorem gives you the posterior probability of an event given what is known as prior knowledge.
    
    Mathematically, it’s expressed as the true positive rate of a condition sample divided by the sum of 
    the false positive rate of the population and the true positive rate of a condition. Say you had a 
    60% chance of actually having the flu after a flu test, but out of people who had the flu, the test 
    will be false 50% of the time, and the overall population only has a 5% chance of having the flu. 
    Would you actually have a 60% chance of having the flu after having a positive test?
    
    Bayes’ Theorem says no. It says that you have a (.6 * 0.05) (True Positive Rate of a Condition 
    Sample) / (.6*0.05)(True Positive Rate of a Condition Sample) + (.5*0.95) (False Positive Rate of
    a Population)  = 0.0594 or 5.94% chance of getting a flu.

    Bayes’ Theorem is the basis behind a branch of machine learning that most notably includes the
    Naive Bayes classifier. That’s something important to consider when you’re faced with machine 
    learning interview questions.

#### Q85 Why is “Naive” Bayes naive?

    Despite its practical applications, especially in text mining, Naive Bayes is considered “Naive” 
    because it makes an assumption that is virtually impossible to see in real-life data: the 
    conditional probability is calculated as the pure product of the individual probabilities of 
    components. This implies the absolute independence of features — a condition probably never met 
    in real life.

    As a Quora commenter put it whimsically, a Naive Bayes classifier that figured out that you liked 
    pickles and ice cream would probably naively recommend you a pickle ice cream.

#### Q86 Explain the difference between L1 and L2 regularization.

    L2 regularization tends to spread error among all the terms, while L1 is more binary/sparse, with
    many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a Laplacean
    prior on the terms, while L2 corresponds to a Gaussian prior.
    
![](https://lh6.googleusercontent.com/vXUSHKE11Qpolek11IPPP6Fs-iU1-LeWtf5EXVdrfOl97ytug_cME-vLF1t4BNvoAppxfRhx4dNzHoKkdl8dfGVix4jc2hhvrtDG_wyuByxpVfeFZQdMH-INzG6RSi_9jkJLERto)

#### Q87 What’s your favorite algorithm, and can you explain it to me in less than a minute?

    This type of question tests your understanding of how to communicate complex and technical nuances 
    with poise and the ability to summarize quickly and efficiently. Make sure you have a choice and 
    make sure you can explain different algorithms so simply and effectively that a five-year-old could
    grasp the basics!

#### Q88 What’s the difference between Type I and Type II error?

    Don’t think that this is a trick question! Many machine learning interview questions will be an 
    attempt to lob basic questions at you just to make sure you’re on top of your game and you’ve
    prepared all of your bases.

    Type I error is a false positive, while Type II error is a false negative. Briefly stated, Type I 
    error means claiming something has happened when it hasn’t, while Type II error means that you claim 
    nothing is happening when in fact something is.

    A clever way to think about this is to think of Type I error as telling a man he is pregnant, while
    Type II error means you tell a pregnant woman she isn’t carrying a baby.

#### Q89 What’s a Fourier transform?

    A Fourier transform is a generic method to decompose generic functions into a superposition of symmetric
    functions. Or as this more intuitive tutorial puts it, given a smoothie, it’s how we find the recipe. The 
    Fourier transform finds the set of cycle speeds, amplitudes and phases to match any time signal. A Fourier
    transform converts a signal from time to frequency domain — it’s a very common way to extract features from
    audio signals or other time series such as sensor data.

#### Q90 What’s the difference between probability and likelihood?

![](https://lh3.googleusercontent.com/Yz2xAzLEEjtk62o9zatSDZJ7yBwgw-a1GtSNfAjJ3tq3OY5UbnxYUpNOqAuuKAUj8kVZaraIsr87kX83ejzg2y8DW9goGJbZuPc1Be_2VmGEEsNZ5JMioUw6Xke-KvYzp-sVrLCL)

#### Q91 What is deep learning, and how does it contrast with other machine learning algorithms?

    Deep learning is a subset of machine learning that is concerned with neural networks: how to use
    backpropagation and certain principles from neuroscience to more accurately model large sets of
    unlabelled or semi-structured data. In that sense, deep learning represents an unsupervised learning
    algorithm that learns representations of data through the use of neural nets.

#### Q92 What’s the difference between a generative and discriminative model?

    A generative model will learn categories of data while a discriminative model will simply learn the 
    distinction between different categories of data. Discriminative models will generally outperform 
    generative models on classification tasks.

#### Q93 What cross-validation technique would you use on a time series dataset?

    Instead of using standard k-folds cross-validation, you have to pay attention to the fact that a 
    time series is not randomly distributed data — it is inherently ordered by chronological order. If a 
    pattern emerges in later time periods for example, your model may still pick up on it even if that 
    effect doesn’t hold in earlier years!

    You’ll want to do something like forward chaining where you’ll be able to model on past data then
    look at forward-facing data.

    fold 1 : training [1], test [2]
    fold 2 : training [1 2], test [3]
    fold 3 : training [1 2 3], test [4]
    fold 4 : training [1 2 3 4], test [5]
    fold 5 : training [1 2 3 4 5], test [6]
#### Q94 How is a decision tree pruned?

    Pruning is what happens in decision trees when branches that have weak predictive power are removed 
    in order to reduce the complexity of the model and increase the predictive accuracy of a decision 
    tree model. Pruning can happen bottom-up and top-down, with approaches such as reduced error pruning 
    and cost complexity pruning.

    Reduced error pruning is perhaps the simplest version: replace each node. If it doesn’t decrease 
    predictive accuracy, keep it pruned. While simple, this heuristic actually comes pretty close to an 
    approach that would optimize for maximum accuracy.

#### Q95 Which is more important to you– model accuracy, or model performance?

    This question tests your grasp of the nuances of machine learning model performance! Machine learning 
    interview questions often look towards the details. There are models with higher accuracy that can 
    perform worse in predictive power — how does that make sense?
    
    Well, it has everything to do with how model accuracy is only a subset of model performance, and at 
    that, a sometimes misleading one. For example, if you wanted to detect fraud in a massive dataset with
    a sample of millions, a more accurate model would most likely predict no fraud at all if only a vast 
    minority of cases were fraud. However, this would be useless for a predictive model — a model designed
    to find fraud that asserted there was no fraud at all! Questions like this help you demonstrate that 
    you understand model accuracy isn’t the be-all and end-all of model performance.

#### Q96 What’s the F1 score? How would you use it?

    The F1 score is a measure of a model’s performance. It is a weighted average of the precision and recall
    of a model, with results tending to 1 being the best, and those tending to 0 being the worst. You would
    use it in classification tests where true negatives don’t matter much.

#### Q97 How would you handle an imbalanced dataset?

    An imbalanced dataset is when you have, for example, a classification test and 90% of the data is in one
    class. That leads to problems: an accuracy of 90% can be skewed if you have no predictive power on the
    other category of data! Here are a few tactics to get over the hump:

    1- Collect more data to even the imbalances in the dataset.

    2- Resample the dataset to correct for imbalances.

    3- Try a different algorithm altogether on your dataset.

    What’s important here is that you have a keen sense for what damage an unbalanced dataset can cause, 
    and how to balance that.

#### Q98 When should you use classification over regression?

    Classification produces discrete values and dataset to strict categories, while regression gives you
    continuous results that allow you to better distinguish differences between individual points. You would
    use classification over regression if you wanted your results to reflect the belongingness of data points 
    in your dataset to certain explicit categories (ex: If you wanted to know whether a name was male or 
    female rather than just how correlated they were with male and female names.)

#### Q99 Name an example where ensemble techniques might be useful.

    Ensemble techniques use a combination of learning algorithms to optimize better predictive performance.
    They typically reduce overfitting in models and make the model more robust (unlikely to be influenced by 
    small changes in the training data). 

    You could list some examples of ensemble methods, from bagging to boosting to a “bucket of models” method
    and demonstrate how they could increase predictive power.

#### Q100 How do you ensure you’re not overfitting with a model?

    This is a simple restatement of a fundamental problem in machine learning: the possibility of 
    overfitting training data and carrying the noise of that data through to the test set, thereby
    providing inaccurate generalizations.

    There are three main methods to avoid overfitting:

    1- Keep the model simpler: reduce variance by taking into account fewer variables and parameters, 
    thereby removing some of the noise in the training data.

    2- Use cross-validation techniques such as k-folds cross-validation.

    3- Use regularization techniques such as LASSO that penalize certain model parameters if they’re 
    likely to cause overfitting.

#### Q101 What evaluation approaches would you work to gauge the effectiveness of a machine learning model?

    You would first split the dataset into training and test sets, or perhaps use cross-validation
    techniques to further segment the dataset into composite sets of training and test sets within 
    the data. You should then implement a choice selection of performance metrics: here is a fairly
    comprehensive list. You could use measures such as the F1 score, the accuracy, and the confusion 
    matrix. What’s important here is to demonstrate that you understand the nuances of how a model is
    measured and how to choose the right performance measures for the right situations.

#### Q102 How would you evaluate a logistic regression model?

    A subsection of the question above. You have to demonstrate an understanding of what the typical goals 
    of a logistic regression are (classification, prediction etc.) and bring up a few examples and use cases.

#### Q103 What’s the “kernel trick” and how is it useful?

    The Kernel trick involves kernel functions that can enable in higher-dimension spaces without explicitly 
    calculating the coordinates of points within that dimension: instead, kernel functions compute the inner 
    products between the images of all pairs of data in a feature space. This allows them the very useful 
    attribute of calculating the coordinates of higher dimensions while being computationally cheaper than 
    the explicit calculation of said coordinates. Many algorithms can be expressed in terms of inner products.
    Using the kernel trick enables us effectively run  algorithms in a high-dimensional space with lower-dimensional data.

## Machine Learning Interview Questions: Programming
These machine learning interview questions test your knowledge of programming principles you need to 
implement machine learning principles in practice. Machine learning interview questions tend to be technical
questions that test your logic and programming skills: this section focuses more on the latter.

~~#### Q104 How do you handle missing or corrupted data in a dataset?~~

#### Q105 Do you have experience with Spark or big data tools for machine learning?

    You’ll want to get familiar with the meaning of big data for different companies and the different
    tools they’ll want. Spark is the big data tool most in demand now, able to handle immense datasets
    with speed. Be honest if you don’t have experience with the tools demanded, but also take a look at
    job descriptions and see what tools pop up: you’ll want to invest in familiarizing yourself with them.

#### Q106 Pick an algorithm. Write the psuedo-code for a parallel implementation.

    This kind of question demonstrates your ability to think in parallelism and how you could handle 
    concurrency in programming implementations dealing with big data. Take a look at pseudocode frameworks
    such as Peril-L and visualization tools such as Web Sequence Diagrams to help you demonstrate your 
    ability to write code that reflects parallelism.

#### Q107 What are some differences between a linked list and an array?

    An array is an ordered collection of objects. A linked list is a series of objects with pointers that
    direct how to process them sequentially. An array assumes that every element has the same size, unlike 
    the linked list. A linked list can more easily grow organically: an array has to be pre-defined or 
    re-defined for organic growth. Shuffling a linked list involves changing which points direct where — 
    meanwhile, shuffling an array is more complex and takes more memory.

#### Q108 Describe a hash table.

    A hash table is a data structure that produces an associative array. A key is mapped to certain values 
    through the use of a hash function. They are often used for tasks such as database indexing.

#### Q109 Which data visualization libraries do you use? What are your thoughts on the best data visualization tools?

    What’s important here is to define your views on how to properly visualize data and your personal 
    preferences when it comes to tools. Popular tools include R’s ggplot, Python’s seaborn and matplotlib,
    and tools such as Plot.ly and Tableau.
    
![](https://lh3.googleusercontent.com/79d5jkZBgpZPQa61A4e9opgfX2-mrxWxfQyswec3YxBouNEvAu8wYxjCXNQl-nRdBVQeuco1h-LZbxVblgS9h6bYLi6peoqSd2N7VW7BSeBgpmclKng6IRYEf9QkTMRJKMyPxrCT)

## Machine Learning Interview Questions: Company/Industry Specific

These machine learning interview questions deal with how to implement your general machine learning knowledge 
to a specific company’s requirements. You’ll be asked to create case studies and extend your knowledge of the
company and industry you’re applying for with your machine learning skills.

#### Q110 How would you implement a recommendation system for our company’s users?

    A lot of machine learning interview questions of this type will involve implementation of machine learning
    models to a company’s problems. You’ll have to research the company and its industry in-depth, especially 
    the revenue drivers the company has, and the types of users the company takes on in the context of the 
    industry it’s in.

#### Q111 How can we use your machine learning skills to generate revenue?

    This is a tricky question. The ideal answer would demonstrate knowledge of what drives the business and 
    how your skills could relate. For example, if you were interviewing for music-streaming startup Spotify, 
    you could remark that your skills at developing a better recommendation model would increase user retention,
    which would then increase revenue in the long run.

    The startup metrics Slideshare linked above will help you understand exactly what performance indicators 
    are important for startups and tech companies as they think about revenue and growth.

#### Q112 What do you think of our current data process?

    This kind of question requires you to listen carefully and impart feedback in a manner that is constructive 
    and insightful. Your interviewer is trying to gauge if you’d be a valuable member of their team and whether
    you grasp the nuances of why certain things are set the way they are in the company’s data process based on
    company- or industry-specific conditions. They’re trying to see if you can be an intellectual peer. Act 
    accordingly.

## Machine Learning Interview Questions: General Machine Learning Interest

This series of machine learning interview questions attempts to gauge your passion and interest in machine learning.
The right answers will serve as a testament for your commitment to being a lifelong learner in machine learning.

#### Q113 What are the last machine learning papers you’ve read?

    Keeping up with the latest scientific literature on machine learning is a must if you want to demonstrate
    interest in a machine learning position. This overview of deep learning in Nature by the scions of deep 
    learning themselves (from Hinton to Bengio to LeCun) can be a good reference paper and an overview of what’s 
    happening in deep learning — and the kind of paper you might want to cite.

#### Q114 Do you have research experience in machine learning?

    Related to the last point, most organizations hiring for machine learning positions will look for your 
    formal experience in the field. Research papers, co-authored or supervised by leaders in the field, can make 
    the difference between you being hired and not. Make sure you have a summary of your research experience 
    and papers ready — and an explanation for your background and lack of formal research experience if you don’t.

#### Q115 What are your favorite use cases of machine learning models?

    The Quora thread above contains some examples, such as decision trees that categorize people into different 
    tiers of intelligence based on IQ scores. Make sure that you have a few examples in mind and describe what 
    resonated with you. It’s important that you demonstrate an interest in how machine learning is implemented.

#### Q116 How would you approach the “Netflix Prize” competition?

    The Netflix Prize was a famed competition where Netflix offered $1,000,000 for a better collaborative 
    filtering algorithm. The team that won called BellKor had a 10% improvement and used an ensemble of different
    methods to win. Some familiarity with the case and its solution will help demonstrate you’ve paid attention 
    to machine learning for a while.

#### Q117 Where do you usually source datasets?

    Machine learning interview questions like these try to get at the heart of your machine learning interest.
    Somebody who is truly passionate about machine learning will have gone off and done side projects on their own, 
    and have a good idea of what great datasets are out there. If you’re missing any, check out Quandl for economic
    and financial data, and Kaggle’s Datasets collection for another great list.

#### Q118 How do you think Google is training data for self-driving cars?

    Machine learning interview questions like this one really test your knowledge of different machine learning 
    methods, and your inventiveness if you don’t know the answer. Google is currently using recaptcha to source 
    labelled data on storefronts and traffic signs. They are also building on training data collected by Sebastian
    Thrun at GoogleX — some of which was obtained by his grad students driving buggies on desert dunes!

#### Q119 How would you simulate the approach AlphaGo took to beat Lee Sidol at Go?

    AlphaGo beating Lee Sidol, the best human player at Go, in a best-of-five series was a truly seminal event
    in the history of machine learning and deep learning. The Nature paper above describes how this was accomplished
    with “Monte-Carlo tree search with deep neural networks that have been trained by supervised learning,
    from human expert games, and by reinforcement learning from games of self-play.”


[Reference from dezyre](https://www.dezyre.com/article/100-data-science-interview-questions-and-answers-general-for-2017/184 "悬停显示")

[Rererence from Springbord](https://www.springboard.com/blog/machine-learning-interview-questions/?from=message&isappinstalled=0 "悬停显示")

Reference: Deep Learning (Ian Goodfellow, Yoshua Bengio and Aaron Courville) -- MIT



[![HitCount](http://hits.dwyl.io/{username}/{repo}.svg)](http://hits.dwyl.io/{rbhatia46}/{Data-Science-Interview-Resources})
![Star this repository](https://img.shields.io/github/stars/rbhatia46/Data-Science-Interview-Resources?style=social)


# Data-Science-Interview-Resources

First of all, thanks for visiting this repo, congratulations on making a great career choice, I aim to help you land an amazing Data Science job that you have been dreaming for, by sharing my experience, interviewing heavily at both large product-based companies and fast-growing startups, hope you find it useful.

With an increase in demand for so many Data Scientists, it's really hard to successfully get screened and accepted for an interview. In this repo, I include everything from getting successfully screened and rocking that interview to land that amazing position, make sure to nail it with the following resources.

Every Resource I list here is personally verified by me and most of them I have used personally, which have helped me a lot.

**Word of Caution:** Data Science/Machine Learning has a very big domain and there are a lot of things to learn. This by no means is an exhaustive list and is just for helping you out if you are struggling to find some good resources to start your preparation. However, I try to cover and update this frequently and my goal is to cover and unify everything into one resource that you can use to rock those interviews! Please leave a star if you appreciate the effort.

**Note:** For contribution, refer [Contribution.md](https://github.com/rbhatia46/Data-Science-Interview-Resources/blob/master/Contribution.md)

## How to get an interview ?

* First and foremost, **develop the necessary skills and be sound with the fundamentals**, these are some of the horizons you should be extremely comfortable with - 
  - Business Understanding(this is extremely critical across all seniority levels, but specifically for people with more than 3 years of experience)
  - SQL and Databases(very crucial)
  - Programming Skills(preferably in Python, if you know Scala, extra brownie points for some specific roles)
  - Mathematics(Probability, Statistics, Linear Algebra and Calculus) - https://medium.com/@rbhatia46/essential-probability-statistics-concepts-before-data-science-bb787b7a5aef 
  - Machine Learning(this includes Deep Learning) and Model building
  - Data Structures and Algorithms(must and mandatory for top product based companies like FAANG)
  - Domain Understanding(Optional for most openings, though very critical for some roles based on company's requirement)
  - Literature Review(must for Research based roles) : Being able to read and understand a new research paper is one of the most essential and demanding skills needed in the industry today, as the culture of Research and Development, and innovation grows across most good organizations.
  - Communication Skills - Being able to explain the analysis and results to business stakeholders and executives is becoming a really important skill for Data Scientists these days
  - Some Engineering knowledge(Not mandatory, but good to have) - Being able to develop a RESTful API, writing clean and elegant code, Object Oriented programming are some of the things you can focus on for some extra brownie points.
  - Big data knowledge(not mandatory for most openings, but good to have) - Spark, Hive, Hadoop, Sqoop.

* **Build a personal Brand** 
  - Develop a good GitHub/portfolio of use-cases you have solved, always strive for solving end-to-end use cases, which demonstrate the entire Data Science lifecycle, from business understanding to model deployment. 
  - Write blogs, start a YouTube channel if you enjoy teaching, write a book.
  - Work on a digital, easy-to-open, easy-to-read, clean, concise and easily customizable Resume/CV, always include your demo links and source code of every use-case you have solved.
  - Participate in Kaggle competitions, build a good Kaggle profile and send them to potential employers for increasing the chances of getting an interview call real-quick.

* **Develop good connections**, through LinkedIn, by attending conferences, and doing everything you can, it's very important to land referrals and get yourself started with the interview process through good connections. Connect regularly with Data Scientists working at top product-based organizations, fast-growing startups, build a network, slowly and steadily, it's very important.`

## Some Tips on Resume/CV:
* Describe past roles and an impact you made in a **quantifiable** way, be concise and I repeat, **quantify** the impact, rather than talking with facts that have no relevance. According to Google Recruiters, use the XYZ formula - 
```Accomplished [X] as measured by [Y], by doing [Z]```

* Keep it short, ideally not more than 2 pages, as you might know, an average recruiter scans your resume only for 6 seconds, and makes a decision based on that.

* If you are a fresher and don't have experience, try to solve end-to-end use-cases and mention them in your CV, preferably with the demo link(makes it easy for the recruiter) and the link to source code on GitHub.

* Avoid too much technical jargon, and this goes without saying, do not mention anything you are not confident about, this might become a major bottleneck during your interview.

* Some helpful links :
  * [Advice on building Data Portfolio Projects](https://medium.com/@jasonkgoodman/advice-on-building-data-portfolio-projects-c5f96d8a0627) 📘
  * [How to write a killer Software Engineering Resume](https://www.freecodecamp.org/news/writing-a-killer-software-engineering-resume-b11c91ef699d/) 📘
  * [Get your Data Science Resume past the ATS](https://towardsdatascience.com/up-level-your-data-science-resume-getting-past-ats-64322f0cbb73) 📘
  * [How to write a developer résumé that hiring managers will actually read](https://www.freecodecamp.org/news/how-to-write-a-resume-that-works/) 📘

***

## Probability and Statistics
* [Understand the basics of Descriptive Statistics(Really Important for an interview)](https://towardsdatascience.com/understanding-descriptive-statistics-c9c2b0641291) 📘
* [40 Question on **probability** for a Data Science Interview](https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-probability-for-all-aspiring-data-scientists/) 📘
* [40 Statistics Interview Problems and Answers for Data Scientists](https://towardsdatascience.com/40-statistics-interview-problems-and-answers-for-data-scientists-6971a02b7eee) 📘
* [Probability and Statistics in the context of Deep Learning](https://towardsdatascience.com/probability-and-statistics-explained-in-the-context-of-deep-learning-ed1509b2eb3f) 📘
* [Probability v/s Likelihood](https://www.youtube.com/watch?v=pYxNSUDSFH4) 📹
* [Bootstrap Methods - The Swiss Army Knife of any Data Scientist](https://medium.com/data-science-journal/the-bootstrap-the-swiss-army-knife-of-any-data-scientist-acd6e592be13) 📘
* [Confidence Intervals Explained Simply for Data Scientists](https://mlwhiz.com/blog/2020/02/21/ci/) 📘
* [P-value Explained Simply for Data Scientists](https://towardsdatascience.com/p-value-explained-simply-for-data-scientists-4c0cd7044f14) 📘
* [PDF is not a probability](https://towardsdatascience.com/pdf-is-not-a-probability-5a4b8a5d9531) 📘
* [5 Sampling algorithms every Data Scientist should know](https://mlwhiz.com/blog/2019/07/30/sampling/) 📘
* [The 10 Statistical Techniques Data Scientists Need to Master](https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html) 📘

***

## SQL and Data Acquisition
This is probably the entry point of your Data Science project, SQL is one of the most important skills for any Data Scientist.

* [5 Common SQL Interview Problems for Data Scientists](https://towardsdatascience.com/5-common-sql-interview-problems-for-data-scientists-1bfa02d8bae6) 📘
* [46 Questions to test a Data Scientist on SQL](https://www.analyticsvidhya.com/blog/2017/01/46-questions-on-sql-to-test-a-data-science-professional-skilltest-solution/) 📘
* [30 SQL Interview Questions curated for FAANG by an Ex-Facebook Data Scientist](https://www.nicksingh.com/posts/30-sql-and-database-design-questions-from-real-data-science-interviews) 📘
* [SQL Interview Questions](https://365datascience.com/sql-interview-questions/) 📘
* [How to ace Data Science Interviews - SQL](https://towardsdatascience.com/how-to-ace-data-science-interviews-sql-b71de212e433) 📘
* [3 Must Know SQL Questions to pass your Data Science Interview](https://medium.com/@jayfeng/three-must-know-sql-questions-to-pass-your-data-science-interview-463311c7eaea) 📘
* [10 frequently asked SQL Queries in Interviews](https://www.java67.com/2013/04/10-frequently-asked-sql-query-interview-questions-answers-database.html) 📘
* [Technical Data Science Interview Questions: SQL and Coding](https://hackernoon.com/technical-data-science-interview-questions-sql-and-coding-jv1k32bf) 📘
* [How to optimize SQL Queries - Datacamp](https://www.datacamp.com/community/tutorials/sql-tutorial-query) 📘
* [Ten SQL Concepts You Should Know for Data Science Interviews](https://towardsdatascience.com/ten-sql-concepts-you-should-know-for-data-science-interviews-7acf3e428185) 📘

***
## Data Preparation and Visualization

* [5 Feature Selection Algorithms every Data Scientist should know](https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2) 📘 
* [6 Different Ways to Compensate for Missing Values In a Dataset ](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779) 📘 
* [A Brief Overview of Outlier Detection Techniques](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561) 📘 
* [Cleaning and Prepping Data with Python for Data Science — Best Practices and Helpful Packages](https://medium.com/@rrfd/cleaning-and-prepping-data-with-python-for-data-science-best-practices-and-helpful-packages-af1edfbe2a3) 📘 
* [When to use which plot for visualization](https://towardsdatascience.com/what-plot-why-this-plot-and-why-not-9508a0cb35ea) 📘
* [Ways to detect and remove Outliers](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba) 📘
* [Dealing with Class Imbalances in Machine Learning](https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2) 📘
* [Smarter ways to encode categorical data](https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159)
* [Numpy and Pandas Cheatsheet](https://github.com/jessicayung/data-analyst-nd/blob/master/2-intro-to-data-analysis/numpy_pandas_cheatsheet.pdf) 📘
* [3 Methods to deal with outliers](https://www.kdnuggets.com/2017/01/3-methods-deal-outliers.html) 📘
* [Feature Selection Techniques](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e) 📘
* [Why, how and When to scale your features](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e) 📘
* [Everything you need to know about Scatter plots](https://towardsdatascience.com/everything-you-need-to-know-about-scatter-plots-for-data-visualisation-924144c0bc5) 📘
* [How to Select Features for Machine Learning](https://www.youtube.com/watch?v=YaKMeAlHgqQ) 📹
* [10 ways for Feature Selection](https://www.youtube.com/watch?v=Erp0bjEPoM8) 📹



***
## Classic Machine Learning Algorithms

### 1. Logistic Regression

* [All about Logistic Regression in one article](https://towardsdatascience.com/logistic-regression-b0af09cdb8ad) 📘 
* [Understanding Logistic Regression step-by-step](https://towardsdatascience.com/understanding-logistic-regression-step-by-step-704a78be7e0a) 📘 
* [Logistic Regression - Short and Clear Explanation - 9 Mins](https://www.youtube.com/watch?v=yIYKR4sgzI8) 📹
* [Linear Regression vs Logistic Regression](https://www.youtube.com/watch?v=OCwZyYH14uw) 📹
* [30 Questions to test a Data Scientist on Logistic Regression](https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/) 📘 
* [Logistic Regression - Understand Everything (Theory + Maths + Coding) in 1 video](https://www.youtube.com/watch?v=VCJdg7YBbAQ) 📹


### 2. Linear Regression

* [30 Questions to test a Data Scientist on Linear Regression](https://www.analyticsvidhya.com/blog/2017/07/30-questions-to-test-a-data-scientist-on-linear-regression/) 📘 
* [Linear Regression - Understand Everything (Theory + Maths + Coding) in 1 video](https://www.youtube.com/watch?v=E5RjzSK0fvY) 📹
* [5 Types of Regression and their properties](https://towardsdatascience.com/5-types-of-regression-and-their-properties-c5e1fa12d55e) 📘
* [Ridge Regression - Clearly Explained](https://www.youtube.com/watch?v=Q81RR3yKn30) 📹
* [Lasso Regression - Clearly Explained](https://www.youtube.com/watch?v=NGf0voTMlcs) 📹


### 3. Tree Based/Ensemble Algorithms

* [30 Questions to test a Data Scientist on Tree based models](https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-tree-based-models/) 📘
* [Gini-index v/s Information Entropy](https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb) 📘
* [Decision Tree vs. Random Forest – Which Algorithm Should you Use?](https://www.analyticsvidhya.com/blog/2020/05/decision-tree-vs-random-forest-algorithm/) 📘
* [Why Random Forest doesn't work well for Time-Series?](https://medium.com/datadriveninvestor/why-wont-time-series-data-and-random-forests-work-very-well-together-3c9f7b271631) 📘
* [Comprehensive guide to Ensemble Models](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/) 📘
* [The Simple Math behind 3 Decision Tree Splitting criterions](https://mlwhiz.com/blog/2019/11/12/dtsplits/) 📘


### 4. K-Nearest-Neighbors

* [Fundamental Interview Questions on KNN - A Quick refresh](http://theprofessionalspoint.blogspot.com/2019/01/knn-algorithm-in-machine-learning.html) 📘
* [30 Questions to test a Data Scientist on KNN](https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/) 📘
* [Pros and Cons of KNN](https://www.fromthegenesis.com/pros-and-cons-of-k-nearest-neighbors/) 📘
* [KNN Algorithm - Understand Everything (Theory + Maths + Coding) in 1 video](https://www.youtube.com/watch?v=6kZ-OPLNcgE) 📹

### 5. Support Vector Machines

* [All about SVMs - Math, Terminology, Intuition, Kernels in one article](https://towardsdatascience.com/support-vector-machines-svm-c9ef22815589) 📘
* [25 Questions to test a Data Scientist on SVMs](https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/) 📘

### 6. Naive Bayes

* [12 tips to make most out of Naive Bayes](https://machinelearningmastery.com/better-naive-bayes/) 📘
* [Naive Bayes - Understand Everything (Theory + Maths + Coding) in 1 video](https://www.youtube.com/watch?v=vz_xuxYS2PM) 📹
* [6 easy steps to learn Naive Bayes](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/) 📘


***
## Time Series
* [40 Questions to test a Data Scientist on Time Series](https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-time-series-solution-skillpower-time-series-datafest-2017/) 📘
* [11 Classical Time Series Forecasting Methods](https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/) 📘
* [Interview Questions on ARIMA](https://www.youtube.com/watch?v=654SjiZO5ks) 📹

***
## Unsupervised Learning
* [The DOs and DONTs of PCA(Principal Component Analysis)](https://medium.com/@sadatnazrul/the-dos-and-donts-of-principal-component-analysis-7c2e9dc8cc48) 📘
* [An introduction to t-SNE : DataCamp](https://www.datacamp.com/community/tutorials/introduction-t-sne) 📘
* [Dimensionally Reducing Squeezing out the good stuff](https://www.youtube.com/watch?v=4QMFNg7tjbk) 📘
* [Dimensionality Reduction for Dummies : Part 1 - Intuition](https://towardsdatascience.com/https-medium-com-abdullatif-h-dimensionality-reduction-for-dummies-part-1-a8c9ec7b7e79) 📘
* [In-depth Explanation of DBSCAN Algorithm](https://towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31) 📘

***
## Recommender Systems
* [Recommender Systems in a Nutshell](https://www.kdnuggets.com/2020/07/recommender-systems-nutshell.html)

***
## Deep Learning
* [Why Regularization reduces overfitting in Deep Neural Networks](https://www.youtube.com/watch?v=4nqD5TBlOWU) 📹
* [Pros and Cons of Neural Networks](https://towardsdatascience.com/hype-disadvantages-of-neural-networks-6af04904ba5b) 📘
* [When not to use Neural Networks](https://medium.com/datadriveninvestor/when-not-to-use-neural-networks-89fb50622429) 📘
* [40 Questions to test a Data Scientist on Deep learning](https://www.analyticsvidhya.com/blog/2017/04/40-questions-test-data-scientist-deep-learning/) 📘
* [21 Popular Deep Learning Interview Questions](https://www.analyticsvidhya.com/blog/2020/04/comprehensive-popular-deep-learning-interview-questions-answers/) 📘
* [Deep Learning Interview Questions - Edureka](https://www.youtube.com/watch?v=HGXlFG_Rz4E) 📹
* [Activation Functions in a Neural Network - Explained](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) 📘
* [Vanishing and Exploding Gradient - Clearly Explained](https://www.youtube.com/watch?v=qO_NLVjD6zE) 📹
* [Bias and Variance - Very clearly explained](https://www.youtube.com/watch?v=EuBBz3bI-aA) 📹
* [Why use ReLU over Sigmoid](https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks) 📘
* [25 Deep Learning Interview Qurstions to test your knowledge](https://towardsdatascience.com/50-deep-learning-interview-questions-part-1-2-8bbc8a00ec61) 📘
* [10 Deep Learning Best Practices to Keep in Mind in 2020](https://nanonets.com/blog/10-best-practices-deep-learning/) 📘

***
## Machine Learning Interpretability
* [Four Questions on Deciphering the World of Machine Learning Models](https://narrativescience.com/resource/blog/machine-learning-models/) 📘
* [Machine Learning Explanaibility - Crash Course by Kaggle](https://www.kaggle.com/learn/machine-learning-explainability) 📘
* [SHAP Values explained simply](https://www.youtube.com/watch?v=VB9uV-x0gtg) 📹

***
## Case Studies
Case studies are extremely important for interviews, below are some resources to practice, think first before looking at the solutions.
* [Dawn of Taxi Aggregators](https://www.analyticsvidhya.com/blog/2016/04/case-study-analytics-interviews-dawn-taxi-aggregators/) 📘
* [Optimizing product prices for an online vendor](https://www.analyticsvidhya.com/blog/2016/07/solving-case-study-optimize-products-price-online-vendor-level-hard/) 📘
* [Tips for a Case-Study Interview](https://workera.ai/resources/data-science-case-study-interview/) 📘
* [Mercari Price Prediction](https://towardsdatascience.com/a-data-science-case-study-with-python-mercari-price-prediction-4e852d95654) 📘
* [End-to-End multiclass Text Classification pipeline](https://mlwhiz.com/blog/2020/05/24/multitextclass/) 📘
* [End-to-End multiclass Image Classification pipeline](https://mlwhiz.com/blog/2020/06/06/multiclass_image_classification_pytorch/) 📘
* [Large Scale Forecasting for 1000+ products - Nagarro](https://www.youtube.com/watch?v=8jfDBD6xlFM) 📹
* [Clustering and Classification in E-Commerce](https://lucidworks.com/post/clustering-classification-supervised-unsupervised-learning-ecommerce/) 📘
* [The ABCs of Learning to Rank](https://lucidworks.com/post/abcs-learning-to-rank/) 📘
* [Data Science Case Study: Optimizing Product Placement in Retail ](https://towardsdatascience.com/data-science-case-study-optimizing-product-placement-in-retail-part-1-2e8b27e16e8d) 📘

***
## NLP
* [30 Questions to test a Data Scientist on NLP](https://www.analyticsvidhya.com/blog/2017/07/30-questions-test-data-scientist-natural-language-processing-solution-skilltest-nlp/)
* [11 Most Commonly Asked NLP Interview Questions For Beginners](https://analyticsindiamag.com/11-most-commonly-asked-nlp-interview-questions-for-beginners/)
* [How to solve 90% of NLP Problems](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)
* [Questions asked for NLP Roles at Companies](https://medium.com/modern-nlp/nlp-interview-questions-f062040f32f7)

***
## Data Science Interviews at FAANG and Similar Companies
* [Amazon’s Data Scientist Interview Practice Problems](https://towardsdatascience.com/amazon-data-scientist-interview-practice-problems-15b9b86e86c6) 📘
* [Microsoft Data Science Interview Questions and Answers](https://towardsdatascience.com/microsoft-data-science-interview-questions-and-answers-69ccac16bd9b) 📘
* [Problem Solving Questions for Data Science interview at Google](https://towardsdatascience.com/googles-data-science-interview-brain-teasers-7f3c1dc4ea7f) 📘

***
## Becoming a Rockstar Data Scientist(read if you have extra time)
Going through these will definately add extra brownie points, so don't miss these if you got time.

* [Top 13 Skills To Become a Rockstar Data Scientist](https://towardsdatascience.com/top-13-skills-to-become-a-rockstar-data-scientist-faf2f97e655d) 📘 
* [Understand these 4 ML concepts to sound like a master](https://towardsdatascience.com/understand-these-4-advanced-concepts-to-sound-like-a-machine-learning-master-d32843840b52) 📘
* [12 things I wish I knew before starting as a Data Scientist](https://medium.com/deliberate-data-science/12-things-i-wish-id-known-before-starting-as-a-data-scientist-45989be6300e) 📘
* [Understand the Data Science pipeline](https://towardsdatascience.com/a-beginners-guide-to-the-data-science-pipeline-a4904b2d8ad3) 📘
* [Kaggle Data Science Glossary](https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle) 📘
* [Google Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/) 📘
* [Running your ML Predictions 50 times faster - Hummingbird](https://mlwhiz.com/blog/2020/06/06/hummingbird_faster_ml_preds/) 📘
* [3 Mistakes you should not make in a Data Science Interview](https://mlwhiz.com/blog/2019/12/24/mistakes/) 📘
* [How to find Feature importances for BlackBox Models?](https://mlwhiz.com/blog/2019/12/04/blackbox/) 📘

***

## Data Structures and Algorithms(Optional)
Although this might be optional, but do not miss this if the Job Description explicitly asks for this, and especially never miss this if you are interviewing at FAANG and similar organizations, or if you have a CS Background. You don't have to be as good as an SDE at this, but at least know the basics.

* [A Data Scientist's guide to Data Structures and Algorithms](https://towardsdatascience.com/a-data-scientists-guide-to-data-structures-algorithms-1176395015a0) 📘
* [Handling Trees in Data Science Algorithmic Interview](https://towardsdatascience.com/handling-trees-in-data-science-algorithmic-interview-ea14dd1b6236) 📘
* [A simple introduction to Linked Lists for Data Scientists](https://mlwhiz.com/blog/2020/01/28/ll/) 📘
* [Dynamic Programming for Data Scientists](https://mlwhiz.com/blog/2020/01/28/dp/) 📘
* [3 Programming concepts for Data Scientists](https://mlwhiz.com/blog/2019/12/09/pc/) 📘
* [Data Scientists, The 5 Graph Algorithms that you should know](https://mlwhiz.com/blog/2019/09/02/graph_algs/) 📘

***
## Engineering and Deployment
* [A Layman’s Guide for Data Scientists to create APIs in minutes](https://mlwhiz.com/blog/2020/06/06/fastapi_for_data_scientists/) 📘
* [Take your Machine Learning Models to Production with these 5 simple steps](https://mlwhiz.com/blog/2019/12/25/prod/) 📘
* [2 way to deploy your ML models](https://towardsdatascience.com/there-are-two-very-different-ways-to-deploy-ml-models-heres-both-ce2e97c7b9b1) 📘
* [How to deploy a Keras model as a web app through Flask](https://towardsdatascience.com/deploying-a-keras-deep-learning-model-as-a-web-application-in-p-fc0f2354a7ff) 📘
* [How to write Web apps using simple Python for Data Scientists?](https://towardsdatascience.com/how-to-write-web-apps-using-simple-python-for-data-scientists-a227a1a01582) 📘

***
## Big Data and Spark
* [55 Apache Spark Interview Questions](https://www.edureka.co/blog/interview-questions/top-apache-spark-interview-questions-2016/) 📘
* [10 Questions you can expect in a Spark Interview](https://medium.com/analytics-vidhya/10-questions-you-can-expect-in-spark-interview-24b89b807dfb) 📘
* [Hive Interview Questions](https://www.tutorialspoint.com/hive/hive_interview_questions.htm) 📘
* [Top 20 Apache Spark Interview Questions](https://www.youtube.com/watch?v=Y8LKEDyA5iY) 📹
* [Spark Interview Questions - The entire playlist](https://www.youtube.com/playlist?list=PLtfmIPhU2DkNjQjL08kR3cd4kUzWqS0vg) 📹
* [Another fabulous Playlist for Spark Interview Questions](https://www.youtube.com/playlist?list=PL9sbKmQTkW05mXqnq1vrrT8pCsEa53std) 📹
* [Practical PySpark tips for Data Scientists](https://towardsdatascience.com/practical-spark-tips-for-data-scientists-145d85e9b2d8) 📘
* [3 Ways to parallelize your code using Spark](https://towardsdatascience.com/3-methods-for-parallelization-in-spark-6a1a4333b473) 📘
* [Datashader - Revealing the Structure of Genuinely Big Data](https://www.youtube.com/watch?v=6m3CFbKmK_c) 📹
* [Lightnings Talk : What one should know about Spark-MLlib](https://www.youtube.com/watch?v=DBxcua0Vmvk) 📹
* [Solving “Container Killed by Yarn For Exceeding Memory Limits” Exception in Apache Spark](https://medium.com/analytics-vidhya/solving-container-killed-by-yarn-for-exceeding-memory-limits-exception-in-apache-spark-b3349685df16) 📘

***
## Some amazing stuff on Python and Spark 
You can't afford to miss this if you are interviewing for a Big data role.
* [Improving Python and Spark performance](https://www.youtube.com/watch?v=qIKImANLFtE) 📹
* [High Performance Python on Spark](https://www.youtube.com/watch?v=abZ0f5ug18U) 📹
* [Vectorized UDFs: Scalable Analysis with Python and PySpark](https://www.youtube.com/watch?v=Til-StSDvfA) 📹

***
## General Interview Questions across the Spectrum (Video)

* [Common Data Science Interview Questions - Edureka](https://www.youtube.com/watch?v=tTAieUcNHdY)
* [Common Machine Learning Interview Question - Edureka](https://www.youtube.com/watch?v=t6gOpFLt-Ks)
* [Top 5 algorithms used in Data Science](https://www.youtube.com/watch?v=BfowBtIxNu4) 
* [Common Data Science Interview Questions - Analytics University](https://www.youtube.com/watch?v=BfowBtIxNu4) 
* [3 types of Data Science Interview Questions](https://www.youtube.com/watch?v=4Z6lxfglvUU)
* [Lessons learned the hard way - Hacking the Data Science Interview](https://www.youtube.com/watch?v=3BRLGRqj8p)
* [What it's like to Interview as a Data Scientist](https://www.youtube.com/watch?v=0HmAEWPfMnM)
* [5 Tips for getting a Data Science Job](https://www.youtube.com/watch?v=MfP-P8EHGBo)
* [8 Frequently used Data Science Algorithms](https://www.youtube.com/watch?v=z3wMgOTSE5s) 
* [Scenario Based Practical Interview](https://www.youtube.com/watch?v=bJPhEa3mbwo)
* [KNN v/s K Means](https://www.youtube.com/watch?v=OClrEI_5Ri4)


## General Interview Questions across the Spectrum (Reading)
* [The Data Science Interview Guide](https://towardsdatascience.com/data-science-interview-guide-4ee9f5dc778)
* [Top 30 Data Science Interview Questions](https://towardsdatascience.com/top-30-data-science-interview-questions-7dd9a96d3f5c)
* [35 Important Data Science Interview Questions](https://www.edureka.co/blog/interview-questions/data-science-interview-questions/)
* [100 Data Science Interview Questions across FAANG](https://medium.com/@e22aafa7d95/c5a66186769a)
* [The Most Comprehensive Data Science Interview Guide](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-data-science-machine-learning-interview-guide/)
* [41 essential ML interview questions - Springboard](https://www.springboard.com/blog/machine-learning-interview-questions/)
* [30 days of Data Science Interview Preparation - iNeuron](https://github.com/iNeuronai/interview-question-data-science-)
* [109 Data Science Interview Questions - Springboard](https://www.springboard.com/blog/data-science-interview-questions/)
* [Most asked Data Science interview questions in India - Springboard](https://in.springboard.com/blog/most-asked-data-science-interview-questions-in-india/)
* [List of AI Startups in India and resources for preparing for the interview](https://github.com/theainerd/MLInterview)
* [5 interview questions to predict a good Data Scientist](https://medium.com/predict/five-interview-questions-to-predict-a-good-data-scientist-40d310cdcd68)
* [8 proven ways to improve the accuracy of your ML model ](https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/)
* [60 Interview Questions on Machine Learning - AnalyticsIndiaMag](https://analyticsindiamag.com/60-interview-questions-on-machine-learning/)
* [The Big List of DS and ML interview Resources](https://towardsdatascience.com/the-big-list-of-ds-ml-interview-resources-2db4f651bd63)
* [100 Basic Data Science Interview Questions along with answers](https://www.dezyre.com/article/100-data-science-interview-questions-and-answers-general-for-2018/184)
* [40 interview questions asked at Startups in ML/DS Interview](https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/)
* [My Data Science/Machine Learning Job Interview Experience : List of DS/ML/DL Questions &#8211; Machine Learning in Action](https://appliedmachinelearning.blog/2018/04/13/my-data-science-machine-learning-job-interview-experience-list-of-ds-ml-dl-questions/)
* [How do I prepare for a Data Science phone interview at Airbnb](https://www.quora.com/How-do-I-prepare-for-a-phone-interview-for-a-data-scientist-position-with-Airbnb)
* [Best ML algorithm for regression problems](https://towardsdatascience.com/selecting-the-best-machine-learning-algorithm-for-your-regression-problem-20c330bad4ef)
* [How to ace the In person Data Science Interview](https://towardsdatascience.com/how-to-ace-the-in-person-data-science-interview-584ca11df08a)
* [How to land a Data Scientist job at Airbnb](https://towardsdatascience.com/how-to-land-a-data-scientist-job-at-your-dream-company-my-journey-to-airbnb-f6a1e99892e8)
* [120 Data Science Interview Questions(from all domains)](https://github.com/kojino/120-Data-Science-Interview-Questions)
* [Understanding the Bias-Variance Tradeoff](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)
* [You Need these Cheatsheets if you are tackling ML algorithms](https://medium.freecodecamp.org/you-need-these-cheat-sheets-if-youre-tackling-machine-learning-algorithms-45da5743888e)
* [Red Flags in a Data Science Interview](https://towardsdatascience.com/red-flags-in-data-science-interviews-4f492bbed4c4)
* [A Data Scientist's take on Interview Questions](https://towardsdatascience.com/my-take-on-data-scientist-interview-questions-part-1-6df22252b2e8)
* [What is Cross Entropy(Nice and Short Explanation)](https://stackoverflow.com/questions/41990250/what-is-cross-entropy/41990932#41990932)
* [What does an ideal Data Scientist's profile look like](https://towardsdatascience.com/what-does-an-ideal-data-scientists-profile-look-like-7d7bd78ff7ab)
* [25 Fun Questions for a Machine Learning interview](https://medium.com/analytics-vidhya/25-fun-questions-for-a-machine-learning-interview-373b744a4faa)
* [How to Prepare for Machine Learning Interviews](https://towardsdatascience.com/how-to-prepare-for-machine-learning-interviews-5fac3db58168)
* [How to develop a Machine Learning Model from scratch](https://towardsdatascience.com/machine-learning-general-process-8f1b510bd8af)
* [End to End guide for a Machine Learning Project](https://medium.com/fintechexplained/end-to-end-guide-for-machine-learning-project-146c288186dc)
* [Classification v/s Regression](https://medium.com/fintechexplained/supervised-machine-learning-regression-vs-classification-18b2f97708de)
* [Must Know mathematical measures for Every Data Scientist](https://medium.com/fintechexplained/must-know-mathematical-measures-for-data-scientist-15bfc4f7f39c)
* [Where did the least square come from](https://towardsdatascience.com/where-did-the-least-square-come-from-3f1abc7f7caf)
* [Regularization in Machine Learning - Explained](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)


***
## Interesting Reads
* [3 Common Data Science Career Transitions and how to make them happen](https://towardsdatascience.com/3-common-data-science-career-transitions-and-how-to-make-them-happen-588c3618942f)
* [Navigating the Data Science Career Landscape](https://hackernoon.com/navigating-the-data-science-career-landscape-db746a61ac62)
* [Which model and how much data](https://towardsdatascience.com/which-model-and-how-much-data-75a3999660f3)


## Data Analysis (27 questions)

#### 1. (Given a Dataset) Analyze this dataset and tell me what you can learn from it.
#### 2. What is R2? What are some other metrics that could be better than R2 and why?
  - goodness of fit measure. variance explained by the regression / total variance
  - the more predictors you add the higher R^2 becomes.
    - hence use adjusted R^2 which adjusts for the degrees of freedom 
    - or train error metrics
#### 3. What is the curse of dimensionality?
  - High dimensionality makes clustering hard, because having lots of dimensions means that everything is "far away" from each other.
  - For example, to cover a fraction of the volume of the data we need to capture a very wide range for each variable as the number of variables increases
  - All samples are close to the edge of the sample. And this is a bad news because prediction is much more difficult near the edges of the training sample.
  - The sampling density decreases exponentially as p increases and hence the data becomes much more sparse without significantly more data. 
  - We should conduct PCA to reduce dimensionality
#### 4. Is more data always better?
  - Statistically,
    - It depends on the quality of your data, for example, if your data is biased, just getting more data won’t help.
    - It depends on your model. If your model suffers from high bias, getting more data won’t improve your test results beyond a point. You’d need to add more features, etc.
  - Practically,
    - Also there’s a tradeoff between having more data and the additional storage, computational power, memory it requires. Hence, always think about the cost of having more data.
#### 5. What are advantages of plotting your data before per- forming analysis?
  - 1) Data sets have errors.  You won't find them all but you might find some. That 212 year old man. That 9 foot tall woman.  

2) Variables can have skewness, outliers etc.  Then the arithmetic mean might not be useful. Which means the standard deviation isn't useful.  

3) Variables can be multimodal!  If a variable is multimodal then anything based on its mean or median is going to be suspect. 
#### 6. How can you make sure that you don’t analyze something that ends up meaningless?
  - Proper exploratory data analysis.  

In every data analysis task, there's the exploratory phase where you're just graphing things, testing things on small sets of the data, summarizing simple statistics, and getting rough ideas of what hypotheses you might want to pursue further.  

Then there's the exploitatory phase, where you look deeply into a set of hypotheses.   

The exploratory phase will generate lots of possible hypotheses, and the exploitatory phase will let you really understand a few of them. Balance the two and you'll prevent yourself from wasting time on many things that end up meaningless, although not all.
#### 7. What is the role of trial and error in data analysis? What is the the role of making a hypothesis before diving in?
  - data analysis is a repetition of setting up a new hypothesis and trying to refute the null hypothesis.
  - The scientific method is eminently inductive: we elaborate a hypothesis, test it and refute it or not. As a result, we come up with new hypotheses which are in turn tested and so on. This is an iterative process, as science always is.
#### 8. How can you determine which features are the most im- portant in your model?
  - run the features though a Gradient Boosting Machine or Random Forest to generate plots of relative importance and information gain for each feature in the ensembles.
  - Look at the variables added in forward variable selection 
#### 9. How do you deal with some of your predictors being missing?
  - Remove rows with missing values - This works well if 1) the values are missing randomly (see [Vinay Prabhu's answer](https://www.quora.com/How-can-I-deal-with-missing-values-in-a-predictive-model/answer/Vinay-Prabhu-7) for more details on this) 2) if you don't lose too much of the dataset after doing so.
  - Build another predictive model to predict the missing values - This could be a whole project in itself, so simple techniques are usually used here.
  - Use a model that can incorporate missing data \- Like a random forest, or any tree-based method.
#### 10. You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?
  - Multicollinearity refers to a situation in which two or more explanatory variables in a [multiple regression](https://en.wikipedia.org/wiki/Multiple_regression "Multiple regression") model are highly linearly related. 
  - Leave the model as is, despite multicollinearity. The presence of multicollinearity doesn't affect the efficiency of extrapolating the fitted model to new data provided that the predictor variables follow the same pattern of multicollinearity in the new data as in the data on which the regression model is based.
  - principal component regression
#### 11. Let’s say you’re given an unfeasible amount of predictors in a predictive modeling task. What are some ways to make the prediction more feasible?
  - PCA
#### 12. Now you have a feasible amount of predictors, but you’re fairly sure that you don’t need all of them. How would you perform feature selection on the dataset?
  - ridge / lasso / elastic net regression
  - Univariate Feature Selection where a statistical test is applied to each feature individually. You retain only the best features according to the test outcome scores
  - "Recursive Feature Elimination":  
    - First, train a model with all the feature and evaluate its performance on held out data.
    - Then drop let say the 10% weakest features (e.g. the feature with least absolute coefficients in a linear model) and retrain on the remaining features.
    - Iterate until you observe a sharp drop in the predictive accuracy of the model.
#### 13. Your linear regression didn’t run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?
  - p > n.
  - If some of the explanatory variables are perfectly correlated (positively or negatively) then the coefficients would not be unique. 
#### 14. You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?
  - The dataset might be heterogeneous. In which case, it is recommended to cluster datasets into different subsets wisely, and then draw different models for different subsets. Or, use models like non parametric models (trees) which can deal with heterogeneity quite nicely.
#### 15. What is the main idea behind ensemble learning? If I had many different models that predicted the same response variable, what might I want to do to incorporate all of the models? Would you expect this to perform better than an individual model or worse?
  - The assumption is that a group of weak learners can be combined to form a strong learner.
  - Hence the combined model is expected to perform better than an individual model.
  - Assumptions:
    - average out biases
    - reduce variance
  - Bagging works because some underlying learning algorithms are unstable: slightly different inputs leads to very different outputs. If you can take advantage of this instability by running multiple instances, it can be shown that the reduced instability leads to lower error. If you want to understand why, the original bagging paper( [http://www.springerlink.com/cont...](http://www.springerlink.com/content/l4780124w2874025/)) has a section called "why bagging works"
  - Boosting works because of the focus on better defining the "decision edge". By reweighting examples near the margin (the positive and negative examples) you get a reduced error (see http://citeseerx.ist.psu.edu/vie...)
  - Use the outputs of your models as inputs to a meta-model.   

For example, if you're doing binary classification, you can use all the probability outputs of your individual models as inputs to a final logistic regression (or any model, really) that can combine the probability estimates.  

One very important point is to make sure that the output of your models are out-of-sample predictions. This means that the predicted value for any row in your dataframe should NOT depend on the actual value for that row.
#### 16. Given that you have wi  data in your o ce, how would you determine which rooms and areas are underutilized and overutilized?
  - If the data is more used in one room, then that one is over utilized! Maybe account for the room capacity and normalize the data.
#### 17. How could you use GPS data from a car to determine the quality of a driver?
#### 18. Given accelerometer, altitude, and fuel usage data from a car, how would you determine the optimum acceleration pattern to drive over hills?
#### 19. Given position data of NBA players in a season’s games, how would you evaluate a basketball player’s defensive ability?
#### 20. How would you quantify the influence of a Twitter user?
  - like page rank with each user corresponding to the webpages and linking to the page equivalent to following.
#### 21. Given location data of golf balls in games, how would construct a model that can advise golfers where to aim?
#### 22. You have 100 mathletes and 100 math problems. Each mathlete gets to choose 10 problems to solve. Given data on who got what problem correct, how would you rank the problems in terms of di culty?
  - One way you could do this is by storing a "skill level" for each user and a "difficulty level" for each problem.  We assume that the probability that a user solves a problem only depends on the skill of the user and the difficulty of the problem.*  Then we maximize the likelihood of the data to find the hidden skill and difficulty levels.
  - The Rasch model for dichotomous data takes the form:  
{\displaystyle \Pr\\{X_{ni}=1\\}={\frac {\exp({\beta _{n}}-{\delta _{i}})}{1+\exp({\beta _{n}}-{\delta _{i}})}},}  
where  is the ability of person  and  is the difficulty of item}.
#### 23. You have 5000 people that rank 10 sushis in terms of saltiness. How would you aggregate this data to estimate the true saltiness rank in each sushi?
  - Some people would take the mean rank of each sushi.  If I wanted something simple, I would use the median, since ranks are (strictly speaking) ordinal and not interval, so adding them is a bit risque (but people do it all the time and you probably won't be far wrong).
#### 24. Given data on congressional bills and which congressional representatives co-sponsored the bills, how would you determine which other representatives are most similar to yours in voting behavior? How would you evaluate who is the most liberal? Most republican? Most bipartisan?
  - collaborative filtering. you have your votes and we can calculate the similarity for each representatives and select the most similar representative
  - for liberal and republican parties, find the mean vector and find the representative closest to the center point
#### 25. How would you come up with an algorithm to detect plagiarism in online content?
  - reduce the text to a more compact form (e.g. fingerprinting, bag of words) then compare those with other texts by calculating the similarity
#### 26. You have data on all purchases of customers at a grocery store. Describe to me how you would program an algorithm that would cluster the customers into groups. How would you determine the appropriate number of clusters to include?
  - KMeans
  - choose a small value of k that still has a low SSE (elbow method)
  - <https://bl.ocks.org/rpgove/0060ff3b656618e9136b>
#### 27. Let's say you're building the recommended music engine at Spotify to recommend people music based on past listening history. How would you approach this problem?
  - [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)

## Predictive Modeling (19 questions)
#### 1. (Given a Dataset) Analyze this dataset and give me a model that can predict this response variable.
- Start by fitting a simple model (multivariate regression, logistic regression), do some feature engineering accordingly, and then try some complicated models. Always split the dataset into train, validation, test dataset and use cross validation to check their performance.
- Determine if the problem is classification or regression
- Favor simple models that run quickly and you can easily explain.
- Mention cross validation as a means to evaluate the model.
- Plot and visualize the data.

#### 2. What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?
- The model that has high training accuracy might have low test accuracy. Without further knowledge, it is hard to know which dataset represents the population data and thus the generalizability of the algorithm is hard to measure. This should be mitigated by repeated splitting of train vs test dataset (as in cross validation).
- When there is a change in data distribution, this is called the dataset shift. If the train and test data has a different distribution, then the classifier would likely overfit to the train data.
- This issue can be overcome by using a more general learning method.
- This can occur when:
  - P(y|x) are the same but P(x) are different. (covariate shift)
  - P(y|x) are different. (concept shift)
- The causes can be:
  - Training samples are obtained in a biased way. (sample selection bias)
  - Train is different from test because of temporal, spatial changes. (non-stationary environments)
- Solution to covariate shift
  - importance weighted cv
#### 3. What are some ways I can make my model more robust to outliers?
- We can have regularization such as L1 or L2 to reduce variance (increase bias).
- Changes to the algorithm:
  - Use tree-based methods instead of regression methods as they are more resistant to outliers. For statistical tests, use non parametric tests instead of parametric ones.
  - Use robust error metrics such as MAE or Huber Loss instead of MSE.
- Changes to the data:
  - Winsorizing the data
  - Transforming the data (e.g. log)
  - Remove them only if you’re certain they’re anomalies not worth predicting

#### 4. What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?
- MSE is more strict to having outliers. MAE is more robust in that sense, but is harder to fit the model for because it cannot be numerically optimized. So when there are less variability in the model and the model is computationally easy to fit, we should use MAE, and if that’s not the case, we should use MSE.
- MSE: easier to compute the gradient, MAE: linear programming needed to compute the gradient
- MAE more robust to outliers. If the consequences of large errors are great, use MSE
- MSE corresponds to maximizing likelihood of Gaussian random variables

#### 5. What error metric would you use to evaluate how good a binary classifier is? What if the classes are imbalanced? What if there are more than 2 groups?
- Accuracy: proportion of instances you predict correctly. Pros: intuitive, easy to explain, Cons: works poorly when the class labels are imbalanced and the signal from the data is weak
- AUROC: plot fpr on the x axis and tpr on the y axis for different threshold. Given a random positive instance and a random negative instance, the AUC is the probability that you can identify who's who. Pros: Works well when testing the ability of distinguishing the two classes, Cons: can’t interpret predictions as probabilities (because AUC is determined by rankings), so can’t explain the uncertainty of the model
- logloss/deviance: Pros: error metric based on probabilities, Cons: very sensitive to false positives, negatives
- When there are more than 2 groups, we can have k binary classifications and add them up for logloss. Some metrics like AUC is only applicable in the binary case.

#### 6. What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What’s the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)
- Things to look at: N, P, linearly seperable?, features independent?, likely to overfit?, speed, performance, memory usage
- Logistic Regression
  - features roughly linear, problem roughly linearly separable
  - robust to noise, use l1,l2 regularization for model selection, avoid overfitting
  - the output come as probabilities
  - efficient and the computation can be distributed
  - can be used as a baseline for other algorithms
  - (-) can hardly handle categorical features
- SVM
  - with a nonlinear kernel, can deal with problems that are not linearly separable
  - (-) slow to train, for most industry scale applications, not really efficient
- Naive Bayes
  - computationally efficient when P is large by alleviating the curse of dimensionality
  - works surprisingly well for some cases even if the condition doesn’t hold
  - with word frequencies as features, the independence assumption can be seen reasonable. So the algorithm can be used in text categorization
  - (-) conditional independence of every other feature should be met
- Tree Ensembles
  - good for large N and large P, can deal with categorical features very well
  - non parametric, so no need to worry about outliers
  - GBT’s work better but the parameters are harder to tune
  - RF works out of the box, but usually performs worse than GBT
- Deep Learning
  - works well for some classification tasks (e.g. image)
  - used to squeeze something out of the problem

#### 7. What is regularization and where might it be helpful? What is an example of using regularization in a model?
- Regularization is useful for reducing variance in the model, meaning avoiding overfitting . For example, we can use L1 regularization in Lasso regression to penalize large coefficients.

#### 8. Why might it be preferable to include fewer predictors over many?
- When we add irrelevant features, it increases model's tendency to overfit because those features introduce more noise. When two variables are correlated, they might be harder to interpret in case of regression, etc.
- curse of dimensionality
- adding random noise makes the model more complicated but useless
- computational cost
- Ask someone for more details.

#### 9. Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?
- Build a time series model with the training data with a seven day cycle and then use that for a new data with only 2 days data.
- Ask someone for more details.
- Build a regression function to estimate the number of retweets as a function of time t
- to determine if one regression function can be built, see if there are clusters in terms of the trends in the number of retweets
- if not, we have to add features to the regression function
- features + # of retweets on the first and the second day -> predict the seventh day
- https://en.wikipedia.org/wiki/Dynamic_time_warping

#### 10. How could you collect and analyze data to use social media to predict the weather?
- We can collect social media data using twitter, Facebook, instagram API’s. Then, for example, for twitter, we can construct features from each tweet, e.g. the tweeted date, number of favorites, retweets, and of course, the features created from the tweeted content itself. Then use a multi variate time series model to predict the weather.
- Ask someone for more details.

#### 11. How would you construct a feed to show relevant content for a site that involves user interactions with items?
- We can do so using building a recommendation engine. The easiest we can do is to show contents that are popular other users, which is still a valid strategy if for example the contents are news articles. To be more accurate, we can build a content based filtering or collaborative filtering. If there’s enough user usage data, we can try collaborative filtering and recommend contents other similar users have consumed. If there isn’t, we can recommend similar items based on vectorization of items (content based filtering).

#### 12. How would you design the people you may know feature on LinkedIn or Facebook?
- Find strong unconnected people in weighted connection graph
  - Define similarity as how strong the two people are connected
  - Given a certain feature, we can calculate the similarity based on
    - friend connections (neighbors)
    - Check-in’s people being at the same location all the time.
    - same college, workplace
    - Have randomly dropped graphs test the performance of the algorithm
- ref. News Feed Optimization
  - Affinity score: how close the content creator and the users are
  - Weight: weight for the edge type (comment, like, tag, etc.). Emphasis on features the company wants to promote
  - Time decay: the older the less important

#### 13. How would you predict who someone may want to send a Snapchat or Gmail to?
- for each user, assign a score of how likely someone would send an email to
- the rest is feature engineering:
  - number of past emails, how many responses, the last time they exchanged an email, whether the last email ends with a question mark, features about the other users, etc.
- Ask someone for more details.
- People who someone sent emails the most in the past, conditioning on time decay.

#### 14. How would you suggest to a franchise where to open a new store?
- build a master dataset with local demographic information available for each location.
  - local income levels, proximity to traffic, weather, population density, proximity to other businesses
  - a reference dataset on local, regional, and national macroeconomic conditions (e.g. unemployment, inflation, prime interest rate, etc.)
  - any data on the local franchise owner-operators, to the degree the manager
- identify a set of KPIs acceptable to the management that had requested the analysis concerning the most desirable factors surrounding a franchise
  - quarterly operating profit, ROI, EVA, pay-down rate, etc.
- run econometric models to understand the relative significance of each variable
- run machine learning algorithms to predict the performance of each location candidate

#### 15. In a search engine, given partial data on what the user has typed, how would you predict the user’s eventual search query?
- Based on the past frequencies of words shown up given a sequence of words, we can construct conditional probabilities of the set of next sequences of words that can show up (n-gram). The sequences with highest conditional probabilities can show up as top candidates.
- To further improve this algorithm,
  - we can put more weight on past sequences which showed up more recently and near your location to account for trends
  - show your recent searches given partial data

#### 16. Given a database of all previous alumni donations to your university, how would you predict which recent alumni are most likely to donate?
- Based on frequency and amount of donations, graduation year, major, etc, construct a supervised regression (or binary classification) algorithm.

#### 17. You’re Uber and you want to design a heatmap to recommend to drivers where to wait for a passenger. How would you approach this?
- Based on the past pickup location of passengers around the same time of the day, day of the week (month, year), construct
- Ask someone for more details.
- Based on the number of past pickups
  - account for periodicity (seasonal, monthly, weekly, daily, hourly)
  - special events (concerts, festivals, etc.) from tweets

#### 18. How would you build a model to predict a March Madness bracket?
- One vector each for team A and B. Take the difference of the two vectors and use that as an input to predict the probability that team A would win by training the model. Train the models using past tournament data and make a prediction for the new tournament by running the trained model for each round of the tournament
- Some extensions:
  - Experiment with different ways of consolidating the 2 team vectors into one (e.g concantenating, averaging, etc)
  - Consider using a RNN type model that looks at time series data.

#### 19. You want to run a regression to predict the probability of a flight delay, but there are flights with delays of up to 12 hours that are really messing up your model. How can you address this?
- This is equivalent to making the model more robust to outliers.
- See Q3.

## Probability (19 questions)


#### 1. Bobo the amoeba has a 25%, 25%, and 50% chance of producing 0, 1, or 2 offspring, respectively. Each of Bobo’s descendants also have the same probabilities. What is the probability that Bobo’s lineage dies out?
  - p=1/4+1/4*p+1/2*p^2 => p=1/2
#### 2. In any 15-minute interval, there is a 20% probability that you will see at least one shooting star. What is the proba- bility that you see at least one shooting star in the period of an hour?
  - 1-(0.8)^4. Or, we can use Poisson processes
#### 3. How can you generate a random number between 1 - 7 with only a die?
* Launch it 3 times: each throw sets the nth bit of the result. 
* For each launch, if the value is 1-3, record a 0, else 1.
The result is between 0 (000) and 7 (111), evenly spread (3 independent throw). Repeat the throws if 0 was obtained: the process stops on evenly spread values.
#### 4. How can you get a fair coin toss if someone hands you a coin that is weighted to come up heads more often than tails?
  - Flip twice and if HT then H, TH then T.
#### 5. You have an 50-50 mixture of two normal distributions with the same standard deviation. How far apart do the means need to be in order for this distribution to be bimodal?
  - more than two standard deviations
#### 6. Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?
  - plug in the value to the CDF of the same random variable
#### 7. A certain couple tells you that they have two children, at least one of which is a girl. What is the probability that they have two girls?
  - 1/3
#### 8. You have a group of couples that decide to have children until they have their first girl, after which they stop having children. What is the expected gender ratio of the children that are born? What is the expected number of children each couple will have?
  - gender ratio is 1:1. Expected number of children is 2. let X be the number of children until getting a female (happens with prob 1/2). this follows a geometric distribution with probability 1/2
#### 9. How many ways can you split 12 people into 3 teams of 4?
  - the outcome follows a multinomial distribution with n=12 and k=3. but the classes are indistinguishable
#### 10. Your hash function assigns each object to a number between 1:10, each with equal probability. With 10 objects, what is the probability of a hash collision? What is the expected number of hash collisions? What is the expected number of hashes that are unused.
  - the probability of a hash collision: 1-(10!/10^10)
  - the expected number of hash collisions: 1-10*(9/10)^10
  - the expected number of hashes that are unused: 10*(9/10)^10
#### 11. You call 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?
  - All Lyft's first
  
    * probability that the first car is Lyft = 3/5
    * probability that the second car is Lyft = 2/4
    * probability that the third car is Lyft = 1/3
    Therefore, probability that all the Lyfts arrive first = (3/5) * (2/4) * (1/3) = 1/10
  - All Uber's first
  
    * probability that the first car is Uber = 2/5
    * probability that the second car is Uber = 1/4
    Therefore, probability that all the Ubers arrive first = (2/5) * (1/4) = 1/10
#### 12. I write a program should print out all the numbers from 1 to 300, but prints out Fizz instead if the number is divisible by 3, Buzz instead if the number is divisible by 5, and FizzBuzz if the number is divisible by 3 and 5. What is the total number of numbers that is either Fizzed, Buzzed, or FizzBuzzed?
  - 100+60-20=140
#### 13. On a dating site, users can select 5 out of 24 adjectives to describe themselves. A match is declared between two users if they match on at least 4 adjectives. If Alice and Bob randomly pick adjectives, what is the probability that they form a match?
  - 24C5*(1+5(24-5))/24C5*24C5 = 4/1771
#### 14. A lazy high school senior types up application and envelopes to n different colleges, but puts the applications randomly into the envelopes. What is the expected number of applications that went to the right college?
  - 1
#### 15. Let’s say you have a very tall father. On average, what would you expect the height of his son to be? Taller, equal, or shorter? What if you had a very short father?
  - Shorter. Regression to the mean
#### 16. What’s the expected number of coin flips until you get two heads in a row? What’s the expected number of coin flips until you get two tails in a row?
  - After the first two flips, you can see this problem as a Markov chain, with states HH, HT, TH, TT. 
  - HH is the final state. You can than define the expected number of steps N before reaching HH: E(N) = 2 + 0.25nHH, 0.25nHT, 0.25nTH, 0.25nTT. nXX represents the expected number of steps before reaching HH starting from state XX.
  - Solve linear equation:
  * nHH = 0
  * nHT = 1 + 0.5nTT + 0.5nTH
  * nTH = 1 + 0.5nHH + 0.5nHT
  * nTT = 1 + 0.5nTH + 0.5nTT
  - Result gives E(N) = 6.
#### 17. Let’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you 2n-1 dollars. How much would you pay me to play this game?
  - less than $3
#### 18. You have two coins, one of which is fair and comes up heads with a probability 1/2, and the other which is biased and comes up heads with probability 3/4. You randomly pick coin and flip it twice, and get heads both times. What is the probability that you picked the fair coin?
  - 4/13
#### 19. You have a 0.1% chance of picking up a coin with both heads, and a 99.9% chance that you pick up a fair coin. You flip your coin and it comes up heads 10 times. What’s the chance that you picked up the fair coin, given the information that you observed?
  * Events: F = "picked a fair coin", T = "10 heads in a row"
  * (1) P(F|T) = P(T|F)P(F)/P(T) (Bayes formula)
  * (2) P(T) = P(T|F)P(F) + P(T|¬F)P(¬F) (total probabilities formula)
  * Injecting (2) in (1): P(F|T) = P(T|F)P(F)/(P(T|F)P(F) + P(T|¬F)P(¬F)) = 1 / (1 + P(T|¬F)P(¬F)/(P(T|F)P(F)))
  * Numerically: 1/(1 + 0.001 * 2^10 /0.999).
  * With 2^10 ≈ 1000 and 0.999 ≈ 1 this simplifies to 1/2
#### 20. What is a P-Value ?
  * The probability to obtain a similar or more extreme result than observed when the null hypothesis is assumed.
  * ⇒ If the p-value is small, the null hypothesis is unlikely

## Product Metrics (15 questions)

#### 1. What would be good metrics of success for an advertising-driven consumer product? (Buzzfeed, YouTube, Google Search, etc.) A service-driven consumer product? (Uber, Flickr, Venmo, etc.)
  * advertising-driven: Pageviews and daily actives, CTR, CPC (cost per click)
    * click-ads  
    * display-ads  
  * service-driven: number of purchases, conversion rate
#### 2. What would be good metrics of success for a productiv- ity tool? (Evernote, Asana, Google Docs, etc.) A MOOC? (edX, Coursera, Udacity, etc.)
  * productivity tool: same as premium subscriptions
  * MOOC: same as premium subscriptions, completion rate
#### 3. What would be good metrics of success for an e-commerce product? (Etsy, Groupon, Birchbox, etc.) A subscrip- tion product? (Net ix, Birchbox, Hulu, etc.) Premium subscriptions? (OKCupid, LinkedIn, Spotify, etc.) 
  * e-commerce: number of purchases, conversion rate, Hourly, daily, weekly, monthly, quarterly, and annual sales, Cost of goods sold, Inventory levels, Site traffic, Unique visitors versus returning visitors, Customer service phone call count, Average resolution time
  * subscription
    * churn, CoCA, ARPU, MRR, LTV
  * premium subscriptions: 

#### 4. What would be good metrics of success for a consumer product that relies heavily on engagement and interac- tion? (Snapchat, Pinterest, Facebook, etc.) A messaging product? (GroupMe, Hangouts, Snapchat, etc.)
  * heavily on engagement and interaction: uses AU ratios, email summary by type, and push notification summary by type, resurrection ratio
  * messaging product: 
#### 5. What would be good metrics of success for a product that o ered in-app purchases? (Zynga, Angry Birds, other gaming apps)
  * Average Revenue Per Paid User
  * Average Revenue Per User
#### 6. A certain metric is violating your expectations by going down or up more than you expect. How would you try to identify the cause of the change?
  * breakdown the KPI’s into what consists them and find where the change is
  * then further breakdown that basic KPI by channel, user cluster, etc. and relate them with any campaigns, changes in user behaviors in that segment
#### 7. Growth for total number of tweets sent has been slow this month. What data would you look at to determine the cause of the problem?
  * look at competitors' tweet growth
  * look at your social media engagement on other platforms
  * look at your sales data 
#### 8. You’re a restaurant and are approached by Groupon to run a deal. What data would you ask from them in order to determine whether or not to do the deal?
  * for similar restaurants (they should define similarity), average increase in revenue gain per coupon, average increase in customers per coupon, number of meals sold
#### 9. You are tasked with improving the e ciency of a subway system. Where would you start?
  * define efficiency
#### 10. Say you are working on Facebook News Feed. What would be some metrics that you think are important? How would you make the news each person gets more relevant?
  * rate for each action, duration users stay, CTR for sponsor feed posts
  * ref. News Feed Optimization
    * Affinity score: how close the content creator and the users are
    * Weight: weight for the edge type (comment, like, tag, etc.). Emphasis on features the company wants to promote
    * Time decay: the older the less important
#### 11. How would you measure the impact that sponsored stories on Facebook News Feed have on user engagement? How would you determine the optimum balance between sponsored stories and organic content on a user’s News Feed?
  * AB test on different balance ratio and see 
#### 12. You are on the data science team at Uber and you are asked to start thinking about surge pricing. What would be the objectives of such a product and how would you start looking into this?
  *  there is a gradual step-function type scaling mechanism until that imbalance of requests-to-drivers is alleviated and then vice versa as too many drivers come online enticed by the surge pricing structure. 
  * I would bet the algorithm is custom tailored and calibrated to each location as price elasticities almost certainly vary across different cities depending on a huge multitude of variables: income, distance/sprawl, traffic patterns, car ownership, etc. With the massive troves of user data that Uber probably has collected, they most likely have tweaked the algos for each city to adjust for these varying sensitivities to surge pricing. Throw in some machine learning and incredibly rich data and you've got yourself an incredible, constantly-evolving algorithm.  

#### 13. Say that you are Netflix. How would you determine what original series you should invest in and create?
  * Netflix uses data to estimate the potential market size for an original series before giving it the go-ahead.
#### 14. What kind of services would  nd churn (metric that tracks how many customers leave the service) helpful? How would you calculate churn?
  * subscription based services
#### 15. Let’s say that you’re are scheduling content for a content provider on television. How would you determine the best times to schedule content?


## Statistical Inference (16 questions)

#### 1. In an A/B test, how can you check if assignment to the various buckets was truly random?
  - Plot the distributions of multiple features for both A and B and make sure that they have the same shape. More rigorously, we can conduct a permutation test to see if the distributions are the same.
  - MANOVA to compare different means
#### 2. What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?
  - Verify the sampling algorithm is random.
#### 3. What would be the hazards of letting users sneak a peek at the other bucket in an A/B test?
  - The user might not act the same suppose had they not seen the other bucket. You are essentially adding additional variables of whether the user peeked the other bucket, which are not random across groups.
#### 4. What would be some issues if blogs decide to cover one of your experimental groups?
  - Same as the previous question. The above problem can happen in larger scale.
#### 5. How would you conduct an A/B test on an opt-in feature? 
  - Ask someone for more details.
#### 6. How would you run an A/B test for many variants, say 20 or more?
  - one control, 20 treatment, if the sample size for each group is big enough.
  - Ways to attempt to correct for this include changing your confidence level (e.g. Bonferroni Correction) or doing family-wide tests before you dive in to the individual metrics (e.g. Fisher's Protected LSD).
#### 7. How would you run an A/B test if the observations are extremely right-skewed?
  - lower the variability by modifying the KPI
  - cap values
  - percentile metrics
  - log transform
  - <https://www.quora.com/How-would-you-run-an-A-B-test-if-the-observations-are-extremely-right-skewed>
#### 8. I have two different experiments that both change the sign-up button to my website. I want to test them at the same time. What kinds of things should I keep in mind?
  - exclusive -> ok
#### 9. What is a p-value? What is the difference between type-1 and type-2 error?
  - A p-value is defined such that under the null hypothesis less than the fraction p of events have parameter values more extreme than the observed parameter. It is not the probability that the null hypothesis is wrong. 
  - type-1 error: rejecting Ho when Ho is true
  - type-2 error: not rejecting Ho when Ha is true
#### 10. You are AirBnB and you want to test the hypothesis that a greater number of photographs increases the chances that a buyer selects the listing. How would you test this hypothesis?
  - For randomly selected listings with more than 1 pictures, hide 1 random picture for group A, and show all for group B. Compare the booking rate for the two groups.
  - Ask someone for more details.
#### 11. How would you design an experiment to determine the impact of latency on user engagement?
  - The best way I know to quantify the impact of performance is to isolate just that factor using a slowdown experiment, i.e., add a delay in an A/B test.
#### 12. What is maximum likelihood estimation? Could there be any case where it doesn’t exist?
  - A method for parameter optimization (fitting a model). We choose parameters so as to maximize the likelihood function (how likely the outcome would happen given the current data and our model).
  - maximum likelihood estimation (MLE) is a method of [estimating](https://en.wikipedia.org/wiki/Estimator "Estimator") the [parameters](https://en.wikipedia.org/wiki/Statistical_parameter "Statistical parameter") of a [statistical model](https://en.wikipedia.org/wiki/Statistical_model "Statistical model") given observations, by finding the parameter values that maximize the [likelihood](https://en.wikipedia.org/wiki/Likelihood "Likelihood") of making the observations given the parameters. MLE can be seen as a special case of the [maximum a posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation "Maximum a posteriori estimation") (MAP) that assumes a [uniform](https://en.wikipedia.org/wiki/Uniform_distribution_\(continuous\) "Uniform distribution \(continuous\)") [prior distribution](https://en.wikipedia.org/wiki/Prior_probability "Prior probability") of the parameters, or as a variant of the MAP that ignores the prior and which therefore is [unregularized](https://en.wikipedia.org/wiki/Regularization_\(mathematics\) "Regularization \(mathematics\)").
  - for gaussian mixtures, non parametric models, it doesn’t exist
#### 13. What’s the difference between a MAP, MOM, MLE estima\- tor? In which cases would you want to use each?
  - MAP estimates the posterior distribution given the prior distribution and data which maximizes the likelihood function. MLE is a special case of MAP where the prior is uninformative uniform distribution.
  - MOM sets moment values and solves for the parameters. MOM is not used much anymore because maximum likelihood estimators have higher probability of being close to the quantities to be estimated and are more often unbiased.
#### 14. What is a confidence interval and how do you interpret it?
  - For example, 95% confidence interval is an interval that when constructed for a set of samples each sampled in the same way, the constructed intervals include the true mean 95% of the time.
  - if confidence intervals are constructed using a given confidence level in an infinite number of independent experiments, the proportion of those intervals that contain the true value of the parameter will match the confidence level.
  - [confidence intervals refresher from khanacademy](https://www.khanacademy.org/math/ap-statistics/estimating-confidence-ap/introduction-confidence-intervals/v/confidence-intervals-and-margin-of-error)
#### 15. What is unbiasedness as a property of an estimator? Is this always a desirable property when performing inference? What about in data analysis or predictive modeling?
  - Unbiasedness means that the expectation of the estimator is equal to the population value we are estimating. This is desirable in inference because the goal is to explain the dataset as accurately as possible. However, this is not always desirable for data analysis or predictive modeling as there is the bias variance tradeoff. We sometimes want to prioritize the generalizability and avoid overfitting by reducing variance and thus increasing bias.
#### 16. What is Selection Bias?
  - Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is usually associated with research where the selection of participants isn’t random. It is sometimes referred to as the selection effect. It is the distortion of statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may not be accurate.
  - The types of selection bias include:
  - Sampling bias:  It is a systematic error due to a non-random sample of a population causing some members of the population to be less likely to be included than others resulting in a biased sample.
  - Time Interval bias: A trial may be terminated early at an extreme value (often for ethical reasons), but the extreme value is likely to be reached by the variable with the largest variance, even if all variables have a similar mean.
  - Data: When specific subsets of data are chosen to support a conclusion or rejection of bad data on arbitrary grounds, instead of according to previously stated or generally agreed criteria.
  - Attrition: Attrition bias is a kind of selection bias caused by attrition (loss of participants) discounting trial subjects/tests that did not run to completion.


     

Interviewers seek practical knowledge on the data science basics and its industry-applications along with a good knowledge of tools and processes. Here we will provide you with a list of important data science interview questions for freshers as well as experienced candidates that one could face during job interviews. If you are aspiring to be a data scientist then you can start from here.
Data Science Interview Questions for Freshers
1. What is the difference between Type I Error & Type II Error? Also, Explain the Power of the test?

When we perform hypothesis testing we consider two types of Error, Type I error and Type II error, sometimes we reject the null hypothesis when we should not or choose not to reject the null hypothesis when we should. 

A Type I Error is committed when we reject the null hypothesis when the null hypothesis is actually true. On the other hand, a Type II error is made when we do not reject the null hypothesis and the null hypothesis is actually false. 

The probability of a Type I error is denoted by α and the probability of Type II error is denoted by β.

For a given sample n, a decrease in α will increase β and vice versa. Both α  and β decrease as n increases. 

The table given below explains the situation around the Type I error and Type II error:
Decision	Null Hypothesis is true	Null hypothesis is false
Reject the Null Hypothesis	Type I error	Correct Decision
Fail to reject Null Hypothesis	Correct Decision	Type II error

Two correct decisions are possible: not rejecting the null hypothesis when the null hypothesis is true and rejecting the null hypothesis when the null hypothesis is false. 

Conversely, two incorrect decisions are also possible: Rejecting the null hypothesis when the null hypothesis is true(Type I error), and not rejecting the null hypothesis when the null hypothesis is false (Type II error).

Type I error is false positive while Type II error is a false negative.

Power of Test: The Power of the test is defined as the probability of rejecting the null hypothesis when the null hypothesis is false. Since β is the probability of a Type II error, the power of the test is defined as 1- β.  In advanced statistics, we compare various types of tests based on their size and power, where the size denotes the actual proportion of rejections when the null is true and the power denotes the actual proportion of rejections when the null is false. 
2. What do you understand by Over-fitting and Under-fitting?

Overfitting is observed when there is a small amount of data and a large number of variables, If the model we finish with ends up modelling the noise as well, we call it “overfitting” and if we are not modelling all the information, we call it “underfitting”. Most commonly underfitting is observed when a linear model is fitted to a non-linear data. 

The hope is that the model that does the best on testing data manages to capture/model all the information but leave out all the noise. Overfitting can be avoided by using cross-validation techniques (like K Folds) and regularisation techniques (like Lasso regression). 
3. When do you use the Classification Technique over the Regression Technique?

Classification problems are mainly used when the output is the categorical variable (Discrete) whereas Regression Techniques are used when the output variable is Continuous variable.

In the Regression algorithm, we attempt to estimate the mapping function (f) from input variables (x) to numerical (continuous) output variable (y).

For example, Linear regression, Support Vector Machine (SVM) and Regression trees.

In the Classification algorithm, we attempt to estimate the mapping function (f) from the input variable (x) to the discrete or categorical output variable (y). 

For example, Logistic Regression, naïve Bayes, Decision Trees & K nearest neighbours.

Both Classifications, as well as Regression techniques, are Supervised Machine Learning Algorithms.
4. What is the importance of Data Cleansing?

Ans. As the name suggests, data cleansing is a process of removing or updating the information that is incorrect, incomplete, duplicated, irrelevant, or formatted improperly. It is very important to improve the quality of data and hence the accuracy and productivity of the processes and organisation as a whole.

Real-world data is often captured in formats which have hygiene issues. There are sometimes errors due to various reasons which make the data inconsistent and sometimes only some features of the data. Hence data cleansing is done to filter the usable data from the raw data, otherwise many systems consuming the data will produce erroneous results.
5. Which are the important steps of Data Cleaning?

Different types of data require different types of cleaning, the most important steps of Data Cleaning are:

    Data Quality
    Removing Duplicate Data (also irrelevant data)
    Structural errors
    Outliers
    Treatment for Missing Data

Data Cleaning is an important step before analysing data, it helps to increase the accuracy of the model. This helps organisations to make an informed decision.

Data Scientists usually spends 80% of their time cleaning data.
6. How is k-NN different from k-means clustering?

Ans. K-nearest neighbours is a classification algorithm, which is a subset of supervised learning. K-means is a clustering algorithm, which is a subset of unsupervised learning.

And K-NN is a Classification or Regression Machine Learning Algorithm while K-means is a Clustering Machine Learning Algorithm. 

K-NN is the number of nearest neighbours used to classify or (predict in case of continuous variable/regression) a test sample, whereas K-means is the number of clusters the algorithm is trying to learn from the data.   
7. What is p-value?

Ans. p-value helps you determine the strengths of your results when you perform a hypothesis test. It is a number between 0 and 1. The claim which is on trial is called the Null Hypothesis. Lower p-values, i.e. ≤ 0.05, means we can reject the Null Hypothesis. A high p-value, i.e. ≥ 0.05, means we can accept the Null Hypothesis. An exact p-value 0.05 indicates that the Hypothesis can go either way.

P-value is the measure of the probability of events other than suggested by the null hypothesis. It effectively means the probability of events rarer than the event being suggested by the null hypothesis.
8. How is Data Science different from Big Data and Data Analytics?

Ans. Data Science utilises algorithms and tools to draw meaningful and commercially useful insights from raw data. It involves tasks like data modelling, data cleansing, analysis, pre-processing etc. 
Big Data is the enormous set of structured, semi-structured, and unstructured data in its raw form generated through various channels.
And finally, Data Analytics provides operational insights into complex business scenarios. It also helps in predicting upcoming opportunities and threats for an organisation to exploit.

Essentially, big data is the process of handling large volumes of data. It includes standard practices for data management and processing at a high speed maintaining the consistency of data. Data analytics is associated with gaining meaningful insights from the data through mathematical or non-mathematical processes. Data Science is the art of making intelligent systems so that they learn from data and then make decisions according to past experiences.
data science interview questions
How is Data Science different from Big Data and Data Analytics?
Statistics in Data Science Interview Questions
9. What is the use of Statistics in Data Science?

Ans. Statistics in Data Science provides tools and methods to identify patterns and structures in data to provide a deeper insight into it. Serves a great role in data acquisition, exploration, analysis, and validation. It plays a really powerful role in Data Science.

Data Science is a derived field which is formed from the overlap of statistics probability and computer science. Whenever one needs to do estimations, statistics is involved. Many algorithms in data science are built on top of statistical formulae and processes. Hence statistics is an important part of data science.

Also Read: Practical Ways to Implement Data Science in Marketing
10. What is the difference between Supervised Learning and Unsupervised Learning?

Ans. Supervised Machine Learning requires labelled data for training while Unsupervised Machine Learning does not require labelled data. It can be trained on unlabelled data.

To elaborate, supervised learning involves training of the model with a target value whereas unsupervised has no known results to learn and it has a state-based or adaptive mechanism to learn by itself. Supervised learning involves high computation costs whereas unsupervised learning has low training cost. Supervised learning finds applications in classification and regression tasks whereas unsupervised learning finds applications in clustering and association rule mining.
11. What is a Linear Regression?

Ans. The linear regression equation is a one-degree equation with the most basic form being Y = mX + C where m is the slope of the line and C is the standard error. It is used when the response variable is continuous in nature for example height, weight, and the number of hours. It can be a simple linear regression if it involves continuous dependent variable with one independent variable and a multiple linear regression if it has multiple independent variables. 

Linear regression is a standard statistical practice to calculate the best fit line passing through the data points when plotted. The best fit line is chosen in such a way so that the distance of each data point is minimum from the line which reduces the overall error of the system. Linear regression assumes that the various features in the data are linearly related to the target. It is often used in predictive analytics for calculating estimates in the foreseeable future.
12. What is Logistic Regression?

Ans. Logistic regression is a technique in predictive analytics which is used when we are doing predictions on a variable which is dichotomous(binary) in nature. For example, yes/no or true/false etc. The equation for this method is of the form Y = eX + e – X . It is used for classification based tasks. It finds out probabilities for a data point to belong to a particular class for classification.
13. Explain Normal Distribution

Ans. Normal Distribution is also called the Gaussian Distribution. It is a type of probability distribution such that most of the values lie near the mean. It has the following characteristics:

    The mean, median, and mode of the distribution coincide
    The distribution has a bell-shaped curve
    The total area under the curve is 1
    Exactly half of the values are to the right of the centre, and the other half to the left of the centre

14. Mention some drawbacks of the Linear Model

Ans. Here a few drawbacks of the linear model:

    The assumption regarding the linearity of the errors
    It is not usable for binary outcomes or count outcome
    It can’t solve certain overfitting problems
    It also assumes that there is no multicollinearity in the data. 

15. Which one would you choose for text analysis, R or Python?

Ans. Python would be a better choice for text analysis as it has the Pandas library to facilitate easy to use data structures and high-performance data analysis tools. However, depending on the complexity of data one could use either which suits best.
16. What steps do you follow while making a decision tree?

Ans. The steps involved in making a decision tree are:

    Determine the Root of the Tree Step
    Calculate Entropy for The Classes Step
    Calculate Entropy After Split for Each Attribute
    Calculate Information Gain for each split
    Perform the Split
    Perform Further Splits Step
    Complete the Decision Tree 

data science interview questions Steps involved in making a Decision Tree
17. What is correlation and covariance in statistics?

Ans. Correlation is defined as the measure of the relationship between two variables. If two variables are directly proportional to each other, then its positive correlation. If the variables are indirectly proportional to each other, it is known as a negative correlation. Covariance is the measure of how much two random variables vary together.
18. What is ‘Naive’ in a Naive Bayes?

Ans. A naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, given the class variable. Basically, it’s “naive” because it makes assumptions that may or may not turn out to be correct.
19. How can you select k for k-means?

Ans. The two methods to calculate the optimal value of k in k-means are:

    Elbow method
    Silhouette score method

Silhouette score is the most prevalent while determining the optimal value of k.
20. What Native Data Structures Can You Name in Python? Of These, Which Are Mutable, and Which Are Immutable?

Ans. The native data structures of python are:

    Lists
    Tuples
    Sets
    Dictionary

Tuples are immutable. Others are mutable.
21. What libraries do data scientists use to plot data in Python?

Ans. The libraries used for data plotting are:

    matplotlib
    seaborn
    ggplot. 

Apart from these, there are many opensource tools, but the aforementioned are the most used in common practice.
22. How is Memory Managed in Python?

Ans. Memory management in Python involves a private heap containing all Python objects and data structures. The management of this private heap is ensured internally by the Python memory manager.
23. What is a recall?

Ans. Recall gives the rate of true positives with respect to the sum of true positives and false negatives. It is also known as true positive rate.
24. What are lambda functions?

Ans. A lambda function is a small anonymous function. A lambda function can take any number of arguments, but can only have one expression.
25. What is reinforcement learning?

Ans. Reinforcement learning is an unsupervised learning technique in machine learning. It is a state-based learning technique. The models have predefined rules for state change which enable the system to move from one state to another, while the training phase.
26. What is Entropy and Information Gain in decision tree algorithm?

Ans. Entropy is used to check the homogeneity of a sample. If the value of entropy is ‘0’ then the sample is completely homogenous. On the other hand, if entropy has a value ‘1’, the sample is equally divided. Entropy controls how a Decision Tree decides to split the data. It actually affects how a Decision Tree draws its boundaries.

The information gain depends on the decrease in entropy after the dataset is split on an attribute. Constructing a decision tree is always about finding the attributes that return highest information gain.
27. What is Cross-Validation? 

Ans. It is a model validation technique to asses how the outcomes of a statistical analysis will infer to an independent data set. It is majorly used where prediction is the goal and one needs to estimate the performance accuracy of a predictive model in practice.
The goal here is to define a data-set for testing a model in its training phase and limit overfitting and underfitting issues. The validation and the training set is to be drawn from the same distribution to avoid making things worse.

Also Read: Why Data Science Jobs Are in Demand
28. What is Bias-Variance tradeoff?

Ans. The error introduced in your model because of over-simplification of the algorithm is known as Bias. On the other hand, Variance is the error introduced to your model because of the complex nature of machine learning algorithm. In this case, the model also learns noise and perform poorly on the test dataset.

The bias-variance tradeoff is the optimum balance between bias and variance in a machine learning model. If you try to decrease bias, the variance will increase and vice-versa.

Total Error= Square of bias+variance+irreducible error. Bias variance tradeoff is the process of finding the exact number of features while model creation such that the error is kept minimum, but also taking effective care such that the model does not overfit or underfit.
29. Mention the types of biases that occur during sampling?

Ans. The three types of biases that occur during sampling are:
a. Self-Selection Bias
b. Under coverage bias
c. Survivorship Bias

Self selection is when the participants of the analysis select themselves. Undercoverage occurs when very few samples are selected from a segment of the population. Survivorship bias occurs when the observations recorded at the end of the investigation are a non-random set of those present at the beginning of the investigation.
30. What is the Confusion Matrix?

Ans. A confusion matrix is a 2X2 table that consists of four outputs provided by the binary classifier.

A binary classifier predicts all data instances of a test dataset as either positive or negative. This produces four outcomes-

    True positive(TP) — Correct positive prediction
    False-positive(FP) — Incorrect positive prediction
    True negative(TN) — Correct negative prediction
    False-negative(FN) — Incorrect negative prediction

It helps in calculating various measures including error rate (FP+FN)/(P+N), specificity(TN/N), accuracy(TP+TN)/(P+N), sensitivity (TP/P), and precision( TP/(TP+FP) ).

A confusion matrix is essentially used to evaluate the performance of a machine learning model when the truth values of the experiments are already known and the target class has more than two categories of data. It helps in visualisation and evaluation of the results of the statistical process.
31. Explain selection bias

Ans. Selection bias occurs when the research does not have a random selection of participants. It is a distortion of statistical analysis resulting from the method of collecting the sample. Selection bias is also referred to as the selection effect. When professionals fail to take selection bias into account, their conclusions might be inaccurate.

Some of the different types of selection biases are:

    Sampling Bias – A systematic error that results due to a non-random sample
    Data – Occurs when specific data subsets are selected to support a conclusion or reject bad data
    Attrition – Refers to the bias caused due to tests that didn’t run to completion.

32. What are exploding gradients?

Ans. Exploding Gradients is the problematic scenario where large error gradients accumulate to result in very large updates to the weights of neural network models in the training stage. In an extreme case, the value of weights can overflow and result in NaN values. Hence the model becomes unstable and is unable to learn from the training data.
33. Explain the Law of Large Numbers

Ans. The ‘Law of Large Numbers’ states that if an experiment is repeated independently a large number of times, the average of the individual results is close to the expected value. It also states that the sample variance and standard deviation also converge towards the expected value.
34. What is the importance of A/B testing

Ans. The goal of A/B testing is to pick the best variant among two hypotheses, the use cases of this kind of testing could be a web page or application responsiveness, landing page redesign, banner testing, marketing campaign performance etc. 
The first step is to confirm a conversion goal, and then statistical analysis is used to understand which alternative performs better for the given conversion goal.
35. Explain Eigenvectors and Eigenvalues

Ans. Eigenvectors depict the direction in which a linear transformation moves and acts by compressing, flipping, or stretching. They are used to understand linear transformations and are generally calculated for a correlation or covariance matrix. 
The eigenvalue is the strength of the transformation in the direction of the eigenvector. 

An eigenvector’s direction remains unchanged when a linear transformation is applied to it.
Upskill with Great Learning’s DSBA Program today!
36. Why Is Re-sampling Done?

Ans. Resampling is done to:

    Estimate the accuracy of sample statistics with the subsets of accessible data at hand
    Substitute data point labels while performing significance tests
    Validate models by using random subsets 

37. What is systematic sampling and cluster sampling

Ans. Systematic sampling is a type of probability sampling method. The sample members are selected from a larger population with a random starting point but a fixed periodic interval. This interval is known as the sampling interval. The sampling interval is calculated by dividing the population size by the desired sample size.

Cluster sampling involves dividing the sample population into separate groups, called clusters. Then, a simple random sample of clusters is selected from the population. Analysis is conducted on data from the sampled clusters.
38.What are Autoencoders?

Ans. An autoencoder is a kind of artificial neural network. It is used to learn efficient data codings in an unsupervised manner. It is utilised for learning a representation (encoding) for a set of data, mostly for dimensionality reduction, by training the network to ignore signal “noise”. Autoencoder also tries to generate a representation as close as possible to its original input from the reduced encoding.
39. What are the steps to build a Random Forest Model?

A Random Forest is essentially a build up of a number of decision trees. The steps to build a random forest model include:

Step1: Select ‘k’ features from a total of ‘m’ features, randomly. Here k << m

Step2: Calculate node D using the best split point — along the ‘k’ features 

Step 3: Split the node into daughter nodes using best splitStep 4: Repeat Steps 2 and 3 until the leaf nodes are finalised

Step5: Build a Random forest by repeating steps 1-4 for ‘n’ times to create ‘n’ number of trees. 
40. How do you avoid the overfitting of your model?

Overfitting basically refers to a model that is set only for a small amount of data. It tends to ignore the bigger picture. Three important methods to avoid overfitting are:

    Keeping the model simple—using fewer variables and removing major amount of the noise in the training data
    Using cross-validation techniques. E.g.: k folds cross-validation 
    Using regularisation techniques — like LASSO, to penalise model parameters that are more likely to cause overfitting.

41. Differentiate between univariate, bivariate, and multivariate analysis.

Univariate data, as the name suggests, contains only one variable. The univariate analysis describes the data and finds patterns that exist within it. 

Bivariate data contains two different variables. The bivariate analysis deals with causes, relationships and analysis between those two variables.

Multivariate data contains three or more variables. Multivariate analysis is similar to that of a bivariate, however, in a multivariate analysis, there exists more than one dependent variable.
42. How is random forest different from decision trees?

Ans. A Decision Tree is a single structure. Random forest is a collection of decision trees.
43. What is dimensionality reduction? What are its benefits?

Dimensionality reduction is defined as the process of converting a data set with vast dimensions into data with lesser dimensions — in order to convey similar information concisely. 

This method is mainly beneficial in compressing data and reducing storage space. It is also useful in reducing computation time due to fewer dimensions. Finally,  it helps remove redundant features — for instance, storing a value in two different units (meters and inches) is avoided.

In short, dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.
44. For the given points, how will you calculate the Euclidean distance in Python? plot1 = [1,3 ]  ; plot2 = [2,5] 

Ans.

import math
# Example points in 2-dimensional space...
x = (1,3)
y = (2,5)
distance = math.sqrt(sum([(a - b) ** 2 for a, b in zip(x, y)]))
print("Euclidean distance from x to y: ",distance)

45. Mention feature selection methods used to select the right variables.

The methods for feature selection can be broadly classified into two types:

Filter Methods: These methods involve:

    Linear discrimination analysis
    ANOVA
    Chi-Square

Wrapper Methods: These methods involve

    Forward Selection: One feature at a time is tested and a good fit is obtained
    Backward Selection: All features are reviewed to see what works better
    Recursive Feature Elimination: Every different feature is looked at recursively and paired together accordingly. 

Others are Forward Elimination, Backward Elimination for Regression, Cosine Similarity-Based Feature Selection for Clustering tasks, Correlation-based eliminations etc.
Machine Learning in Data Science Interview Questions
46. What are the different types of clustering algorithms?

Ans. Kmeans Clustering, KNN (K nearest neighbour), Hierarchial clustering, Fuzzy Clustering are some of the common examples of clustering algorithms.
47. How should you maintain a deployed model?

Ans. A deployed model needs to be retrained after a while so as to improve the performance of the model. Since deployment, a track should be kept of the predictions made by the model and the truth values. Later this can be used to retrain the model with the new data. Also, root cause analysis for wrong predictions should be done.
48. Which of the following machine learning algorithms can be used for inputting missing values of both categorical and continuous variables? K-means clustering Linear regression K-NN (k-nearest neighbour) Decision trees

Ans. KNN and Kmeans
49. What is a ROC Curve? Explain how a ROC Curve works?

Ans. AUC – ROC curve is a performance measurement for the classification problem at various thresholds settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.
50. How do you find RMSE and MSE in a linear regression model?

Ans. Mean square error is the squared sum of (actual value-predicted value) for all data points. It gives an estimate of the total square sum of errors. Root mean square is the square root of the squared sum of errors.
51. Can you cite some examples where a false negative holds more importance than a false positive?

Ans. In cases of predictions when we are doing disease prediction based on symptoms for diseases like cancer.
52. How can outlier values be treated?

Ans. Outlier treatment can be done by replacing the values with mean, mode, or a cap off value. The other method is to remove all rows with outliers if they make up a small proportion of the data. A data transformation can also be done on the outliers.
53. How can you calculate accuracy using a confusion matrix?

Ans. Accuracy score can be calculated by the formula: (TP+TN)/(TP+TN+FP+FN), where TP= True Positive, TN=True Negatives, FP=False positive, and FN=False Negative.
54. What is the difference between “long” and “wide” format data?

Ans. Wide-format is where we have a single row for every data point with multiple columns to hold the values of various attributes. The long format is where for each data point we have as many rows as the number of attributes and each row contains the value of a particular attribute for a given data point.
55. Explain the SVM machine learning algorithm in detail.

Ans. SVM is an ML algorithm which is used for classification and regression. For classification, it finds out a muti dimensional hyperplane to distinguish between classes. SVM uses kernels which are namely linear, polynomial, and rbf. There are few parameters which need to be passed to SVM in order to specify the points to consider while the calculation of the hyperplane.
56. What are the various steps involved in an analytics project?

Ans. The steps involved in a text analytics project are:

    Data collection
    Data cleansing
    Data pre-processing
    Creation of train test and validation sets
    Model creation
    Hyperparameter tuning
    Model deployment 

57. Explain Star Schema.

Ans. Star schema is a data warehousing concept in which all schema is connected to a central schema.
58. How Regularly Must an Algorithm be Updated?

Ans. It completely depends on the accuracy and precision being required at the point of delivery and also on how much new data we have to train on. For a model trained on 10 million rows its important to have new data with the same volume or close to the same volume. Training on 1 million new data points every alternate week, or fortnight won’t add much value in terms of increasing the efficiency of the model.
59. What is Collaborative Filtering?

Ans. Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users. It works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user.
60. How will you define the number of clusters in a clustering algorithm?

Ans. By determining the Silhouette score and elbow method, we determine the number of clusters in the algorithm.
61. What is Ensemble Learning? Define types.

Ans. Ensemble learning is clubbing of multiple weak learners (ml classifiers) and then using aggregation for result prediction. It is observed that even if the classifiers perform poorly individually, they do better when their results are aggregated. An example of ensemble learning is random forest classifier.
62. What are the support vectors in SVM?

Ans. Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximise the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.
63. What is pruning in Decision Tree?

Ans. Pruning is the process of reducing the size of a decision tree. The reason for pruning is that the trees prepared by the base algorithm can be prone to overfitting as they become incredibly large and complex.
64. What are the various classification algorithms?

Ans. Different types of classification algorithms include logistic regression, SVM, Naive Bayes, decision trees, and random forest.
65. What are Recommender Systems?

Ans. A recommendation engine is a system, which on the basis of data analysis of the history of users and behaviour of similar users, suggests products, services, information to users. A recommendation can take user-user relationship, product-product relationships, product-user relationship etc. for recommendations.
Data Analysis Interview Questions
66. List out the libraries in Python used for Data Analysis and Scientific Computations.

Ans. The libraries NumPy, Scipy, Pandas, sklearn, Matplotlib which are most prevalent. For deep learning Pytorch, Tensorflow is great tools to learn.
67. State the difference between the expected value and the mean value.

Ans. Mathematical expectation, also known as the expected value, is the summation or integration of possible values from a random variable. Mean value is the average of all data points.
68. How are NumPy and SciPy related?

Ans. NumPy and SciPy are python libraries with support for arrays and mathematical functions. They are very handy tools for data science.
69. What will be the output of the below Python code?

def multipliers ():
return [lambda x: i * x for i in range (4)]
print [m (2) for m in multipliers ()]

Ans. Error
70. What do you mean by list comprehension?

Ans. List comprehension is an elegant way to define and create a list in Python. These lists often have the qualities of sets but are not in all cases sets. List comprehension is a complete substitute for the lambda function as well as the functions map(), filter(), and reduce().
71. What is __init__ in Python?

Ans. “__init__” is a reserved method in python classes. It is known as a constructor in object-oriented concepts. This method is called when an object is created from the class and it allows the class to initialise the attributes of the class.
72. What is the difference between append() and extend() methods?

Ans. append() is used to add items to list. extend() uses an iterator to iterate over its argument and adds each element in the argument to the list and extends it.
73. What is the output of the following? x = [ ‘ab’, ‘cd’ ] print(len(list(map(list, x))))

Ans. 2
74. Write a Python program to count the total number of lines in a text file.

Ans.

count=0
with open ('filename.txt','rb') as f:
    for line in f:
        count+=1

print count

75. How will you read a random line in a file?

Ans.

import random 
def random_line(fname): lines = open(fname).read().splitlines() 
    return random.choice(lines) print(random_line('test.txt'))

76. How would you effectively represent data with 5 dimensions?

Ans. It can be represented in a NumPy array of dimensions (n*n*n*n*5)
77. Whenever you exit Python, is all memory de-allocated?

Ans. Objects having circular references are not always free when python exits. Hence when we exit python all memory doesn’t necessarily get deallocated.
78. How would you create an empty NumPy array?

Ans.

"import numpy as np
np.empty([2, 2])"

79. Treating a categorical variable as a continuous variable would result in a better predictive model?

Ans. There is no substantial evidence for that, but in some cases, it might help. It’s totally a brute force approach. Also, it only works when the variables in question are ordinal in nature.
80. How and by what methods data visualisations can be effectively used?

Ans. Data visualisation is greatly helpful while creation of reports. There are quite a few reporting tools available such as tableau, Qlikview etc. which make use of plots, graphs etc for representing the overall idea and results for analysis. Data visualisations are also used in exploratory data analysis so that it gives us an overview of the data.
81. You are given a data set consisting of variables with more than 30 per cent missing values. How will you deal with them?

Ans. If 30 per cent data is missing from a single column then, in general, we remove the column. If the column is too important to be removed we may impute values. For imputation, several methods can be used and for each method of imputation, we need to evaluate the model. We should stick with one that model which gives us the best results and generalises well to unseen data.
82. What is skewed Distribution & uniform distribution?

Ans. The skewed distribution is a distribution in which the majority of the data points lie to the right or left of the centre. A uniform distribution is a probability distribution in which all outcomes are equally likely.
83. What can be used to see the count of different categories in a column in pandas?

Ans. value_counts will show the count of different categories.
84. What is the default missing value marker in pandas, and how can you detect all missing values in a DataFrame?

Ans. NaN is the missing values marker in pandas. All rows with missing values can be detected by is_null() function in pandas.
85. What is root cause analysis?

Ans. Root cause analysis is the process of tracing back of occurrence of an event and the factors which lead to it. It’s generally done when a software malfunctions. In data science, root cause analysis helps businesses understand the semantics behind certain outcomes.
86. What is a Box-Cox Transformation?

Ans. A Box Cox transformation is a way to normalise variables. Normality is an important assumption for many statistical techniques; if your data isn’t normal, applying a Box-Cox means that you are able to run a broader number of tests.
87. What if instead of finding the best split, we randomly select a few splits and just select the best from them. Will it work?

Ans. The decision tree is based on a greedy approach. It selects the best option for each branching. If we randomly select the best split from average splits, it would give us a locally best solution and not the best solution producing sub-par and sub-optimal results.
88. What is the result of the below lines of code?

def fast (items= []):
items.append (1)
return items

print fast ()
print fast ()

Ans. [1]
89. How would you produce a list with unique elements from a list with duplicate elements?

Ans.

l=[1,1,2,2]
l=list(set(l))
l

90. How will you create a series from dict in Pandas?

Ans.

import pandas as pd 
  
# create a dictionary 
dictionary = {'cat' : 10, 'Dog' : 20} 
  
# create a series 
series = pd.Series(dictionary) 
  
print(series) 

91. How will you create an empty DataFrame in Pandas?

Ans.

column_names = ["a", "b", "c"]

df = pd.DataFrame(columns = column_names)

92. How to get the items of series A not present in series B?

Ans. We can do so by using series.isin() in pandas.
93. How to get frequency counts of unique items of a series?

Ans. pandas.Series.value_counts gives the frequency of items in a series.
94. How to convert a numpy array to a dataframe of given shape?

Ans. If matrix is the numpy array in question: df = pd.DataFrame(matrix) will convert matrix into a dataframe.
95. What is Data Aggregation?

Ans. Data aggregation is a process in which aggregate functions are used to get the necessary outcomes after a groupby. Common aggregation functions are sum, count, avg, max, min.
96. What is Pandas Index?

Ans. An index is a unique number by which rows in a pandas dataframe are numbered.
97. Describe Data Operations in Pandas?

Ans. Common data operations in pandas are data cleaning, data preprocessing, data transformation, data standardisation, data normalisation, data aggregation.
98. Define GroupBy in Pandas?

Ans. groupby is a special function in pandas which is used to group rows together given certain specific columns which have information for categories used for grouping data together.
99. How to convert the index of a series into a column of a dataframe?

Ans. df = df.reset_index() will convert index to a column in a pandas dataframe.
Advanced Data Science Interview Questions
100. How to keep only the top 2 most frequent values as it is and replace everything else as ‘Other’?

Ans.

"s = pd.Series(np.random.randint(1, 5, [12]))
print(s.value_counts())
s[~s.isin(ser.value_counts().index[:2])] = 'Other'
s"

101. How to convert the first character of each element in a series to uppercase?

Ans. pd.Series([x.title() for x in s])
102. How to get the minimum, 25th percentile, median, 75th, and max of a numeric series?

Ans.

"randomness= np.random.RandomState(100)
s = pd.Series(randomness.normal(100, 55, 5))
np.percentile(ser, q=[0, 25, 50, 75, 100])"

103. What kind of data does Scatterplot matrices represent?

Ans. Scatterplot matrices are most commonly used to visualise multidimensional data. It is used in visualising bivariate relationships between a combination of variables.
104. What is the hyperbolic tree?

Ans. A hyperbolic tree or hypertree is an information visualisation and graph drawing method inspired by hyperbolic geometry.
105. What is scientific visualisation? How it is different from other visualisation techniques?

Ans. Scientific visualization is representing data graphically as a means of gaining insight from the data. It is also known as visual data analysis. This helps to understand the system that can be studied in ways previously impossible.
106. What are some of the downsides of Visualisation?

Ans. Few of the downsides of visualisation are: It gives estimation not accuracy, a different group of the audience may interpret it differently, Improper design can cause confusion.
107. What is the difference between a tree map and heat map?

Ans. A heat map is a type of visualisation tool that compares different categories with the help of colours and size. It can be used to compare two different measures. The ‘tree map’ is a chart type that illustrates hierarchical data or part-to-whole relationships.
108. What is disaggregation and aggregation of data?

Ans. Aggregation basically is combining multiple rows of data at a single place from low level to a higher level. Disaggregation, on the other hand, is the reverse process i.e breaking the aggregate data to a lower level.
109. What are some common data quality issues when dealing with Big Data?

Ans. Some of the major quality issues when dealing with big data are duplicate data, incomplete data, the inconsistent format of data, incorrect data, the volume of data(big data), no proper storage mechanism, etc.
110. What is a confusion matrix?

Ans. A confusion matrix is a table for visualising the performance of a classification algorithm on a set of test data for which the true values are known.
111. What is clustering?

Ans. Clustering means dividing data points into a number of groups. The division is done in a way that all the data points in the same group are more similar to each other than the data points in other groups. A few types of clustering are Hierarchical clustering, K means clustering, Density-based clustering, Fuzzy clustering etc.
112. What are the data mining packages in R?

Ans. A few popular data mining packages in R are Dplyr- data manipulation, Ggplot2- data visualisation, purrr- data wrangling, Hmisc- data analysis, datapasta- data import etc.
113. What are techniques used for sampling? Advantage of sampling 

There are various methods for drawing samples from data.

The two main Sampling techniques are

    Probability sampling
    Non-probability sampling 

Probability sampling

Probability sampling means that each individual of the population has a possibility of being included in the sample. Probability sampling methods include – 

    Simple random sampling

In simple random sampling, each individual of the population has an equivalent chance of being selected or included.

    Systematic sampling

Systematic sampling is very much similar to random sampling. The difference is just that instead of randomly generating numbers, in systematic sampling every individual of the population is assigned a number and are chosen at regular intervals.

    Stratified sampling

In stratified sampling, the population is split into sub-populations. It allows you to conclude more precise results by ensuring that every sub-population is represented in the sample.

    Cluster sampling

Cluster sampling also involves dividing the population into sub-populations, but each subpopulation should have analogous characteristics to that of the whole sample. Rather than sampling individuals from each subpopulation, you randomly select the entire subpopulation.

Non-probability sampling 

In non-probability sampling, individuals are selected using non-random ways and not every individual has a possibility of being included in the sample.

    Convenience sampling

Convenience sampling is a method where data is collected from an easily accessible group.

    Voluntary Response sampling
    Voluntary Response sampling is similar to convenience sampling, but here instead of researchers choosing individuals and then contacting them, people or individuals volunteer themselves.

    Purposive sampling

Purposive sampling also known as judgmental sampling is where the researchers use their expertise to select a sample that is useful or relevant to the purpose of the research.

    Snowball sampling

Snowball sampling is used where the population is difficult to access. It can be used to recruit individuals via other individuals.

Advantages of Sampling  

    Low cost advantage 
    Easy to analyze by limited resources 
    Less time than other techniques
    Scope is considered to be considerably high 
    Sampled data is considered to be high 
    Organizational convenience 

114. What is imbalance data?

Imbalance data in simple words is a reference to different types of datasets where there is an uneven distribution of observations to the target class.  Which means, one class label has higher observations than the other comparatively. 
115. Define Lift, KPI, Robustness, Model fitting and DOE

Lift is used to understand the performance of a given targeting model in predicting performance, when compared against a randomly picked targeting model. 

KPI or Key performance indicators is a yardstick used to measure the performance of an organization or an employee based on organizational objectives. 

Robustness is a property that identifies the effectiveness of an algorithm when tested with a new independent dataset. 

Model fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained.

Design of Experiment (DOE) is a set of mathematical methods for process optimization and for quality by design (QbD).
116. Define Confounding Variables

A confounding variable is an external influence in an experiment. In simple words, these variables change the effect of a dependent and independent variable. A variable should satisfy below conditions to be a confounding variable :

    Variables should be correlated to the independent variable.
    Variables should be informally related to the dependent variable.

For example, if you are studying whether a lack of exercise has an effect on weight gain, then the lack of exercise is an independent variable and weight gain is a dependent variable. A confounder variable can be any other factor that has an effect on weight gain. Amount of food consumed, weather conditions etc. can be a confounding variable.
117. Why are time series problems different from other regression problems?

Time series is extrapolation whereas Regression is interpolation. Time-series refers to an organized chain of data. Time-series forecasts what comes next in the sequence. Time-series could be assisted with other series which can occur together. 

Regression can be applied to Time-series problems as well as to non-ordered sequences which are termed as Features. While making a projection, new values of Features are presented and Regression calculates results for the target variable. 
118. What is the difference between the Test set and validation set?

Test set : Test set is a set of examples used only to evaluate the performance of a fully specified classifier. In simple words, it is used to fit the parameters. It is used to test the data which is passed as input to your model.

Validation set : Validation set is a set of examples used to tune the parameters of a classifier. In simple words, it is used to tune the parameters. Validation set is used to validate the output which is produced by your model.

Kernel Trick

A Kernel Trick is a method where a linear classifier is used to solve non-linear problems. In other words, it is a method where a non-linear object is projected to a higher dimensional space to make it easier to categorize where the data would be divided linearly by a plane.

Let’s understand it better,

Let’s define a Kernel function K as xi and xj as just being the dot product.

K(xi,xj) = xi . xj = xTixj   

If every data point is mapped into the high-dimensional space via some transformation

Φ:x -> Φ(x)

The dot product becomes: 

K(xi,xj) = ΦxTiΦxj 

Box Plot and Histograms

Box Plot and Histogram are types of charts that represent numerical data graphically. It is an easier way to visualize data. It makes it easier to compare characteristics of data between categories.



11. What is POS tagging?

Parts of speech tagging better known as POS tagging refers to the process of identifying specific words in a document and group them as part of speech, based on its context. POS tagging is also known as grammatical tagging since it involves understanding grammatical structures and identifying the respective component.

POS tagging is a complicated process since the same word can be different parts of speech depending on the context. The same generic process used for word mapping is quite ineffective for POS tagging because of the same reason.
12. What is NES?

Name entity recognition is more commonly known as NER is the process of identifying specific entities in a text document which are more informative and have a unique context. These often denote places, people, organisations, and more. Even though it seems like these entities are proper nouns, the NER process is far from identifying just the nouns. In fact, NER involves entity chunking or extraction wherein entities are segmented to categorise them under different predefined classes. This step further helps in extracting information. 
NLP Interview Questions for Experienced

13. Which of the following techniques can be used for keyword normalization in NLP, the process of converting a keyword into its base form?
a. Lemmatization
b. Soundex
c. Cosine Similarity
d. N-grams
Answer: a)
Lemmatization helps to get to the base form of a word, e.g. are playing -> play, eating -> eat, etc..
Other options are meant for different purposes.

14. Which of the following techniques can be used to compute the distance between two word vectors in NLP?
a. Lemmatization
b. Euclidean distance
c. Cosine Similarity
d. N-grams
Answer: b) and c)
Distance between two word vectors can be computed using Cosine similarity and Euclidean Distance.  Cosine Similarity establishes a cosine angle between the vector of two words. A cosine angle close to each other between two word vectors indicates the words are similar and vice a versa.
E.g. cosine angle between two words “Football” and “Cricket” will be closer to 1 as compared to angle between the words “Football” and “New Delhi”

Python code to implement CosineSimlarity function would look like this
def cosine_similarity(x,y):
    return np.dot(x,y)/( np.sqrt(np.dot(x,x)) * np.sqrt(np.dot(y,y)) )
q1 = wikipedia.page(‘Strawberry’)
q2 = wikipedia.page(‘Pineapple’)
q3 = wikipedia.page(‘Google’)
q4 = wikipedia.page(‘Microsoft’)
cv = CountVectorizer()
X = np.array(cv.fit_transform([q1.content, q2.content, q3.content, q4.content]).todense())
print (“Strawberry Pineapple Cosine Distance”, cosine_similarity(X[0],X[1]))
print (“Strawberry Google Cosine Distance”, cosine_similarity(X[0],X[2]))
print (“Pineapple Google Cosine Distance”, cosine_similarity(X[1],X[2]))
print (“Google Microsoft Cosine Distance”, cosine_similarity(X[2],X[3]))
print (“Pineapple Microsoft Cosine Distance”, cosine_similarity(X[1],X[3]))
Strawberry Pineapple Cosine Distance 0.8899200413701714
Strawberry Google Cosine Distance 0.7730935582847817
Pineapple Google Cosine Distance 0.789610214147025
Google Microsoft Cosine Distance 0.8110888282851575
Usually Document similarity is measured by how close semantically the content (or words) in the document are to each other. When they are close, the similarity index is close to 1, otherwise near 0.
The Euclidean distance between two points is the length of the shortest path connecting them. Usually computed using Pythagoras theorem for a triangle.
15. What are the possible features of a text corpus in NLP?

a. Count of the word in a document
b. Vector notation of the word
c. Part of Speech Tag
d. Basic Dependency Grammar
e. All of the above
Answer: e)
All of the above can be used as features of the text corpus.

16. You created a document term matrix on the input data of 20K documents for a Machine learning model. Which of the following can be used to reduce the dimensions of data?

    Keyword Normalization
    Latent Semantic Indexing
    Latent Dirichlet Allocation

a. only 1
b. 2, 3
c. 1, 3
d. 1, 2, 3
Answer: d)

17. Which of the text parsing techniques can be used for noun phrase detection, verb phrase detection, subject detection, and object detection in NLP.
a. Part of speech tagging
b. Skip Gram and N-Gram extraction
c. Continuous Bag of Words
d. Dependency Parsing and Constituency Parsing
Answer: d)

18. Dissimilarity between words expressed using cosine similarity will have values significantly higher than 0.5
a. True
b. False
Ans: a)

19. Which one of the following are keyword Normalization techniques in NLP
a.  Stemming
b.  Part of Speech
c. Named entity recognition
d. Lemmatization
Answer: a) and d)
Part of Speech (POS) and Named Entity Recognition(NER) are not keyword Normalization techniques. Named Entity help you extract Organization, Time, Date, City, etc..type of entities from the given sentence, whereas Part of Speech helps you extract Noun, Verb, Pronoun, adjective, etc..from the given sentence tokens.

20. Which of the below are NLP use cases?
a. Detecting objects from an image
b. Facial Recognition
c. Speech Biometric
d. Text Summarization
Ans: d)
a) And b) are Computer Vision use cases, and c) is Speech use case.
Only d) Text Summarization is an NLP use case.

21. In a corpus of N documents, one randomly chosen document contains a total of T terms and the term “hello” appears K times.
What is the correct value for the product of TF (term frequency) and IDF (inverse-document-frequency), if the term “hello” appears in approximately one-third of the total documents?
a. KT * Log(3)
b. T * Log(3) / K
c. K * Log(3) / T
d. Log(3) / KT
Answer: (c)
formula for TF is K/T
formula for IDF is log(total docs / no of docs containing “data”)
= log(1 / (⅓))
= log (3)
Hence correct choice is Klog(3)/T

22. In NLP, The algorithm decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents
a. Term Frequency (TF)
b. Inverse Document Frequency (IDF)
c. Word2Vec
d. Latent Dirichlet Allocation (LDA)
Ans: b)

While there are ample resources available online to help you understand the subject, there’s nothing quite like taking up a certificate course. Take up the Introduction to Natural Language Processing Free Online Course offered by Great Learning Academy to learn the basics concepts and earn a certificate that’ll help you step into the world of NLP.

23. In NLP, The process of removing words like “and”, “is”, “a”, “an”, “the” from a sentence is called as
a. Stemming
b. Lemmatization
c. Stop word
d. All of the above
Ans: c) 
In Lemmatization, all the stop words such as a, an, the, etc.. are removed. One can also define custom stop words for removal.

24. In NLP, The process of converting a sentence or paragraph into tokens is referred to as Stemming
a. True
b. False
Ans: b)
The statement describes the process of tokenization and not stemming, hence it is False.

25. In NLP, Tokens are converted into numbers before giving to any Neural Network
a. True
b. False
Ans: a)
In NLP, all words are converted into a number before feeding to a Neural Network.

26. identify the odd one out
a. nltk
b. scikit learn
c. SpaCy
d. BERT
Ans: d)
All the ones mentioned are NLP libraries except BERT, which is a word embedding

27. TF-IDF helps you to establish?
a. most frequently occurring word in the document
b. most important word in the document
Ans: b)

TF-IDF helps to establish how important a particular word is in the context of the document corpus. TF-IDF takes into account the number of times the word appears in the document and offset by the number of documents that appear in the corpus.

    TF is the frequency of term divided by a total number of terms in the document.
    IDF is obtained by dividing the total number of documents by the number of documents containing the term and then taking the logarithm of that quotient.
    Tf.idf is then the multiplication of two values TF and IDF.

Suppose that we have term count tables of a corpus consisting of only two documents, as listed here
Term	Document 1 Frequency	Document 2 Frequency
This	1	1
is	1	1
a	2	 
Sample	1	 
another 	 	2
example	 	3

The calculation of tf–idf for the term “this” is performed as follows:
for “this”
———–
tf(“this”, d1) = 1/5 = 0.2
tf(“this”, d2) = 1/7 = 0.14
idf(“this”, D) = log (2/2) =0
hence tf-idf
tfidf(“this”, d1, D) = 0.2* 0 = 0
tfidf(“this”, d2, D) = 0.14* 0 = 0
for “example”
————
tf(“example”, d1) = 0/5 = 0
tf(“example”, d2) = 3/7 = 0.43
idf(“example”, D) = log(2/1) = 0.301
tfidf(“example”, d1, D) = tf(“example”, d1) * idf(“example”, D) = 0 * 0.301 = 0
tfidf(“example”, d2, D) = tf(“example”, d2) * idf(“example”, D) = 0.43 * 0.301 = 0.129
In its raw frequency form, TF is just the frequency of the “this” for each document. In each document, the word “this” appears once; but as document 2 has more words, its relative frequency is smaller.
An IDF is constant per corpus, and accounts for the ratio of documents that include the word “this”. In this case, we have a corpus of two documents and all of them include the word “this”. So TF–IDF is zero for the word “this”, which implies that the word is not very informative as it appears in all documents.
The word “example” is more interesting – it occurs three times, but only in the second document.

28. In NLP, The process of identifying people, an organization from a given sentence, paragraph is called
a. Stemming
b. Lemmatization
c. Stop word removal
d. Named entity recognition
Ans: d)

29. Which one of the following is not a pre-processing technique in NLP
a. Stemming and Lemmatization
b. converting to lowercase
c. removing punctuations
d. removal of stop words
e. Sentiment analysis
Ans: e)
Sentiment Analysis is not a pre-processing technique. It is done after pre-processing and is an NLP use case. All other listed ones are used as part of statement pre-processing.

30. In text mining, converting text into tokens and then converting them into an integer or floating-point vectors can be done using
a. CountVectorizer
b.  TF-IDF
c. Bag of Words
d. NERs
Ans: a)
CountVectorizer helps do the above, while others are not applicable.
text =[“Rahul is an avid writer, he enjoys studying understanding and presenting. He loves to play”]
vectorizer = CountVectorizer()
vectorizer.fit(text)
vector = vectorizer.transform(text)
print(vector.toarray())

output 
[[1 1 1 1 2 1 1 1 1 1 1 1 1 1]]
The second section of the interview questions covers advanced NLP techniques such as Word2Vec, GloVe  word embeddings, and advanced models such as GPT, ELMo, BERT, XLNET based questions, and explanations.


31. In NLP, Words represented as vectors are called as Neural Word Embeddings
a. True
b. False
Ans: a)
Word2Vec, GloVe based models build word embedding vectors that are multidimensional.

32. In NLP, Context modeling is supported with which one of the following word embeddings

    a. Word2Vec
    b) GloVe
    c) BERT
    d) All of the above

Ans: c)
Only BERT (Bidirectional Encoder Representations from Transformer) supports context modelling where the previous and next sentence context is taken into consideration. In Word2Vec, GloVe only word embeddings are considered and previous and next sentence context is not considered.

33. In NLP, Bidirectional context is supported by which of the following embedding
a. Word2Vec
b. BERT
c. GloVe
d. All the above
Ans: b)
Only BERT provides a bidirectional context. The BERT model uses the previous and the next sentence to arrive at the context.Word2Vec and GloVe are word embeddings, they do not provide any context.

34. Which one of the following Word embeddings can be custom trained for a specific subject in NLP
a. Word2Vec
b. BERT
c. GloVe
d. All the above
Ans: b)
BERT allows Transform Learning on the existing pre-trained models and hence can be custom trained for the given specific subject, unlike Word2Vec and GloVe where existing word embeddings can be used, no transfer learning on text is possible.

35. Word embeddings capture multiple dimensions of data and are represented as vectors
a. True
b. False
Ans: a)

36. In NLP, Word embedding vectors help establish distance between two tokens
a. True
b. False
Ans: a)
One can use Cosine similarity to establish distance between two vectors represented through Word Embeddings

37. Language Biases are introduced due to historical data used during training of word embeddings, which one amongst the below is not an example of bias
a. New Delhi is to India, Beijing is to China
b. Man is to Computer, Woman is to Homemaker
Ans: a)
Statement b) is a bias as it buckets Woman into Homemaker, whereas statement a) is not a biased statement.

38. Which of the following will be a better choice to address NLP use cases such as semantic similarity, reading comprehension, and common sense reasoning
a. ELMo
b. Open AI’s GPT
c. ULMFit
Ans: b)
Open AI’s GPT is able to learn complex pattern in data by using the Transformer models Attention mechanism and hence is more suited for complex use cases such as semantic similarity, reading comprehensions, and common sense reasoning.

39. Transformer architecture was first introduced with?
a. GloVe
b. BERT
c. Open AI’s GPT
d. ULMFit
Ans: c)
ULMFit has an LSTM based Language modeling architecture. This got replaced into Transformer architecture with Open AI’s GPT

40. Which of the following architecture can be trained faster and needs less amount of training data
a. LSTM based Language Modelling
b. Transformer architecture
Ans: b)
Transformer architectures were supported from GPT onwards and were faster to train and needed less amount of data for training too.

41. Same word can have multiple word embeddings possible with ____________?
a. GloVe
b. Word2Vec
c. ELMo
d. nltk
Ans: c)

EMLo word embeddings supports same word with multiple embeddings, this helps in using the same word in a different context and thus captures the context than just meaning of the word unlike in GloVe and Word2Vec. Nltk is not a word embedding.
NLP Interview questions infographicsai-01
token, segment and position embeddings


42. For a given token, its input representation is the sum of embedding from the token, segment and position embedding
a. ELMo
b. GPT
c. BERT
d. ULMFit
Ans: c)
BERT uses token, segment and position embedding.

43. Trains two independent LSTM language model left to right and right to left and shallowly concatenates them
a. GPT
b. BERT
c. ULMFit
d. ELMo
Ans: d)
ELMo tries to train two independent LSTM language models (left to right and right to left) and concatenates the results to produce word embedding.

44. Uses unidirectional language model for producing word embedding
a. BERT
b. GPT
c. ELMo
d. Word2Vec
Ans: b) 
GPT is a idirectional model and word embedding are produced by training on information flow from left to right. ELMo is bidirectional but shallow. Word2Vec provides simple word embedding.
45. In this architecture, the relationship between all words in a sentence is modelled irrespective of their position. Which architecture is this?

a. OpenAI GPT
b. ELMo
c. BERT
d. ULMFit
Ans: c)

BERT Transformer architecture models the relationship between each word and all other words in the sentence to generate attention scores. These attention scores are later used as weights for a weighted average of all words’ representations which is fed into a fully-connected network to generate a new representation.
46. List 10 use cases to be solved using NLP techniques?

    Sentiment Analysis
    Language Translation (English to German, Chinese to English, etc..)
    Document Summarization
    Question Answering
    Sentence Completion
    Attribute extraction (Key information extraction from the documents)
    Chatbot interactions
    Topic classification
    Intent extraction
    Grammar or Sentence correction
    Image captioning
    Document Ranking
    Natural Language inference

Difference between BERT and XLNet
47. Transformer model pays attention to the most important word in Sentence


a. True
b. False
Ans: a) Attention mechanisms in the Transformer model are used to model the relationship between all words and also provide weights to the most important word.

48. Which NLP model gives the best accuracy amongst the following?

a. BERT
b. XLNET
c. GPT-2
d. ELMo
Ans: b) XLNET
XLNET has given best accuracy amongst all the models. It has outperformed BERT on 20 tasks and achieves state of art results on 18 tasks including sentiment analysis, question answering, natural language inference, etc.

49. Permutation Language models is a feature of


a. BERT
b. EMMo
c. GPT
d. XLNET
Ans: d) 
XLNET provides permutation-based language modelling and is a key difference from BERT. In permutation language modeling, tokens are predicted in a random manner and not sequential. The order of prediction is not necessarily left to right and can be right to left. The original order of words is not changed but a prediction can be random. 
The conceptual difference between BERT and XLNET can be seen from the following diagram.

50. Transformer XL uses relative positional embedding

a. True
b. False
Ans: a)
Instead of embedding having to represent the absolute position of a word, Transformer XL uses an embedding to encode the relative distance between the words. This embedding is used to compute the attention score between any 2 words that could be separated by n words before or after.

There, you have it – all the probable questions for your NLP interview. Now go, give it your best shot.



